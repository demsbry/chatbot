{
  "faq_questions": [
    {
      "question": "Do you have an overview I can share with my team?",
      "answer": "Yes, here’s a link to a short presentation which covers our investment parameters, team, portfolio examples, popular resources and case studies."
    },
    {
      "question": "What markets do you invest in?",
      "answer": "Intelligent business software. We believe that the trends in cloud, AI, and data are combining to transform the entire enterprise stack and expanding the markets software can address. We call this new megatrend Cognitive Applications and if you’re a founder focused on these trends in markets like DataOps, DevOps, Digital Health, Fintech, Open Source, Productivity, Security and Vertical SaaS, we’d love to talk to you."
    },
    {
      "question": "What stage(s) do you invest in?",
      "answer": "Scale is an early-stage venture capital firm. We typically lead Series A or Series B rounds after you’ve signed your first customers and when you’re ready to move from founder-led growth to a repeatable, go-to-market machine. We also invest in later stage opportunities (Series C or beyond), focused on markets where we have strong conviction on the size of the opportunity. Across our funds we can support our companies through multiple rounds of financing all the way to an IPO."
    },
    {
      "question": "What do you look for in a startup?",
      "answer": "We look for founders who are developing technology to transform the way we work, collaborate, or live. Their companies are in the early stages of revenue with vocal customer champions and they’re looking for capital to rapidly scale and achieve category leadership."
    },
    {
      "question": "How do you select investments?",
      "answer": "Scale has a formal and repeatable process for identifying, evaluating and selecting specific markets and technology investments. We develop forward-looking opinions on the dynamics of emerging trends impacting enterprises and how people work. Then we search for the companies benefiting from—sometimes even defining—these trends."
    },
    {
      "question": "How do you support your portfolio companies?",
      "answer": "Scale plays an active role in its portfolio companies, including in most cases serving on the board. A dedicated Operating Partner also serves as each company’s point of contact, leveraging our Scaling Platform that uses executive networks, go-to-market playbooks, private communities, and Scale Studio benchmarks to help you move from founder-led growth to a repeatable go-to-market machine."
    },
    {
      "question": "How much capital have you invested?",
      "answer": "Scale raised its $900 million Fund VIII in September 2022. Total assets under management stand at $2.8 billion."
    },
    {
      "question": "In what geographies do you invest?",
      "answer": "Scale invests in great software companies wherever they are located. Our portfolio includes companies based throughout the United States, in Europe, and Israel."
    }
  ],
  "team_members": [
    {
      "name": "Eric Anderson",
      "title": "Partner",
      "email": "eric@scalevp.com",
      "bio": "Eric is a Partner at Scale Venture Partners, where he focuses on investments in cloud infrastructure and security. He is a Board member at Scale portfolio companies Datastax and Upsolver, and a Board observer at Matillion, BigID, Expel, Honeycomb, and AppOmni. \nBefore Scale, Eric led early Google Cloud and Amazon Web Services product teams. At Google, he was a Product Manager in the Data Analytics and Machine Learning group. He led the team that launched Cloud Dataprep and key components of Cloud Dataflow. Previously, Eric built aircraft engines in General Electric's Operation Management Leadership Program. He's a contributor to open source projects including Alluxio, where he is on the Project Management Committee, and Apache Beam. \nEric holds a BS in Mechanical Engineering from University of Utah and an MBA from Harvard Business School. He is the creator and host of Contributor, an interview series detailing the origin stories of open source projects.",
      "investments": [
        "AppOmni",
        "BigID",
        "Comet",
        "DataStax",
        "Expel",
        "Honeycomb",
        "Matillion",
        "PerimeterX",
        "Tetrate",
        "Upsolver"
      ]
    },
    {
      "name": "Stacey Bishop",
      "title": "Partner",
      "email": "stacey@scalevp.com",
      "bio": "Stacey invests in Intelligent business software. She currently serves on the Board of Directors of Airspace, Demandbase, Extole, Narvar (observer), Textio, and Verusen. Prior board positions include HubSpot (NYSE: HUBS), Bizible (Acq: Marketo), Lever (Acq: Employ), and Vitrue (Acq: Oracle). Stacey also originated investments in Bill.com, ExactTarget, and Omniture.\n\nStacey is a founding member of All Raise and an advisor to The University Growth Fund. Prior to Scale, Stacey specialized in M&A within Bank of America’s Corporate Development Group, worked in equity research at Morgan Stanley, and began her career as an account manager at Syntel.\n\nStacey holds an MBA from Columbia Business School and a BA from The University of Michigan.",
      "investments": [
        "Abstract",
        "Airspace",
        "AllyO",
        "Bizible",
        "DemandBase",
        "ExactTarget",
        "Extole",
        "HubSpot",
        "Lever",
        "Namely",
        "Narvar",
        "Omniture",
        "Textio",
        "Verusen",
        "Vitrue"
      ]
    },
    {
      "name": "Jeremy Kaufmann",
      "title": "Partner",
      "email": "jeremyk@scalevp.com",
      "bio": "Jeremy focuses on investments across Scale’s technology portfolio with a primary focus on companies adding machine intelligence to business and vertical software, especially in industries untouched by the first decade of SaaS. His efforts have contributed to investments across vertical software (Motive formally KeepTruckin), AI and automation (Datagen, Flatfile, Cognata, TechSee, and Solvvy), and the intersection of machine learning and digital health (OM1, Proscia, Robin Healthcare and Viz.ai).\n\nPrior to joining Scale, Jeremy worked on the Sales Strategy and Business Operations team at Salesforce, where he drove initiatives in sales operations, partnerships, and growth, serving as the embedded strategy lead for the SMB business and the sales development organization. Additionally, he worked as a data scientist at Analysis Group, focused on health economics, healthcare outcomes research, and drug pricing. Jeremy started his career as a policy researcher at the Federal Reserve Bank of New York, where he concentrated on consumer credit, college loan accumulation, and education in the age of automation.\n\nJeremy holds a B.A. in Economics from Dartmouth College, where he graduated summa cum laude. Outside of work, Jeremy enjoys cycling, tennis, and just about any outdoor adventure you can throw at him.",
      "investments": [
        "Cognata",
        "Datagen",
        "Esper",
        "Flatfile",
        "Motive",
        "OM1",
        "regie.ai",
        "Solvvy",
        "Spot",
        "TechSee",
        "Proscia",
        "Robin",
        "VergeSense",
        "Viz.ai"
      ]
    },
    {
      "name": "Alex Niehenke",
      "title": "Partner",
      "email": "alex@scalevp.com",
      "bio": "Alex joined Scale in 2013 as a Principal, responsible for sourcing investments in DroneDeploy and Forter. A Partner since 2017, Alex has focused on early investments in vertical software markets where incumbents have failed to invest in advanced technology offerings. That thesis has led to investments in Motive formally KeepTruckin (2017), Root Insurance (2018), Scout RFP (2019), Proxy (2020), Spruce (2020), Proscia (2020) and Archipelago (2021), and Dusty Robotics. KeepTruckin has been one of the fastest growing SaaS companies of all time; Scout RFP was acquired by Workday in late 2019 for $540M; and Root completed its IPO in late 2020. \n\nAlex sits on the Board of Directors of KeepTruckin, Proxy, Spruce, Proscia, and Archipelago. He has written about SaaS in TechCrunch and been profiled by Business Insider, Forbes, and Silicon Valley Business Journal.",
      "investments": [
        "Archipelago",
        "Dusty",
        "Motive",
        "Proscia",
        "Proxy",
        "Root",
        "Scout",
        "Spruce",
        "DroneDeploy",
        "Forter"
      ]
    },
    {
      "name": "Rory O'Driscoll",
      "title": "Partner",
      "email": "rory@scalevp.com",
      "bio": "Rory has been investing in enterprise software for the past 25 years. An active investor in the rise of SaaS and the wider transition of enterprise computing to the cloud, Rory led investments in Bill.com (BILL), Box (BOX), DocuSign (DOCU), ExactTarget (ET; Acq: SFDC), Omniture (OMTR; Acq: ADBE), Placeware (Acq: MSFT), and WalkMe (WKME) among others. Currently, Rory is focused on investing in business applications powered by AI (cognitive applications), frontier tech, and the continuing adoption of SaaS and cloud. He currently sits on the board of directors at Bill.com, Cognata, DroneDeploy, Flatfile, Forter, Locus Robotics, OM1, Pantheon, Papaya Global, Robin Healthcare, Socure, Soft Robotics, Solvvy, VergeSense, Viz.ai, and WalkMe. Rory has been recognized by the Forbes Midas List and AlwaysOn Power Players in Venture Capital for his investments. He holds a BSc from the London School of Economics.",
      "investments": [
        "Bill.com",
        "Box",
        "Chef",
        "Cognata",
        "DocuSign",
        "DroneDeploy",
        "ExactTarget",
        "Flatfile",
        "Forter",
        "Locus",
        "OM1",
        "Omniture",
        "OneLogin",
        "Pantheon",
        "Papaya",
        "Plus",
        "regie.ai",
        "Robin",
        "Socure",
        "Soft",
        "Solvvy",
        "Spot",
        "VergeSense",
        "Viz.ai",
        "WalkMe",
        "Wrike"
      ]
    },
    {
      "name": "Ariel Tseitlin",
      "title": "Partner",
      "email": "ariel@scalevp.com",
      "bio": "Ariel is a Partner at Scale focused on investments in the cloud and security industries. He currently sits on the board of directors at AppOmni, BigID, CyberGRX, Expel, Honeycomb, Human, and Upsolver.  Ariel is also a Board observer at Tetrate. \n \nPreviously, Ariel was Director of Cloud Solutions at Netflix where he was responsible for creating and operating one of the most modern cloud infrastructures in the industry, accounting for a full third of all US downstream internet traffic at peak. Ariel’s team built many of the Netflix OSS components like Asgard and the Simian Army, including the Chaos Monkey, making the Netflix streaming service more resilient, reliable, and manageable.\n \nPrior to Netflix, Ariel was VP of Technology and Products at Sungevity and before that was the Founder & CEO of CTOWorks, a software consultancy helping early-stage entrepreneurs deliver their first product to market. Earlier in his career, Ariel held senior management positions at Siebel Systems and Oracle. Ariel holds a bachelor’s degree in Computer Science from UC Berkeley and an MBA with honors from the Wharton School of Business at the University of Pennsylvania.",
      "investments": [
        "Agari",
        "AppOmni",
        "BigID",
        "Chef",
        "CloudHealth",
        "CyberGRX",
        "Expel",
        "Honeycomb",
        "Loops",
        "PerimeterX",
        "Tetrate",
        "Threat",
        "Upsolver",
        "Vantage"
      ]
    },
    {
      "name": "Andy Vitus",
      "title": "Partner",
      "email": "andy@scalevp.com",
      "bio": "Andy has been investing with Scale for nearly 15 years and focuses on cloud infrastructure, machine-learning applications, and enterprise software.  Andy sits on the boards of CircleCI, Comet, Datagen, Esper,  JFrog, Matillion, Observe.AI, PubNub, TechSee and Unbabel. He was previously on the board of Treasure Data (Acq: ARM) and Stormpath (Acq: Okta).\n \nAn engineer at heart, Andy spends his free time coding, testing and trialing the software and technologies that interest him. Prior to joining Scale, Andy worked as an engineer for Electronics for Imaging, designing embedded print server platforms and video compression circuits. Andy earned an M.S. in Electrical Engineering from Stanford University and a B.S. in Electrical Engineering from the University of Cape Town, South Africa.",
      "investments": [
        "CircleCI",
        "Comet",
        "Datagen",
        "DataStax",
        "Esper",
        "JFrog",
        "Matillion",
        "Observe.AI",
        "PubNub",
        "Realm",
        "Stormpath",
        "TechSee",
        "Treasure",
        "Unbabel",
        "Unifi"
      ]
    },
    {
      "name": "Sam Baker",
      "title": "Principal",
      "email": "sam@scalevp.com",
      "bio": "Sam joined Scale in 2016 and focuses on the firm's investment in SaaS, vertical software, robotics, and other frontier technology. He was named to Business Insider's list of Rising Stars in Venture Capital, and has contributed to the firm's investments in Dusty Robotics, Locus Robotics, Socure, CyberGRX, OM1, Proxy and PerimeterX. \n\nPrior to joining Scale, Sam was a Growth Manager at Tilt.com (acquired by Airbnb) where he was responsible for building out the company's user acquisition and retention programs. Beforehand, he worked for Box (NYSE: BOX) on the company's SMB Sales and Strategic Planning & Analysis teams, concentrating on business development and go-to-market initiatives. Sam began his career at Permal Capital Management (now Glouston Capital Partners), a Boston-based investment firm focused on private equity and venture capital secondaries.\n\nSam holds a B.A. from Brown University. He is a committed endurance athlete, completing thirteen marathons and qualifying for Boston on nine occasions.",
      "investments": [
        "Dusty",
        "CyberGRX",
        "Locus",
        "OM1",
        "PerimeterX",
        "Plus",
        "Proxy",
        "Socure"
      ]
    },
    {
      "name": "Javier Redondo",
      "title": "Principal",
      "email": "javier@scalevp.com",
      "bio": "Javier focuses on opportunities in infrastructure and business software, with particular interest in open source and fintech.\n\nPrior to Scale, Javier led product efforts in the distributed systems space.  At Anyscale, he was responsible for the development of a managed compute offering and a contributor to the Ray project.  At Confluent, he drove major initiatives within Apache Kafka and the company's SaaS solution.\n\nPreviously, Javier built up the card issuing and processing functions at Klarna.  He started his career at McKinsey, advising enterprise clients in digital transformation and ML adoption.  He holds BS and MS degrees in Engineering from ICAI School of Engineering in Spain, and an MBA from Stanford University Graduate School of Business.",
      "investments": [
        "Loops",
        "regie.ai"
      ]
    },
    {
      "name": "Juliette Chevallier",
      "title": "Principal",
      "email": "Juliette@scalevp.com",
      "bio": "Prior to Scale, Juliette was an investor at Anza Partners, focusing on AI/ML, robotics, and advanced analytics. Her investments included an AI in-memory computing chip developer, a leading provider of hardware and software products for embedded systems, and a microgrid design and operation SaaS tool.\n\nPrior to her tenure as an investor, Juliette spent several years in product development and product management. Juliette began her career as a systems engineer in the Autonomous Vehicle (AV) industry working on both ground and air platforms at companies such as Google [X] (now Waymo) and Boeing NeXt. Additionally, Juliette worked in product management at Starry, a disruptive internet service provider using millimeter-band LMDS connections, which went public via SPAC in 2022.\n\nJuliette holds a BA in Applied Mathematics from Wellesley College, a BS in Mechanical Engineering with a focus in Design from Olin College of Engineering, an MS in Aeronautics and Astronautics with a focus on Autonomy from MIT and an MBA from MIT’s Sloan School of Management.\n\nOutside of work, Juliette enjoys spending time hiking with her partner and pup, playing classical piano, and baking new recipes.",
      "investments": []
    },
    {
      "name": "John Gianakopoulos",
      "title": "Vice President",
      "email": "john@scalevp.com",
      "bio": "John joined Scale in 2018 and focuses on the firm's investments in fintech and broader enterprise SaaS with a particular focus on companies adding machine intelligence to business and vertical software. He has been featured multiple times on Business Insider's list of up-and-coming financial technology investors and has sourced and contributed to the firm's investments in Papaya Global, AppOmni, Spot AI, Vantage, Socure, and Scout RFP (among others listed below).\n\nBefore Scale, John was most recently a fintech Product Manager at Personal Capital where he focused on customer engagement and growth. Previously, he was also a Product Manager at Intuit and launched the first iteration of their QuickBooks Capital lending product, which provided working capital loans to small business owners. During his time as a PM, John also started a payments-based advertising business focused on the film industry.\n\nJohn holds dual degrees in both the business and liberal arts honors programs from the University of Texas at Austin. Outside of work, John is an avid runner in the Marin headlands and enjoys playing soccer (and watching the USMNT consistently underperform even his lowest expectations).",
      "investments": [
        "AppOmni",
        "Archipelago",
        "Comet",
        "Papaya",
        "Scout",
        "Socure",
        "Spot",
        "Spruce",
        "Tetrate",
        "Vantage"
      ]
    },
    {
      "name": "Noah Gross",
      "title": "Vice President",
      "email": "noah@scalevp.com",
      "bio": "Noah joined Scale in 2019 from Cisco Investments and focuses on B2B applications adding automation and machine intelligence as well as the fintech ecosystem. He is closely involved with portfolio companies Root Insurance, Airspace Technologies, Matillion, Textio, Upsolver, Verusen, and Robin. Prior to joining Scale, Noah worked on Cisco’s Investments and Acquisitions team focused on the enterprise collaboration space. While at Cisco, Noah supported investments in Whatfix, Luma Health, Theatro, ActionIQ, and the acquisitions of Voicea and CloudCherry. Noah started his career at JPMorgan on the Technology Investment Banking team where he advised companies on acquisitions and capital raises.\n\nNoah holds a B.A. from The University of Michigan's Ross School of Business. Outside of work, Noah enjoys hiking, cooking, and traveling.",
      "investments": [
        "Upsolver",
        "Root",
        "Airspace",
        "Matillion",
        "Textio",
        "Verusen",
        "Robin",
        "Spot"
      ]
    },
    {
      "name": "Max Abram",
      "title": "Senior Associate",
      "email": "maxwell@scalevp.com",
      "bio": "Max joined Scale in 2021 and focusses primarily on investments in Applied AI. He has contributed to the firm's investments in Regie.ai, Dusty Robotics, and Verusen.\n\nBefore Scale, Max was a private equity investor at Kayne Anderson, focused on growth-stage vertical software companies.\n\nMax holds a B.A. in Philosophy, Politics, & Economics from the University of Pennsylvania, where he was also a varsity lightweight rower. Outside of work, Max remains an active endurance athlete, and he is an aficionado of narrative-driven podcasts (favorites include Hardcore History, Radiolab, and The Memory Palace).",
      "investments": [
        "Dusty",
        "regie.ai",
        "Verusen"
      ]
    },
    {
      "name": "Maggie Basta",
      "title": "Associate",
      "email": "maggie@scalevp.com",
      "bio": "Maggie joined Scale in 2022 and works across the Scale technology portfolio. Some of her primary focus areas include machine learning and data infrastructure, machine intelligent applications, and developer tools. \n\nPrior to joining Scale, Maggie worked at QuantCo, where she built machine learning and data solutions for algorithmic pricing, fraud detection, and data migration. She also previously interned at the Blackstone Group as a member of the company’s Innovations team.  \n\nMaggie holds a joint B.A. in Engineering and Computer Science, from Harvard University. At Harvard, she was a member of multiple research groups and played on the Women’s Varsity Soccer team. Outside of work, she likes to spend her time running, surfing, and painting.",
      "investments": []
    },
    {
      "name": "Sreya Chagarlamudi",
      "title": "Associate",
      "email": "sreya@scalevp.com",
      "bio": "Sreya joined Scale’s investing team in 2022 and focuses on opportunities in infrastructure and business software. Prior to joining Scale, Sreya worked at Tesla as a Product Manager. While there she worked on leading product development and engineering for internal tools, document management, and automation.\n\nSreya holds multiple degrees from the University of Arizona where she studied Finance, Management Information Systems, Operations Management, and Computer Science. Outside of work, Sreya has been training in Indian classical dance and singing since the age of 5 and she enjoys spending her free time taking part in the arts. Sreya also enjoys running, hiking, yoga, and exploring San Francisco.",
      "investments": []
    },
    {
      "name": "Jonas Ciplickas",
      "title": "Associate",
      "email": "jonas@scalevp.com",
      "bio": "Jonas joined Scale in 2022 and focuses on opportunities across enterprise tech. Prior to Scale, Jonas was a member of Morgan Stanley's technology investment banking team, where he partnered with management teams across software, health tech, consumer internet, and semiconductors. \n\nJonas holds a B.A. in math and an M.S. in computer science from the University of Chicago. Outside of work, he spends his time walking up hills in San Francisco, cooking new dishes, and hunting for vintage furniture.",
      "investments": []
    },
    {
      "name": "Abbas Haider",
      "title": "Associate",
      "email": "abbas@scalevp.com",
      "bio": "Abbas joined Scale's investing team in 2022. He previously worked for Deutsche Bank's technology investment banking group, where he helped businesses across the software, internet, and hardware verticals with capital raising and M&A. \n\nAbbas holds a B.S. in Economics and Finance from Arizona State University. Outside of work, Abbas enjoys cheering on the Phoenix Suns and Arizona Cardinals, competing in local trivia nights, and discovering new music.",
      "investments": []
    },
    {
      "name": "Shayan Shafii",
      "title": "Associate",
      "email": "shayan@scalevp.com",
      "bio": "Shayan joined the Scale team in 2022. He focuses on investments in infrastructure software, with particular interests in cybersecurity, developer tools, open source software, and the shift to cloud-native infrastructure.\n\nPrior to joining Scale, Shayan was a Senior Product Manager at Rubrik Inc., where he led roadmap, development, and go-to-market efforts for infrastructure backup products. During his time at Rubrik, he launched new services for data protection and oversaw key components of the company’s SaaS strategy.\n\nShayan holds a BS in Electrical Engineering and Computer Science and a BA in Business Administration from the University of Michigan, where he was a Douglas and Charlotte McGregor Scholar. Outside of work, he enjoys listening to music and playing soccer.",
      "investments": []
    },
    {
      "name": "Dale Chang",
      "title": "Operating Partner",
      "email": "dale@scalevp.com",
      "bio": "Dale is the Operating Partner at Scale. In his role, Dale is a resource for guidance on evolving go-to-market strategies as well as providing best practices and benchmarks across the portfolio.\n\nPreviously, Dale was a Director at the Alexander Group; a consulting firm specialized in go-to-market strategy.  At the Alexander Group, he was the leader of the Cloud practice and advised companies including Salesforce.com, LinkedIn, Box, Docusign, New Relic, Optimizely and MixPanel.  Prior to his career in consulting, he worked in sales and sales operations roles at startups including Proofpoint and Jaspersoft.",
      "investments": []
    },
    {
      "name": "Hillá Watkins",
      "title": "Chief Marketing Officer",
      "email": "hilla@scalevp.com",
      "bio": "Hillá joined Scale as Chief Marketing Officer in 2022. \n\nPrior to Scale, Hillá was VP of Brand Marketing at Pendo, where she grew the brand, along with the company, from 80 to 1,000 employees. Hillá managed the content, creative, and community teams at Pendo, where she also created ProductCraft, Pendo's community for product teams. Prior to that, she managed content marketing at several startups. Hillá started her career in nonprofit management, running development for several progressive education organizations. \n\nHillá holds a B.A. in history from Yale University, and a M.A. in Technology, Culture, and Communication from Georgetown University. She is an avid cook and language learner.",
      "investments": []
    },
    {
      "name": "Craig Rosenberg",
      "title": "Chief Platform Officer",
      "email": "craig@scalevp.com",
      "bio": "Craig joined Scale in 2022 to help build and manage their Go-To-Market Platform.\nThe Scale GTM Platform delivers critical expertise to help software \ncompanies drive efficient hyper-growth. \n\nPrior to joining Scale, Craig was Distinguished Vice President in the \nSales Practice for Gartner. While there he advised revenue \nleaders on their strategic decisions and wrote innovative research \non new GTM strategies such as revenue operations and account-\nbased strategy.  He joined Gartner in 2019 as part of the acquisition\nof TOPO where Craig was the co-founder and Chief Analyst.\n\nCraig was a Magna Cum Laude graduate of UCLA. When he isn’t \ntalking about GTM strategy, he is spending time with his wife and \nthree young boys.",
      "investments": []
    },
    {
      "name": "Anisha Agarwal",
      "title": "PORTFOLIO OPERATIONS ASSOCIATE",
      "email": "anisha@scalevp.com",
      "bio": "Anisha joined Scale in 2023 as a member of the Portfolio Operations team. Her focus areas include defining and implementing capabilities within the GTM platform as well as partnering with portfolio companies to drive success.\n\nPrior to Scale, Anisha worked in management consulting at ZS Associates, where she helped Fortune 500 clients build strategies to determine GTM approach, evaluate new verticals, and accelerate growth. Anisha graduated from UC Berkeley, where she studied Computer Science and Cognitive Science. Outside of work, she enjoys exploring new hiking trails, bouldering, and baking. She is also searching for someone to beat her record on the New York Times mini crossword.",
      "investments": []
    },
    {
      "name": "Eduard Danalache",
      "title": "Portfolio Operations Associate",
      "email": "edi@scalevp.com",
      "bio": "Edi joined Scale in 2021 as a member of the Portfolio Operations team, focusing on building tools and partnering with companies to drive success throughout the Scale portfolio.Prior to joining Scale Edi worked at McKinsey & Company in Houston, where he helped cross-industry clients develop strategies to shape future growth, drive go-to-market success, and build new businesses. \nEdi is originally from Romania by way of Northern Virginia and graduated from Rice University with a B.A. in Computer Science and Managerial Studies, and a minor in Business. Outside of work, Edi is an avid gamer and rock climber.",
      "investments": []
    }
  ],
  "portfolio_companies": [
    {
      "name": "Abstract",
      "description": "Abstract is changing the way designers work together and collaborate with the rest of the company.  As design has become a critical function for all modern businesses,  Abstract has created the first design workflow system providing one place for design teams to version, manage and collaborate on design files.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Agari",
      "description": "Agari is the Trusted Email Identity Company™, protecting brands and people from devastating phishing and socially-engineered attacks. Using applied data science and a diverse set of signals, Agari protects the workforce from inbound business email compromise, supply chain fraud, spear phishing, and account takeover-based attacks, reducing business risk and restoring trust to the inbox. Agari also prevents spoofing of outbound email from the enterprise to customers, increasing deliverability and preserving brand integrity.",
      "investors": [
        "Ariel Tseitlin"
      ]
    },
    {
      "name": "Airspace Technologies",
      "description": "Airspace Technologies is creating the most trusted delivery network the world has ever seen, making time-critical shipping faster, safer, and more transparent.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "AllyO",
      "description": "AllyO is an “end to end AI recruiter” that uses conversational AI in the form of a customer-tailored chatbot interface to automate many of the routine aspects of qualifying and managing a talent pipeline",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Applause",
      "description": "Applause is leading the app quality revolution by enabling companies to deliver digital experiences that win - from web to mobile to wearables and beyond. By combining in-the-wild testing services, software tools and analytics, Applause helps companies achieve the 360° app quality™ they need to thrive in the modern apps economy.",
      "investors": []
    },
    {
      "name": "AppOmni",
      "description": "AppOmni is a leading provider of SaaS Security Management software, making it easy for security and IT teams to secure their entire SaaS environment from each vendor to every end-user.",
      "investors": [
        "Ariel Tseitlin",
        "Eric Anderson",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "Apteligent",
      "description": "Apteligent provides real-time App performance and crash metrics. When milliseconds can cost millions, Apteligent reveals operational and usage trends, uncovers issues, and analyzes behaviors. Allowing customers to continuously improve apps, keep users happy, and put mobile business at the top of its game.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "Archipelago",
      "description": "Archipelago is a Risk Data Platform that uses AI to digitize risk for large owners of commercial property, so they can increase their resiliency and lower their total cost of risk, including by improving their insurance outcomes.",
      "investors": [
        "Alex Niehenke",
        "Chris Yin",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "Aviso",
      "description": "Aviso provides the leading predictive insights software designed to help sales teams optimize their performance and exceed their revenue goals. Aviso transforms and automates tedious sales forecasting processes with an easy-to-use, secure cloud application using a unique combination of machine learning algorithms and portfolio management techniques.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Axcient",
      "description": "Axcient replaces legacy backup, business continuity, disaster recovery and archiving products, with a single disaster recovery-as-a-service cloud platform, making it simple to restore data, failover applications, and virtualize servers or an entire office with a click.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "BigID",
      "description": "BigID is transforming how enterprises protect and manage the privacy of personal data. Organizations are facing record breaches of personal information and proliferating global privacy regulations like the EU GDPR with fines reaching 4% of annual revenue. Currently, enterprises lack dedicated purpose built technology to help them track and govern their customer data at scale. By bringing data science to data privacy, BigID aims to give enterprises the software to safeguard and steward the most important asset organizations manage: their customer data.",
      "investors": [
        "Ariel Tseitlin",
        "Eric Anderson"
      ]
    },
    {
      "name": "Bill.com",
      "description": "Bill.com is the leading digital business payments company, managing $70 billion in payments annually. The company is the preferred solution for more than 70% of the top 100 U.S. accounting firms.\r\n\r\nOn December 12, 2019 shares of BILL began trading on the NYSE.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "Bizible",
      "description": "Bizible is pioneering the marketing attribution space by offering an easy to use software-as-a-service product to help online marketers answer the simple question “what’s working” in terms of real return-on-investment, revenue, and closed sales. Our mission is to make every marketing dollar profitable for our customers.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Boundary",
      "description": "Boundary is the application performance monitoring solution designed for distributed application environments. Boundary addresses two critical challenges faced by developers and IT operations professionals: the need to immediately see and understand the impact of a rapidly changing application together with a significantly higher degree of visibility.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "Box",
      "description": "Box (NYSE:BOX) is transforming the way people and organizations work so they can achieve their greatest ambitions. As the world's leading enterprise software platform for content collaboration, Box helps businesses of all sizes in every industry securely access and manage their critical information in the cloud.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "BrightRoll",
      "description": "BrightRoll builds software that automates and improves digital video advertising globally. The company offers the industry’s leading programmatic video solutions, including a demand side platform and marketplace. These solutions help advertisers, publishers and partners grow their business and connect with consumers on web, mobile and TV.",
      "investors": []
    },
    {
      "name": "Chef",
      "description": "Chef is the leader in automation for DevOps, providing a model for automating IT infrastructure and applications that drive self-reliance across development and operations teams. The Chef community is tens of thousands strong, helping businesses become faster, safer and more flexibleto win in today’s 24×7 digital economy.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "CircleCI",
      "description": "Thousands of companies trust CircleCI to automate their testing and development cycle, enabling software teams to detect and fix bugs on web and mobile apps before they ever reach customers.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "CloudHealth Technologies",
      "description": "CloudHealth Technologies is pioneering IT Service Management for the cloud. By combining enterprise requirements, best practices, and cloud technology, we are transforming the way in which customers can see, evaluate, and manage their entire cloud ecosystem.",
      "investors": [
        "Ariel Tseitlin"
      ]
    },
    {
      "name": "Cognata",
      "description": "Fast Lane to Autonomous –  Building the world’s next generation simulation platform for the autonomous industry",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "Comet",
      "description": "Comet is a meta machine learning platform designed to help AI practitioners and teams build reliable machine learning models for real-world applications by streamlining and connecting the machine learning model lifecycle. By leveraging Comet, users can track, compare, explain and reproduce their machine learning experiments. Backed by thousands of users and multiple Fortune 100 companies, Comet provides insights and data to build better, more accurate AI models while improving productivity, collaborationand visibility across teams.",
      "investors": [
        "Andy Vitus",
        "Eric Anderson",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "CyberGRX",
      "description": "At CyberGRX we’re dedicated to helping organizations streamline their third-party cyber risk programs. We believe the process can be more efficient and effective, and we make it our mission to ensure our customers are getting the most of our innovative Exchange, every day.",
      "investors": [
        "Ariel Tseitlin",
        "Sam Baker",
        "Eric Anderson"
      ]
    },
    {
      "name": "Datagen",
      "description": "Datagen is powering the AI revolution with high-performance synthetic data used for human-centric computer vision applications.",
      "investors": [
        "Andy Vitus",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "DataSift",
      "description": "DataSift is the leader in Human Data Intelligence. Processing more than 2 billion interactions a day, DataSift empowers application developers and agencies to maximize value from human-generated data by transforming it into actionable intelligence.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "DataStax",
      "description": "DataStax delivers Apache Cassandra™ in a database platform purpose-built for the performance and availability demands for IOT, web and mobile applications. This gives enterprises a secure, always-on database technology",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "DemandBase",
      "description": "Demandbase is the leading advertising and personalization company for B2B. The Demandbase B2B Marketing Cloud is the only subscription-based ad targeting and web personalization solution that enables marketers to connect campaigns directly to revenue.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "DocuSign",
      "description": "DocuSign pioneered the development of e-signature technology, helping organizations connect and automate how they manage agreements. The company's platform more than 500,000 customers and hundreds of millions of users in over 180 countries.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "DroneDeploy",
      "description": "DroneDeploy makes the power of aerial data accessible and productive for everyone. As the leading cloud software platform for commercial drones it is transforming the way businesses of all sizes and industries operate. DroneDeploy's more than 5,000 customers have flown at 400,000 job sites in the oil & gas, agriculture, and construction industries.",
      "investors": [
        "Rory O'Driscoll",
        "Alex Niehenke"
      ]
    },
    {
      "name": "Dusty Robotics",
      "description": "Founded in 2018, Dusty Robotics develops robot-powered tools for the modern workforce, automating construction’s most painful manual workflows. Our FieldPrinter subscription service automates the BIM-to-field workflow through a combination of hardware, software, and services that fit seamlessly into existing construction work processes. Founded by serial entrepreneurs and roboticists who have built several successful robotics companies, Dusty is backed by leading venture capital firms Scale Venture Partners, Canaan Partners, NextGen Venture Partners, Baseline Ventures, Root Ventures and Cantos Ventures. For more information, please visit www.dustyrobotics.com.",
      "investors": [
        "Alex Niehenke",
        "Sam Baker",
        "Max Abram"
      ]
    },
    {
      "name": "Esper",
      "description": "Esper offers a DevOps SaaS platform for intelligent edge devices. As the industry’s leading solution for Android DevOps, Esper is on a mission to let software teams ship without worrying about the hardware -- even for enterprise fleets as large as 100,000+ devices.",
      "investors": [
        "Andy Vitus",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "ExactTarget",
      "description": "ExactTarget is a leading global provider of on-demand email marketing and interactive marketing solutions. The company’s Interactive Marketing Hub technology provides organizations a single solution to connect with customers via e-mail, integrated text messaging, landing pages and social media.",
      "investors": [
        "Rory O'Driscoll",
        "Stacey Bishop"
      ]
    },
    {
      "name": "Expel",
      "description": "Expel provides transparent managed security. It’s the antidote for companies trapped in failed relationships with their managed security service provider (MSSP) and those looking to avoid the frustration of working with one in the first place.",
      "investors": [
        "Ariel Tseitlin"
      ]
    },
    {
      "name": "Extole",
      "description": "Extole enables marketers to acquire new, high value customers at scale by rewarding existing customers. Its industry-leading referral platform helps brands take advantage of a unique resource they already have in front of them: their current customers.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Flatfile",
      "description": "Flatfile's mission is to remove barriers between humans and data. Their AI-assisted data onboarding eliminates repetitive work and makes B2B data transactions fast, intuitive, and error-free. Flatfile automatically learns how imported data should be structured and cleaned, enabling customers and teams to spend more time using their data instead of fixing it.",
      "investors": [
        "Rory O'Driscoll",
        "Chris Yin",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "Forter",
      "description": "Forter provides new generation fraud prevention for the challenges of modern enterprise e-commerce. Only Forter provides fully automated, real-time Decision as a Service™ antifraud, backed by a 100% chargeback guarantee. With no rules, scores or manual reviews, it’s friction-free: invisible to buyers and empowering merchants with increased approvals and smoother checkout. The result: more sales and happier customers.",
      "investors": [
        "Rory O'Driscoll",
        "Alex Niehenke"
      ]
    },
    {
      "name": "Honeycomb",
      "description": "Honeycomb defined Observability, and its solution is designed for modern Engineering and DevOps teams to observe, debug, and improve their production systems more efficiently so that business-critical apps perform with minimal disruption to users.",
      "investors": [
        "Ariel Tseitlin",
        "Eric Anderson"
      ]
    },
    {
      "name": "HubSpot",
      "description": "HubSpot is the world’s leading inbound marketing and sales platform. Since 2006, HubSpot has been on a mission to make the world more inbound. Today, more than 64,500 customers in more than 100 countries use HubSpot’s award-winning software, services, and support.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "JFrog",
      "description": "JFrog is a fully hybrid, multi-cloud and universal DevOps and DevSecOps solution that powers software packages from code repository to deployment at more than 5,500 customers, including more than 70% of the Fortune 500.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "Lever",
      "description": "Lever was founded to tackle the most strategic challenge that companies face: how to grow their teams. With its unique candidate-centric approach and emphasis on hiring team collaboration, Lever’s applicant tracking and sourcing technology helps businesses source, interview, and hire top talent in a fundamentally more human way.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Locus Robotics",
      "description": "Locus’s award-winning, innovative, autonomous mobile robots work collaboratively alongside workers, helping them pick 100-200% faster with near-100% accuracy, while saving 30% or more in operating expenses. This proven, powerful technology integrates easily into existing warehouse infrastructure without disrupting workflows.",
      "investors": [
        "Rory O'Driscoll",
        "Sam Baker"
      ]
    },
    {
      "name": "Loops",
      "description": "Loops enables product and growth teams to improve their key performance indicators (KPIs) by automatically identifying concrete opportunities from their data.",
      "investors": [
        "Ariel Tseitlin",
        "Javier Redondo"
      ]
    },
    {
      "name": "Matillion",
      "description": "Matillion is a software company that offers purpose-built, native ELT solutions for cloud data platforms. Matillion products are designed and developed for fast-growing, tech-savvy, and intelligence- and data- driven companies that are leveraging the cloud to gain insights.",
      "investors": [
        "Andy Vitus",
        "Eric Anderson"
      ]
    },
    {
      "name": "Motive (Formerly KeepTruckin)",
      "description": "Motive (formerly KeepTruckin) builds technology to improve the safety, productivity, and profitability of businesses that power the physical economy. The Motive Automated Operations Platform combines IoT hardware with AI-powered applications to automate vehicle and equipment tracking, driver safety, compliance, maintenance, spend management, and more. Motive serves more than 120,000 businesses, across a wide range of industries including trucking and logistics, construction, oil and gas, food and beverage, field service, agriculture, passenger transit, and delivery. Visit gomotive.com to learn more.",
      "investors": [
        "Alex Niehenke",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "Namely",
      "description": "Namely is the first HR platform that employees actually love to use. Namely’s powerful, easy-to-use technology allows companies to handle all of their HR, payroll, and benefits in one place. Coupled with dedicated account support, every Namely client gets the software and service they need to deliver great HR and a strong, engaged company culture.",
      "investors": [
        "Stacey Bishop",
        "Susan Liu"
      ]
    },
    {
      "name": "Narvar",
      "description": "By bringing together expertise in eCommerce, Supply Chain Management, Customer Care, and Machine Learning, Narvar empowers retailers to deliver world-class customer experiences through a comprehensive, turnkey, and scalable platform. So customers can turn shoppers into advocates.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Observe.AI",
      "description": "Observe.AI is the leading provider of next-generation voice AI that helps contact center agents provide better customer experience through AI-based coaching and insights.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "OM1",
      "description": "OM1 uses machine learning to create unique datasets based on real-world evidence that help healthcare companies accelerate regulatory approvals and better measure ROI.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "Omniture",
      "description": "Omniture is an online marketing and web analytics business unit of Adobe Systems",
      "investors": [
        "Rory O'Driscoll",
        "Stacey Bishop"
      ]
    },
    {
      "name": "OneLogin",
      "description": "OneLogin simpliﬁes identity management with secure, one-click access, for employees, customers and partners, through all device types, to all enterprise cloud and on-premises applications.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "Pantheon",
      "description": "The web is Earth's most powerful communication tool. Our mission is to put this power in the hands of our customers by delivering the most complete, bulletproof, and creativity enabling platform for professional website creation. We’re building the world’s best website management platform—one that gives web teams all of the developer tools, hosting, scaling, performance, workflow, and automation they need to build the best websites in the world.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "Papaya Global",
      "description": "Papaya Global's automated, cloud-based SaaS platform offers a total people & payroll management solution supporting all types of global employment (payroll, EoR, and contractors) in more than 140 countries.",
      "investors": [
        "Rory O'Driscoll",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "PeopleMatter",
      "description": "PeopleMatter is a powerful and complete workforce management platform designed to fit the specific needs of service-industry brands. Our complete set of mobile, workforce solutions and business analytics tools connect processes, employees and customers in entirely new ways.",
      "investors": [
        "Kate Mitchell",
        "Alex Niehenke"
      ]
    },
    {
      "name": "PerimeterX",
      "description": "Protects the world’s largest and most reputable websites, mobile apps and APIs from automated threats.",
      "investors": [
        "Ariel Tseitlin",
        "Eric Anderson",
        "Sam Baker"
      ]
    },
    {
      "name": "Plus One Robotics",
      "description": "Plus One Robotics is a provider of advanced AI vision software and solutions for parcel handing that accelerate shipping for some of the world’s largest logistics providers. Plus One’s unique human-in-the-loop system called “Yonder” allows employees to supervise multiple robots at once, enabling 24/7 operations with greater speed and fewer errors.",
      "investors": [
        "Rory O'Driscoll",
        "Sam Baker"
      ]
    },
    {
      "name": "Proscia",
      "description": "Digitizing and reinventing the 150-year-old standard for pathology.  Proscia’s Concentriq digital pathology platform and pipeline of AI-powered applications let laboratories leverage new kinds of data to accelerate discoveries and improve patient outcomes.",
      "investors": [
        "Alex Niehenke",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "Proxy",
      "description": "Proxy's digital identity solution transforms how people access and experience the physical world.",
      "investors": [
        "Alex Niehenke",
        "Sam Baker"
      ]
    },
    {
      "name": "PubNub",
      "description": "PubNub set out in 2009 to develop a Data Stream Network for developers to build realtime apps as easily as building a web page. The PubNub Data Stream Network provides global cloud infrastructure and key building blocks for realtime interactivity. We let developers spend their time on what they do best... creating killer realtime apps!",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "Realm",
      "description": "Realm is a mobile database. A replacement for SQLite & Core Data, Realm can save users thousands of lines of code & weeks of work enabling them to craft amazing new user experiences.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "regie.ai",
      "description": "Regie.ai blends the art of language and the science of delivery to create, test, and analyze personalized prospecting sales campaigns, helping businesses deliver engaging and effective communication tools.",
      "investors": [
        "Rory O'Driscoll",
        "Jeremy Kaufmann",
        "Javier Redondo",
        "Max Abram"
      ]
    },
    {
      "name": "RingCentral",
      "description": "Since 2003, RingCentral has been breaking down the communication barriers created by complex on-premise hardware. RingCentral delivers cloud business communications solutions that free people to work the way they want in today’s mobile, distributed and always-on work world. Delivered on a state-of-the-art cloud infrastructure, RingCentral’s cloud communications solutions help more than 300,000 customers thrive in a new world of work.",
      "investors": []
    },
    {
      "name": "Robin Healthcare",
      "description": "At Robin, we believe nothing should get in the way of the time doctors and patients spend together. Since 2017, we’ve been on a mission to remove barriers to health.  Using a combination of cutting-edge technology and human expertise, we’re creating a more direct path to care. Robin takes on the administrative burdens of medicine so doctors can reclaim their time and control over their practice.  We’re a bunch of incredibly smart, driven, problem-solving people on a mission to change the future of healthcare. We can’t do it alone. Check out our job listings to see if there’s a fit and come join us.",
      "investors": [
        "Rory O'Driscoll",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "Root Insurance",
      "description": "The insurance industry is broken. Root is here to fix it. To that end, Root is the first car insurance company that was founded on the relentless pursuit of fairness. We use an app to rate drivers based on how they actually drive—not just their demographics. By not insuring bad drivers, Root is able to save good drivers as much as 52% on their car insurance.",
      "investors": [
        "Alex Niehenke"
      ]
    },
    {
      "name": "Sailthru",
      "description": "Sailthru deliver personalized omnichannel experiences to consumers across email, onsite, mobile, recommendations, social and offline. A leading platform for modern marketers, with unparalleled data and technology.",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Scout RFP",
      "description": "Scout RFP makes strategic sourcing simpler, smarter, and more streamlined than ever before. Its intuitive, cloud-based platform – encompassing everything from project intake through sourcing pipeline to contract and supplier management to RFx and reverse auction tools– closes the loop on sourcing and, in the process, empowers collaboration, centralizes data, makes processes more transparent, and, ultimately, drives better business outcomes.",
      "investors": [
        "Alex Niehenke",
        "Susan Liu",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "Socure",
      "description": "Socure’s predictive analytics technology applies artificial intelligence and machine learning to verify identities in real-time. The platform confirms a customer’s identity from their digital breadcrumbs and calculates a risk score, empowering businesses to make smarter decisions about their customers.",
      "investors": [
        "Rory O'Driscoll",
        "Sam Baker",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "Soft Robotics",
      "description": "Soft Robotics opens new markets to automation through the application and commercialization of its proprietary soft robotics technology.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "Solvvy",
      "description": "Powered by artificial intelligence at its core, Solvvy is reimagining customer service by unlocking the power of enterprise knowledge. Solvvy answers common customer questions by tapping into their customers knowledge base and ticket history and learning from prior customer interactions. Solvvy provides immediate resolutions to customers, reduces ticket volume and drives operational efficiencies.",
      "investors": [
        "Rory O'Driscoll",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "Spot AI",
      "description": "Spot AI is on a mission to create a safer and smarter physical work with the power of sight. With an AI camera system, Spot AI makes video footage actionable to help companies improve their security, safety, efficiency, and customer experience.",
      "investors": [
        "Jeremy Kaufmann",
        "Rory O'Driscoll",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "Spruce",
      "description": "Spruce helps coordinate transactions between homeowners and lenders, handling title search, policy, settlement, and escrow by pairing intuitive software with human expertise.",
      "investors": [
        "Alex Niehenke",
        "Chris Yin",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "Stormpath",
      "description": "Stormpath helps developers launch applications and services faster with an Identity API that includes everything from user registration, password reset, authorization and user data storage to more advanced features like single sign-on and token authentication.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "TalkIQ",
      "description": "TalkIQ is a voice analytics platform that helps increase revenue by aggregating and predicting insights around customer satisfaction. Unlike traditional sales coaching platforms, our predictions and aggregations are surfaced in real-time using advanced artificial intelligence, machine learning, and natural language processing technologies.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "TechSee",
      "description": "Homes are getting smarter thanks to the rapid growth of the Internet of things (IoT). Yet the acquisition of more smart technology means that consumers need to spend more time installing, activating, maintaining, and repairing IoT devices and applications. \r\nTechSee helps businesses deliver a better customer experience by providing the simple self-service and fast, effective care that customers require, and meeting the unique demands of technical support from all perspectives: customers, contact center agents, field technicians, and self-service channels.",
      "investors": [
        "Andy Vitus",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "Tetrate",
      "description": "Tetrate is the enterprise service mesh company managing the complexity of modern, hybrid cloud application infrastructure. Its flagship product, Tetrate Service Bridge, provides a comprehensive, enterprise-ready service mesh platform built for multi-cluster, multitenancy, and multi-cloud deployments. Customers get consistent, baked-in observability, runtime security and traffic management in any environment.",
      "investors": [
        "Ariel Tseitlin",
        "Eric Anderson",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "Textio",
      "description": "Textio is the inventor of augmented writing. With Textio, your rough ideas are instantly transformed into powerful language with a single keystroke. Building on the words that you type, Textio’s data-fueled predictive engine generates highly effective writing that sounds like you. Textio customers include 25% of the Fortune 100.",
      "investors": [
        "Stacey Bishop",
        "Susan Liu"
      ]
    },
    {
      "name": "Threat Stack",
      "description": "Threat Stack is a cloud native security monitoring platform that enables growth-driven companies to scale with confidence by identifying and verifying insider threats, external attacks and data loss in real-time.",
      "investors": [
        "Ariel Tseitlin"
      ]
    },
    {
      "name": "Treasure Data",
      "description": "Treasure Data provides managed analytics infrastructure as a service, which simplifies data management and is transforming the way organizations collect, store and analyze big data.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "Unbabel",
      "description": "Unbabel believes that language shouldn’t stand in the way of relationships. Powered by advanced AI and refined by a global community of translators, Unbabel’s enterprise translation platform enables customer-centric brands to provide multilingual customer service and experience at scale. The platform is designed and optimized to transform customer contact centers from a cost to profit center, enabling multinational brands to open up and grow new markets.",
      "investors": [
        "Andy Vitus",
        "Susan Liu"
      ]
    },
    {
      "name": "Unifi",
      "description": "Unifi’s Data-as-a-Platform breaks down the barriers of operational data silos and democratizes information across the enterprise. At the heart of the platform is a comprehensive suite of self-service data catalog and preparation tools to empower business users. Employing machine learning and artificial intelligence technologies, and optimized for the cloud, Unifi predicts what the business user wants to visualize and then connects the resulting data natively to the BI tool for fast, accurate results.",
      "investors": [
        "Andy Vitus"
      ]
    },
    {
      "name": "Upsolver",
      "description": "Upsolver is the cloud data lake house builder. It enables organizations to combine the economics and openness of data lakes with the performance and ease-of-use of databases. Upsolver makes data lakes accessible for all data practitioners by replacing months of data pipelines engineering with a visual SQL-based interface.",
      "investors": [
        "Ariel Tseitlin",
        "Eric Anderson",
        "Noah Gross"
      ]
    },
    {
      "name": "Vantage",
      "description": "Vantage is a FinOps platform that allows engineering teams to manage and optimize cloud costs in conjunction with their finance and operations counterparts. Vantage’s all-in-one platform supports organizations through every step of their cloud cost management journey. Through a host of best-of-breed features, the company has attracted hundreds of customers across a wide variety of industries.",
      "investors": [
        "Ariel Tseitlin",
        "John Gianakopoulos"
      ]
    },
    {
      "name": "VergeSense",
      "description": "VergeSense is the leading workplace analytics platform trusted by enterprises around the world to transform their static office into a dynamic workplace. Powered by intelligent sensors and AI-driven insights, customers rely on VergeSenseto identify opportunities to reduce or reinvest real estate and create spaces that employees need and value. The impact is improved culture, collaboration, employee retention, innovation, and growth.",
      "investors": [
        "Rory O'Driscoll",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "Verusen",
      "description": "Verusen is a Supply Chain Intelligence company that simplifies the data complexity of materials management, creating resilient, connected supply networks. Verusen's AI provides complex global supply chains Material Truth for data, inventory optimization, and procurement intelligence.",
      "investors": [
        "Stacey Bishop",
        "Jeremy Kaufmann",
        "Max Abram"
      ]
    },
    {
      "name": "Vitrue",
      "description": "Vitrue is the leading provider of social media publishing software, offering software-as-a-service (SaaS) solutions to help brands and agencies harness the marketing potential of social media and manage all of their social connections. Acquired by Oracle in 2012",
      "investors": [
        "Stacey Bishop"
      ]
    },
    {
      "name": "Viz.ai",
      "description": "Viz.ai is a leader in applied artificial intelligence in healthcare. Viz.ai’s mission is to fundamentally improve how healthcare is delivered through intelligent software that reduces time to treatment, improves access to care, and increases the speed of diffusion of medical innovation.",
      "investors": [
        "Rory O'Driscoll",
        "Jeremy Kaufmann"
      ]
    },
    {
      "name": "WalkMe",
      "description": "WalkMe pioneered the Digital Adoption Platform for accelerated user onboarding that lets businesses simplify their online experiences and eliminate user confusion.",
      "investors": [
        "Rory O'Driscoll"
      ]
    },
    {
      "name": "Wrike",
      "description": "Wrike is a work management and collaboration platform used by high-performance teams everywhere. Wrike makes day-to-day work easier, more transparent and efficient for thousands of companies.",
      "investors": [
        "Rory O'Driscoll",
        "Susan Liu"
      ]
    }
  ],
  "blog_posts": [
    {
      "title": "The Liquidation Survival Guide and Model: Vol 1",
      "body": "The Perfect Storm: Inflated Valuations + Scared Investors = Lots of Structure We all now look back on 2021 as a euphoric time period where capital felt free and money was thrown around gleefully. Over the last year the valuation bubble burst, the chaos in the banking sector with SVB and First Republic Bank happened, and inflation and the cost of capital have risen dramatically. All these factors and many more have created a much tighter fundraising environment for startups. The same VCs that were throwing money at startups and encouraging growth at all costs are now sitting on the sidelines and telling those same startups that they are overvalued and too inefficient. As a result of this changing market, entrepreneurs have deferred their next raise, hoping to grow into their lofty valuations or that markets will return to their previous exuberant state. But for some startups, the time for deferring the next raise has run out. Some companies may be in a strong position for an up round or flat round if they grew into their previous lofty valuation or took a more conservative valuation in the past. But difficult choices lay ahead for those who have found themselves with a valuation that is now seen as too rich. They could take a down round or recapitalize their company to have a more reasonable valuation. But many instead opt to keep the public headline price of a flat or up round while structure is added behind the scenes. There have been lots of articles and posts about terms, definitions, high-level scenarios, and why structure is scary. But as a firm, we have struggled to find a comprehensive resource that shows how to actually build these complicated analyses. Instead, we have passed down this tribal knowledge over time. The goal of this guide is to help entrepreneurs, venture capitalists, and any others who are entangled in the nitty gritty details of modeling these complex situations. For entrepreneurs raising new rounds and finding themselves with multiple term sheets with diverging valuations and structure, this guide and template will be crucial to understanding how each different scenario affects the bottom-line return to each class of shareholders during an eventual exit. For instance, you may get a lower valuation or even a down round, but that could actually provide better returns for you than a higher valuation with structure. This guide is not for the novice, and if you need a refresher or a more general overview of common terms, check out this great article by our venture peer Charles Yu. For companies currently going through an M&A; process, this guide will be more practical than theoretical. This is not meant to be exhaustive but to provide you with a core level of understanding and a foundational template that can be modified to handle any scenario that comes your way. Over the course of 5 blog posts, we’re going to discuss 4 primary Liquidation Preference Scenarios and will have an associated Excel file completed for each. In the fifth blog post, an ‘all-in-one’ automated template will be attached to assist your analyses moving forward. The 5 posts will cover: Non-Participating Liquidation Preference without Seniority (this post) Non-Participating Liquidation Preference with Seniority Participating Liquidation Preference without a Cap Participating Liquidation Preference with a Cap The all-in-one Liquidation Preference Template and Other Terms to Watch Out For During the first four blog posts, we walk through each scenario, the logic behind them, and the actual calculations that need to be done. As you will see, each scenario requires a lot of specific formulas, adjustments, and some manual calculations, especially when scenarios get mixed. In the fifth blog post, we will share a fully automated template with an instructional guide on how to use it. There are, of course, an unlimited number of scenarios and edge cases where you will need to add additional components to it or may break it. However, the hope is that this “all-in-one” template is a strong starting point for any analysis and will save others time and avoid inaccuracies. We’ve thoroughly tested it internally, but very much encourage others to try and report bugs back to us. We'll continue to update the model as we get feedback in an effort to help other VCs, entrepreneurs, and anybody else who is tasked with doing these analyses! So let’s get started! Liquidation Preference Survival Guide Vol 1: Non-Participating Liquidation Preference without Seniority Let’s start with the most simple and foundational scenario. This is the absolute base case and cleanest term sheet. Here, we are assuming a liquidation analysis with the underlying Preferred series having Non-Participating Preferred, no Seniority, and Pari Passu (investors share proceeds pro rata to capital committed in the event that there are not enough proceeds to fully cover all investors’ preference). If you need a refresher on any of those terms, Charles Yu does a great job explaining them. The underlying question when doing this analysis is (for each series): do they get more proceeds by taking their preference or by converting to common? We’re going to walk through how to find that answer across a broad spectrum of Exit Prices. Many of the concepts we’ll walk through here will be essential for future, more complicated scenarios and you will see incremental layers of complexity added to these core concepts. With all of these analyses, the first step is transferring information from the legal documents and carefully noting what type of equity and terms each series is entitled to. The Excel template for this example can be downloaded below: Inserting the Key Inputs All of the below inputs can be found in the company’s latest Cap Table or Term Sheet. If you’re following along in the model, the first step is to insert the dollars invested by each shareholder for each round in the first table below. The bottom table will automatically calculate the % ownership for each shareholder in each series. For instance, if Scale Venture Partners leads the series A by investing $10M in the round out of $11M and then participates in the later rounds bringing their total investment to $24M, you will then see in the top table how much Scale owns of each of the rounds. Then, insert the number of preferred shares and Issued share price (Issued PPS) for each series, noting its liquidation preference (in this scenario it’s 1x) and the preferred shares to common conversion ratio (also 1x in this scenario). It helps in these analyses to have the series ordered by lowest share price to highest share price, but not necessary. It’s important to include the options along with common at the bottom. Note that our analysis does a small simplification here. In reality, options tend to have an associated strike price attached to them. Instead, we are treating the options as equivalent to common in this analysis. It shouldn’t have significant ramifications and if anything is a bit more conservative in terms of the returns to others. Finding the Conversion Point The most important point for any series is the conversion point. This is when the series decides they prefer to convert to common and leave behind any preference they had. For each series, you can think about this point on a price-per-share basis (PPS) or on a total consideration basis. To be more explicit, the series may convert at $5.00 per share which equates to some amount of consideration to equity holders, for instance, $500M. The PPS conversion point is actually fairly easy to find, the total consideration basis is much more difficult. Before calculating this point, we need to first calculate a few other data points: Price-per-share Conversion Point = Issued PPS X Liquidation Preference Multiple / Preferred to Common Ratio. One way to think about this is for a series to convert, they need to get more returns from common than their preference. If Scale Venture Partners buys series A shares at $1.06 per share, then for them to at least get their 1x, the PPS needs to be at $1.06. Liquidation preference multiple of more than 1x increases how much preference you have which raises the hurdle for you to convert. If you have a liquidation preference multiple of 2x, then you would need the share price to be double what you bought it at in order to hit your 2x of preference and for you to convert. The Preferred to Common Ratio has the opposite effect. When this ratio is greater than 1x it increases the amount of common you get so the hurdle to convert is lower. If you have a 2x Preferred to Common Ratio, then you would need only half the share price you initially paid in order to reach the same amount of preference and thus to convert to common. Common Shares Equivalent = Preferred to Common Ratio X # Preferred Shares. If the ratio is 1x, then a series gets the same number of common shares as they had preferred shares. This ratio is typically 1x, but it could be whatever is negotiated (less than or greater than 1x). We talked about in the last point how this ratio affects the conversion point. Preference Amount = Share price X Preferred Share count X Liquidation Preference Multiple. When the liquidation preference multiple is 1x, then it is just the amount of money invested by that series. Entrepreneurs want this multiple as low as possible, while investors would love to increase downside protection by increasing this. % of Total Preference = Preference per Series / Total Preference. Ownership of Common In the last point, we calculated the remaining preference. Now we are going to use a similar methodology to figure out for each series when they convert (at their PPS at Conversion Point), what % of the converted common they will own. If the series are ordered from lowest PPS at conversion from the bottom to highest PPS at the top, then the series will convert in order. Using the series A example again, if the PPS at Conversion Point is $1.06, then all series with a lower PPS at Conversion Point will have converted and be a part of the common share count. For the example above, this means that the 3 Seed rounds and the series A will have all converted at the series A PPS at convert. The series A ownership of common will be the series A’s Common Shares Equivalent divided by the Common and options + the 3 seed rounds + the series A. In the template, this logic is also built into a formula. Point at Conversion ($000) for each series = Remaining Preference + (series Preference amount / Ownership of Common). Thinking about this logically, if each series has the option to either take their preference or convert to common - it makes sense that - if there was no other preference taken - then this series would convert at the point where their preference is less than their ownership of common. To calculate this, it’s simply their preference divided by their ownership of common stock. You then have to take into account that this may not be the last series to convert and there could be remaining preference still, which is why we add the remaining preference on top. An increasing amount of preference above a series logically delays when they convert to common. Enterprise vs. Equity Value When a company is acquired the publicized price is typically the Enterprise Value of the business, or the total value of the business to all shareholders. The amount of money that actually goes to equity investors and management is the Equity Value of the Company. Equity Value = Enterprise Value - Debt + Cash Most M&A; or IPO processes typically involve some amount of transaction fees. The money to the equity holders is after these fees. For ease of example, the model currently assumes no fees, no debt, and no cash. This is not realistic but allows us to work with round numbers. Proceeds Distribution Preference taken: Now for the fun part — the distribution of equity proceeds. The proceeds first go towards meeting the preference of the investors. This refers back to the purpose of preference. These preferred equity holders get paid back before money goes to the common as part of a risk mitigation strategy. Remember - this is the absolute base case and cleanest term sheet. Here, we are assuming no Seniority, Non-Participating Preferred, and Pari Passu. For each series, the test is: do they get more proceeds by taking their preference or by converting to common? Remember, this is non-participating preferred and so each series has to make a choice of converting to common or taking their preference. The formula can be broken down into 3 components: Is the remaining consideration less than the total preference? If so, then the rational thing for each series to do is to take their preference. For Pari Passu, that would be their % of the preference stack X remaining consideration. If the above is false, and the remaining consideration is greater than the conversion point we calculated for each series above, then the rational decision is for the series to convert to common to get more consideration than their preference amount. Lastly, if both of the above conditions are false, where there is enough consideration for each series to get their full preference but not enough consideration for the series to convert to common, then the rational decision is to not convert to common and the series get their full preference amount. Converted Common Shares Above, we deciphered whether each series is taking their preference and what amount they get. Now, we need to figure out the common shares converted for each series. This logic is less convoluted: If the initial remaining consideration before preference is greater than the conversion point for each series, then the rational decision is for the series to convert and the number of shares is the number of common shares each series is entitled to. Remember - we calculated the Common Shares Equivalent above, make sure you reference that field and not the original preferred shares field just in case the preferred to common ratio is not 1:1. If the remaining consideration is less, then the share count is 0 for that series because they will not convert and will instead take their preference above. Common Proceeds For entrepreneurs reading this, this is where things get exciting for you and, typically, your common shares. We’ve already calculated above how many shares convert to common for each series. Now we just have to see what percent of the total common shares converted that represents. Then, to calculate the common proceeds for each series, take the percentage of common shares that they have and multiply it by the remaining consideration to common after the preference has been taken. Total Proceeds and Investor Distribution Total proceeds are simply Proceeds that go to Preference + Proceeds that go to Common for each series. As a test, the sum of all the proceeds to all the series should equal the initial remaining consideration after fees have been taken out. If this is not true, something is wrong! To find the portion of proceeds to each shareholder, take their percent ownership of each series, times the proceeds for each series, and sum that up. For example, for Scale Venture Partners - they invested in the series A and participated in all rounds after. So to calculate their returns, we take their percent ownership of each series and multiply it by the proceeds to each series and sum it all up. Instead of having a messy formula, you can do a SUMPRODUCT formula taking the investor’s percent ownership of each series and the proceeds to each series. Lastly, if you’re an investor, you will care about what multiple you get on your investment which you can calculate on a per-series basis or overall per investor. It is simply the total proceeds divided by the capital for either the series or the investor. Below is a simple summary chart showing for each series their returns as the Exit Value increases. This concludes Volume 1 of our Liquidation Survival Guide. Here's one more link to the template. I hope you’ve found it useful. Please remember, the template is a tool, and please work with your advisors as you use it. If you have any comments, questions, or have found any issues with the template, please let me know! This is an effort to help the startup community better understand these analyses and save everyone time. We will periodically update the guide and template if any issues arise."
    },
    {
      "title": "Cognitive Collective: RLHF Is Not a Magic Wand for Alignment",
      "body": "The world of Generative AI is moving fast. We've found the best way to keep up with the change is to go directly to AI experts and hear their thoughts, free of hype, hyperbole, and marketing spin. So, in this first of many posts, we invite the best minds in AI to share their perspectives and experience in this exciting field. Christopher Rytting is a 5th-year Ph.D student in computer science at Brigham Young University. He studies large pre-trained language models' ability to simulate humans, both for direct study and as research aids in social science. By Christopher Michael Rytting Language Models trained via Reinforcement Learning from Human Feedback (also known as RLHF) are having their time in the academic sun after ChatGPT made its very flashy splash debut during machine learning’s biggest conference, NeurIPS 2022. There is talk of ChatGPT replacing Google, Twitter is athrob with claims of the end of prompt engineering via some general alignment in GPT-4, and lots of the prospective credit is going to RLHF. This hype feels like a fever, verging on the mystic, and I would like to both explore and dispel it. Why we love RL We can begin with the paper Reinforcement Learning: A Survey - published in the Journal of Artificial Intelligence Research over a quarter-century ago–that summarizes reinforcement learning and calls its promise “beguiling–a way of programming agents by reward and punishment without needing to specify how the task is to be achieved.” The idea that reward and punishment alone could get us to artificial intelligence is attractive. Why? Because it resembles a very popular interpretation of natural selection itself, that we ourselves–and all intelligent life forms–were programmed via nothing but reward and punishment in the ultimate pursuit of procreation. The theory goes that every characteristic of evolved creatures (their intelligence, their joy, their pain, every scrap of performance or experience) stems from genetic variations more or less conducive to each creature’s reproductivity. Reinforcement learning invokes the sacrosanct elegance of the evolutionary myth to justify its own promise. Another reason RL is so compelling is that it can be seen as a rejection of the scientific intuition that systems should be–even can be–understood, a rejection that many would consider warranted by the last fifty years of AI research. Observing and considering phenomena have, historically in science, revealed causal mechanisms. Students of those phenomena can then understand and, in some cases, control them, as with nitrogen boosting crop growth or lift flying an airplane. This intuition, call it the value of understanding, was a designing principle of the symbolic, good old-fashioned AI (GOFAI) of the twentieth century. We would think about thinking, profile it, and teach our agents accordingly by writing their minds, rule by rule. However, excitement gave way to weariness as any terminus to this growing body of rules stayed out of sight, always blocked by a long line of edge cases precluding any notion of robust “intelligence.” The failures of GOFAI exhausted us, funding dried up, and an AI winter fell. What snatched us from that period was a change of approach, away from human-crafted and towards data-trained. In domain after domain (most notably games, natural language processing, and vision), training AI how to do via simulation and real-world data - beat training AI what to do via writing rules and logic. In 2019 (even before GPT-3, maybe the most striking example of this minimal supervision approach) Rich Sutton, one of RL’s central figures, summed up this shift and published The Bitter Lesson. The very title captures how difficult it was for scientists to accept their proper role, suppressing their deepest impulse (to understand) and ceding control to the learning algorithms that they would barely design before unleashing on data. An analogous piece by DeepMind is named Reward is Enough, a phrase that you can imagine either exclaimed or sighed. Reinforcement Learning then, can stand conceptually, almost ideologically, for evolution, or for what worked over what did not, despite our wishes and priors. Rejecting or questioning its value on the merits can feel like heresy–even though it is famously finicky, unstable, difficult, and thus far incapable of producing generally intelligent agents. RL in Natural Language Processing One place where Reinforcement Learning has worked well is in Natural Language Processing (NLP). A language model can be considered a policy function–taking text as input and outputting text that follows. A different language model can be trained as a “reward” model–identifying the better of two completions–using human feedback. These two can be paired and unleashed (the policy generating sets of candidate texts, the reward model identifying the better of the two, and the policy being encouraged accordingly), generating and grading in tandem with the ebb and flow of reward. Because we’ve called this reinforcement learning, and because there are two models playing off each other, this can evoke several powerful memories. For example, it resembles AlphaZero–descended from the system that taught itself to play the game Go and defeated the world’s top player, Lee Sedol. Or it can bring to mind generative adversarial networks (GANs), where one of two dueling models is responsible for generating realistic images and fooling the other model, a discriminator trained to tell real from fake. Human absence in these training regimes, which have resulted in some of AI’s greatest achievements, can feel like an eerie portent of the singularity. There are some sensible reasons (with more on the way, I’d imagine) that RL might work better in the regime of human feedback than in broader RL, and best in conjunction with Large Language Models (LLMs). Models don’t have to exhaust the search space before finding sparse rewards, but rather rely on human cues about which search directions are most promising. This is especially interesting with respect to The Bitter Lesson, because one potential takeaway from that piece (perhaps crude) is that human intervention is bad. But in the case of RLHF, maybe the more refined takeaway is that models must learn for themselves as gently steered by rather than entirely built by human designers. Furthermore, these RL models don’t start at all from scratch, since they not only begin with a pre-trained language model, but a fine-tuned one. I’m confident that a fine-tuned model is necessary; training with RLHF from a vanilla LLM would have been a neater (and thus more compelling) story than that of all the recent successes, which first finetune a policy before doing anything with a reward model. Philosophical and Practical Problems However, I’m afraid that the broader mystique around RL, along with the recent concrete successes of RLHF, will lull us into complacency on important alignment and approach questions. There are both fundamental philosophical reasons and more practical reasons for this. First, we don’t know how to define intelligence, consciousness, alignment, or even truth. Even the team at OpenAI acknowledges this limitation, saying “During RL training, there’s currently no source of truth.” Many AI practitioners, and, indeed, anyone who wants to accomplish anything concrete in the world before perfectly understanding it, might reasonably roll their eyes at my making this point. But so long as we don’t have definitions for these things, we cannot speak about models possessing them or not. I don’t mean this normatively, but descriptively. Any claims about “alignment” in a model must be balanced upon the unwieldiness of the term. Alignment means something like “does what we want.” When a model could do what we want but hasn’t been properly coaxed into doing so, we call it misaligned. This is a big field of study since LLMs are both powerful and wild, exhibiting promise but misbehaving all the time. But when pursuing that fashionable goal, we should acknowledge that humans are themselves unaligned–in preferences, in behavior, in ideology. They are also dynamic, hour to hour and year to year. So we must ask: who and when and what is the target for alignment? Someone, be it the C-suite or a team of research scientists, will have to answer that question by choosing–from many possible candidates–a set of priorities, a formalized objective, and a training regime. From there, an actual training plan will be hatched. If the plan’s target for alignment is an open question, then surely things get more complicated in the plan’s execution, which involves actual labelers churning out the Human Feedback part of RLHF. Do we know who these humans are, and how they will steer the ship? Are they trustworthy, qualified to supervise the model, and willing to do so carefully? What is the difference between the cohort hired to train ChatGPT and the one from GopherCite, or between either of these now versus six months from now? Somehow, we’re satisfied to simply call them humans. Each of them is a person who might be what you would call smart or dumb, focused or distracted, grounded or excitable, nuanced or incautious, and will continue changing indefinitely. Yet papers describing RLHF will only refer to them as humans, a homogenous group of soldiers fit for the duty of ranking text completions. A good example of all this is InstructGPT, the predecessor to ChatGPT. OpenAI says they decided to “prioritize helpfulness to the user… However, in our final evaluations we asked labelers prioritize [sic] truthfulness and harmlessness (since this is what we really care about).” Of course, they easily could have prioritized many other sets of characteristics and the objective that would have entailed. Given this one that they happened to choose, though, they trained 40 human data labelers to score completions accordingly. Once the training is out of the hands of the architects and into the hands of the labelers, each labeler will have their own definitions of what is “true” (e.g., whether Donald Trump won/lost the 2020 election), what is “helpful” (e.g., liking or not liking some generated take on the ills of 2023 social media, or lack thereof), what is “harmless” (e.g., taking offense–or not–to a generated slur). None of this is meant to say that OpenAI did the wrong thing, or even that they could have done better, in training these labelers; they realize, instead, that there is no such thing as alignment to “broader human values.” In this sense, no model could ever be trained with the same RLHF twice - even when the training configuration looks the same. I wonder (in full speculation) whether we trust the human in the loop because somehow, madly, we fantasize our own selves into the trainers’ chairs and minds. Whatever the reasons we like RLHF, it probably has to do with the fact that “human” is in the name. This suggests an adult being present who before was absent, as if they–we–are taking back the wheel of a self-driving car on the cusp of a crash. But the truth is that humans were driving all along, despite ideas you might get from The Bitter Lesson that we backed ourselves out of the process. Besides humans needing to have developed the transformer architecture in the first place and tuned its hyperparameters just right, every token of natural language in any GPT-X training set was written by a human being. Evaluating models one at a time So what does all this mean? For one, I’m not suggesting that we need to do away with any ambiguity in the definitions of our words before we can make progress on the problems they describe. Nor am I suggesting that we need to describe every detail about the state of the world in our papers. These are impractical, if not intractable propositions. But it is nearly impossible to define alignment, let alone achieve it. Human beings, in all their variance, capacity, and folly, make up the entire training pipeline, and we take such comfort in the fact that we call them all by the same name. But every human is individual, and models will change as their trainers do. Thus, they will succeed and break differently. As such, anyone using these models needs to find a balance between (a) simply accepting the risk that a candidate model will mess up and (b) evaluating a candidate model as deeply as possible. This is true in all of machine learning, and a big point of this essay is that RLHF won’t change that. Evaluating a model deeply means reading the papers, understanding how models were trained, how they were tested, what the results were, and the other details that will have bearing on their performance and generalization. For example, with InstructGPT, it’s worth going to the appendix and noting that there were 40 coders who were “quite young (75% less than 35 years old), fairly balanced between male and female genders, and mostly come from the US or Southeast Asia”. If you consider that a younger generation has a different idea than an older generation about what makes a text harmful, truthful, or helpful, then you might be more nervous deploying that model to an older group of users. Or if you consider obtaining a terminal degree to be important for some task–say, conducting literature reviews or answering questions about academic papers–then you should know that 0% of the labelers had a Ph.D. To use models, or talk about them authoritatively enough to get by with the vast majority of entrepreneurs or investors, is much easier than deeply understanding their training methodologies. The latter takes humility, hard work, and patience; insomuch as it’s even possible, it’s required for assessing or estimating performance, generalization, and robustness of these models. In this sense, RLHF looks a lot more like the rest of love, business, and life itself than “general alignment” or “the end of prompt engineering.” But it does make quite the chatbot!"
    },
    {
      "title": "These misconceptions about vertical software need to fade away",
      "body": "Time and time again in my career in venture, I’ve been surprised at the negative attitude I’ve encountered from VCs at the thought of investing in vertical SaaS. Many of the largest public software companies operate in vertical segments, and yet, I’ve encountered countless investors who - in their undying quest for a piece of the next Slack or Uber - treat massive industries like healthcare or logistics like they’re too small for investment. My experience is exactly the opposite. Segment-specific startups frequently outperform their horizontal counterparts and do so with greater efficiency, lower churn, and simpler GTM motions. So why the aversion from some investors? My observation is that three wild mischaracterizations drive money away from vertical software, and it’s time that we collectively change our thinking and recognize the opportunity within some of the largest industries in the world. These are the most common objections I see: Objection 1: Vertical = small The most frequent objection to vertical SaaS is simply that the TAM is too small to produce a venture-scale outcome. But the facts just don’t support this. Leading fields like healthcare, insurance, and construction represent huge portions of the economy, and each contains many billion-dollar corporations with hundreds of thousands of employees. Ironically, these verticals represent the majority of American GDP when put together, and in many cases, are the dominant drivers of American innovation and growth. As proof of the opportunity in segment-specific software, here are a few facts to consider: Oracle's Cerner alone does over $5B annually selling hospital software in what some estimate to be a +$30B software market. The largest contractor in the US, Turner, generates over $14B in revenue. If they spent only 5% on technology that would be $700M! Chase spends $14B (11% of revenue) annually on technology Even smaller segments like transportation and agriculture still represent massive opportunities, and these small segments are often the most neglected, meaning that innovative products that bring real value can gain market share quickly. Objection 2: Vertical buyers aren’t motivated enough to drive massive transformation This is a lazy stereotype, and it’s time to put it to bed. The belief that vertical software buyers either aren’t as smart or are unmotivated stems from a complete misunderstanding of the challenges they face. If it feels like technology buyers in these spaces move slowly and resist change, the reason is that the critical institutions they oversee are often built on heavy, legacy systems that contain a lot of technical debt. Ironically, the reason for this technical debt is often that these industries were early adopters, and were among the first to implement emerging digital technologies when they became available decades ago. The scope of replacing these systems is huge, and the cost when an old system breaks or a new one fails to meet its promise is high - especially in a regulated environment where mistakes may result in a congressional hearing (as Southwest Airlines could attest). As a result, buyers carry an enormous amount of stress regarding change and tend to be thoughtful and deliberate. They certainly are not as transactional as a typical sales leader might like. This doesn’t mean that legacy industries aren’t disruptable. But it does mean it takes the right kind of founder to build trust and product market fit - and the right kind of investor to collaborate on realistic growth plans for these fields. For example, Docusign didn’t sign its instrumental deal with the National Association of Realtors until 2009, six years after the company was founded. Objection 3: Vertical markets are difficult to penetrate I wouldn’t necessarily dispute this objection, but rather reframe it. Some verticals are difficult to penetrate because of regulations or other industry-specific factors that raise the barrier to entry. One of the simplest barriers is a shortage of people who have deep knowledge of the problems facing the industry, and therefore the ability to solve them - i.e. founder-market fit. Scale’s investment in Archipelago Analytics was very much driven by founder-market-fit. Founder CEO Hemant Shah had previously founded and run RMS analytics for 20 years, giving him unparalleled domain experience. But the other side of the story, when a market is difficult to enter, is that once a startup successfully gains traction in them, these barriers to entry serve as competitive moats to help defend the business and give them safe breathing room. In addition to competitive moats, the tight networks of buyers within these segments create informal referral networks that drive new business. In short, if a startup creates a positive experience for their first customer, there’s a good chance their next wave of customers is a direct intro away. Horizontal markets are often split in many ways, by customer size, by use case, and by brand. That’s in stark contrast to vertical software, where a vendor has much less competition and therefore an easier opportunity to own a broader customer set. Finally, the difficulty of entering these markets creates opportunities for platformization and upselling. You can look no further than a company like Motive to see how once a startup’s foot is in the door, there are countless ways to become a true industry cloud and grow its TAM by modernizing even more of the stack for that field. Founders get it - it’s time for investors to see the light Founders in vertical SaaS generally come from within the industries they’re building for and bring deep domain expertise with them. The sense I’ve gotten from these founders over the years is frustration with an investment market that is willing to throw 100X multiples at horizontal SaaS while insisting that startups in vertical segments aren’t venture scalable. But vertical SaaS has my attention. In companies like Motive (transportation), Dusty Robotics (construction), Spruce (prop-tech), and Archipelago (insurance) I’ve seen that entrenched founders can meet the needs of skilled buyers and drive real impact - and real revenue. So if you’re a founder in a vertical market, don’t be a stranger. I’m always on the lookout for the next generation of solutions that can bring transformative value to some of the industries where it’s needed most."
    },
    {
      "title": "What’s Up With Open Source LLMs?",
      "body": "By now, we seem to all have accepted that generative AI will drastically change everything. What remains unclear, however, is if OpenAI’s latest release will still seem like the second coming of Christ by the time we reach GPT v1000. Will foundation models become commoditized across a few key providers? Or will most companies be running their own in-house models in the next few years? There are substantial arguments to be made for any of these outcomes. GPT seems to get better in ways we hadn’t even previously thought of with each new release. The fact that GPT4 now performs better than 90% of humans on the LSAT feels both amazing and disturbing. But we have also started to see open-source projects nipping at OpenAI’s heels in what some have called LLM’s “Stable Diffusion Moment.” There are obvious benefits to using an open source model. Privacy and security, affordability, customization, and avoiding lock-in are major considerations for enterprises and areas where open source stands to win. Contingent on their ability to compete reasonably on quality, these factors make open source models hard to ignore. So, to really understand and demonstrate where things stand today, we’ve decided to try several approaches and models ourselves by building an email generator – the use case that seems to resonate with everyone. As thoughtful venture capitalists who highly value our personal interactions with companies (and since I value my employment as an associate), we would never dream of automating our own outreach emails to founders (I promise I’m not kidding, I wake up at dawn every day to email the companies nearest and dearest to my heart, please reply to me). Instead, we’ve decided to build an email generator for founders that replies to outreach from VCs and politely tells us to piss off. Afterall, what am I but a humble servant to our portfolio and prospective companies. The task is simple. You, a founder, receive the usual email from the VC wanting to connect. You just raised, or you just want them to leave you alone right now, so you need to give them the standard “we’re heads down building” or “best to check back in a couple quarters” yadda yadda. Do these emails really require so much thought? Probably not. So, why not build a basic extension to call an LLM that can acknowledge the email, make the VC blush, and send them on their way. Lucky for you, we’ve just built one to reject ourselves. The Method We tried a number of different models and approaches including GPT4, Vicuna, Alpaca, and a fine tuned version of Alpaca. The primary prompt we used was the following: You are a helpful assistant that helps startup founders reply to Venture Capitalists that reach out to them to politely decline requests for meetings. The replies you generate should decline the meeting, notify the venture capitalist that the founder is not fundraising any time soon and that it would be best to check back down the line. Include some personalization in the response and don't make it sound overly formal. Here are a few example responses: Example 1: {...} Example 2: {...} Please generate the response to this email: {….} The examples are randomly selected at runtime from a dataset of the previous rejections I have received (and yes, I did us all a favor by weeding out the ones that have particularly hurt my feelings in the past). This method, as opposed to hard-coding examples, is to add randomization in the case that you are unhappy with the first generation and want to rerun. They usually - *cough* always - look something like this: Example 1: Hey X, appreciate you reaching out! We are very heads down on product at least through the end of this year. Would be happy to circle back when the timing is better on our end. Example 2: Hey X– I think we’re still a few quarters from thinking about fundraising, so let’s check in again in a few months. GPT4 was run via the OpenAI API and the other models were run and/or tuned on a machine with 8 A100 40GB GPUs. Our fine-tuning dataset for Alpaca consisted of approximately 1,000 emails, from both my own inbox and synthetically generated responses. While 1,000 might seem paltry, it was functional for our purposes here. The Results **Disclaimer: in case we have misled you, a reminder that this is not actually a scientific paper. Our results were measured by our eyeballs and our eyeballs only. We know that one email is not indicative of the overall system performance and this is simply for demonstration purposes. In this example scenario, venture capitalist me (Marguerite) reaches out to founder me (Maggie). Founder Maggie is the CEO of the automated VC rejector tool Maggie.ai. GPT4 Unsurprisingly, GPT4 is shockingly good at this. We pass a portion of the original prompt as a system message and the rest as the user. The response has a tone that is friendly, but not excessively so, and it makes subtle references to the original email without robotically repeating it. Vicuna In case you missed it, Vicuna is one of the open-source chatbots that was created by fine-tuning LLaMA on user-shared conversations. Performance here was also good. The personalization isn’t really at the same level, but this is definitely sendable. Here is an example if you append “please make personal references to the original email” to the original prompt: The system does as it is told, but the personalized references are a bit parroting and robotic. Alpaca 7B A little rude and goes a bit rogue at the end there... Fine-tuned Alpaca It's a bit short and still a little curt, but it gets to the point. You can also subtly notice the change in the tone right. BUT this one gets an asterisk – see below. **The quality of responses were a bit erratic, and parts of the original prompt often showed up in the response itself. This was also an issue with the original Alpaca model, but it seemed to be more pronounced here. With some prompt adjustment, and a larger and higher quality dataset, however, getting this to where it needs to be seems feasible. Implications While this was certainly entertaining, it was also an extremely eye-opening activity. There were several key takeaways we came away with: We are off to the races Yes, in many ways, OpenAI is really just that good, and there are two fronts where GPT continues to blow our minds. The first is quality: the ability of GPT4 to follow instructions to a tee, cover a massive breadth of use cases, and reliably output quality prose is nothing short of amazing. The second is ease of use: the barrier to entry GPT has for a developer to get an initial integration into their applications is unbelievably low. POCs have become something that can be knocked out between meetings. Open source is also really freakin’ good. Are these models at the same level as GPT? No. Some might argue that they aren’t nearly as close as we make them out to be. By virtue of the fact that many are trained on the outputs of GPT itself, they in theory have to lag. But, considering Alpaca came onto the scene less than a month ago, it is remarkably impressive. Moreover, the speed of this innovation is far from the only relevant piece here. Alpaca, Vicuna, and many other open source models like GPT–2 and GPTJ are all <100 GB, and many have <10GB versions, which are small enough to run on a standard laptop. Further optimizations like the use of low-rank adaptation (LoRA) and quantization only continue to chip away at compute requirements. Running state-of-the-art LLMs on personal devices (any maybe even the browser!) seem like not such a distant reality. Challenges remain. So do opportunities First of all, managed infrastructure is … nice. Shocking. Getting a proper compute environment was a miserable game that consisted of begging for EC2 limit increases, overly-optimistic bets of running on inadequate amounts of memory, insufficient AWS capacity errors, and spending too much money. Not to mention the deployment and serving requirements you will be responsible for once you actually need these models in production. Obviously, this is probably easier if you are an actual company with a real engineering team and not a semi-washed-up developer turned VC, but I would like to believe that this is at least somewhat challenging for the broader population. In a world where running some form of your own models becomes commonplace, there will be a huge demand for optimized infrastructure and managed solutions. Getting the right data and integrating it properly is key. Alpaca and Vicuna make the importance of instruction tuning clear. They also make the possibility of training customized, subdomain models a more visible reality. But, if you want a small model you can run yourself for something specific (like responding to emails), you will need to get that dataset from somewhere, and the prompt-response format needs to be done well. Fortunately, synthetic data tools have become increasingly available, and many enterprises do sit on enormous amounts of unstructured, proprietary data anyways. Figuring out the best way to incorporate that data is highly dependent on the use case, and far from a determined answer. Remaining questions A world where open source and proprietary models become commonplace requires thinking about a laundry list of additional questions. Some that are particularly front of mind for us: What is the optimal strategy for where information is stored and how it is retrieved? How much do you simply put into the prompt? Is it best to embed it into the model itself by tuning? Is it best to just use a vector database to decouple the process of information retrieval and answer synthesis? When should you let chaining take the reins? For our mini project, it is very easy to simply include everything you need in the prompt, but if you are a law firm using AI to help you work on a case for example, you cannot pass the entirety of federal law into a prompt. What is the golden ratio of performance? Determining the ideal model and information retrieval system will require weighing tradeoffs between size, cost, latency, and performance. Comparing Google’s Bard vs GPT4 is a perfect example. How do you measure the performance of your model and constrain it? The age-old question of LLM testing. If companies build their own models, they assume the responsibility of benchmarking it and making sure it doesn’t go off the rails. All of these questions are much longer discussions. The short answer is that they are all highly dependent on the use case and the various requirements of the company in question. Companies to watch There were more open source projects and hosting platforms than we could possibly have tried ourselves. However, this project made the opportunities for disruption at both the compute and modeling layer abundantly clear. Below is a list of exciting projects and companies we are watching: What now? If you really feel so inclined, you can find the code for the simple GPT4 version of the extension here. This version assumes you have GPT4 access but can easily be changed to use 3.5 if needed. It also hard codes a small set of examples in the prompt in place of the sample dataset since I didn’t feel like publicly exposing examples of every time I get rejected. While I sincerely hope no one will actually be using it to reply to me, building this tool provided an extremely valuable glimpse into the current state of open-source LLMs and the requirements for running one. Yes, GPT is fantastic, but less than two months after the llama weights were leaked, we are already seeing open source models produce output that would have had us jumping out of our seats a year ago. No, our mini project is not revolutionary, exhaustive, or scientifically justified, but if one hobbyist developer can get functional output from these models in a mini POC within a week, it’s hard to believe that we won’t see these models integrated into real business products in the next 12 months. The opportunities for development in this space are immense. References https://hazyresearch.stanford.edu/blog/2023-01-30-ai-linux https://huggingface.co/blog/stackllama https://www.harmdevries.com/post/model-size-vs-compute-overhead/ https://arxiv.org/pdf/2203.15556.pdf https://crfm.stanford.edu/helm/latest/"
    },
    {
      "title": "Peeking under the hood with ChatGPT plugins",
      "body": "OpenAI announced plugins to extend ChatGPT last week, and did so in a surprising way. Many rightly pointed out that these look like “apps” as part of a platform play, with one big exception - there is no lock-in. The way these plugins are designed is as open as I can possibly imagine, which means we can peek under the hood at how they are made. That is, if you know where to look. It also offers us a glimpse into potential business ramifications, which we’ll take a look at at the end of this post. A plugin has two parts: a manifest file, which looks a lot like an app store listing, and an OpenAPI spec (Note: this is OpenAPI not OpenAI). The manifest file is always called ai-plugin.json and stored at the root of the domain of the API being accessed in a folder called .well-known. Notice the name of the file isn’t OpenAI-plugin or ChatGPT-plugin, but AI-plugin. Combine that tidbit with the fact that it is stored in the .well-known folder, which has been a pseudo standard place to put other site configs, and you can see that great care is being placed into being as standard and open as possible. It seems to me, OpenAI wants this to be the standardized way for any and all AI plugins, not just theirs/ChatGPT. In the same way that robots.txt, which is often also stored in the .well-known folder, is the industry standard way to moderate how search engines “plugin” to your site. I expect we’ll see Alpaca/LLaMa support these plugins soon. While OpenAI and its partners don’t publicly publish exactly which domain the manifest is on, we can make an educated guess. I was able to find half of them on my first try and published a list for you. If you find others, you can add them. Manifest files are typically boring and there isn’t much to see here except for the prompts that are used to prime the integration. Wolfram’s is by far the longest one I’ve seen. The prompt provides lots of coaching: When solving any multi-step computational problem, do not send the whole problem at once to getWolframAlphaResults. Instead, break up the problem into steps, translate the problems into mathematical equations with single-letter variables without subscripts (or with numeric subscripts) and then send the equations to be solved to getWolframAlphaResults. Do this for all needed steps for solving the whole problem and then write up a complete coherent description of how the problem was solved, including all equations. I say coaching, but this is programming! That is basically an if-then-else logic statement complete with objects/variables like getWorlframAlphaResults. Herein lies the strange simplicity of the LLM movement: You don’t code. Or you do - but you do it with human-like plain-text communication. An aspect of this new programming is seen in these manifests. Many think human-style programming can’t provide sufficient specificity, and it is true that code is 100% explicit/deterministic. But consider that humans have ways to achieve some determinism in our language. Typically we’ll explain a task to a colleague and follow a general description, which might be misinterpreted, with examples. The Speak plugin does this over and over: “Examples: \\\"how do I say 'do you know what time it is?' politely in German\\\", \\\"say 'do you have any vegetarian dishes?' in spanish\\”. The second part of the plugin is the OpenAPI spec. These can be stored anywhere but are linked in the manifest. Specs like these are not new. They have been around for a decade and are used by developers and other APIs to know what to expect from a given API. Contained is a field that was intended for humans, but Open AI says we can just re-use that for AI-plugins. In theory, that means all the APIs are ChatGPT ready, we just need to slap a manifest pointer on them! But in reality, that isn’t how these plugins are written. Some are writing new specs for their AI-plugins- like Slack’s being called “ai-plugin.yaml” Finally, anyone can develop these plugins today. ChatGPT has a waitlist for developers, but you don’t have to wait. You can host an ai-plugin.json and API specs today. You just can’t point ChatGPT at them, but if Alpaca or another model supported plugins, your definitions would work immediately. To that end, I’ve created this open source catalog of all the AI-Plugins out there, including third party plugins not contained in the announcement like Datasette from Simon Wilson (whose writing about creating that plugin was a big inspiration for this post) I promised a note on the business model, and here's what I can say: OpenAI had a chance to make a walled garden here but didn’t. I speculate the strategy must be to set a standard and make it hard for competitors (Google’s Bard?) to not backtrack their approaches and follow. It also makes it easy for developers to add plugins, which means when the AI “app store” for ChatGPT launches to the public, it will be full of robust and capable plugins out of the gate. The way OpenAI builds ChatGPT plugins suggests they want an open standard. This also means we can inspect the new plugins for tips on building more. You can find the examples mentioned above and more at ai-plugins.xyz"
    },
    {
      "title": "Some reflections on PROMAT 2023: Automation reaching new corners of the warehouse",
      "body": "Last week, I spent a few days in Chicago at PROMAT, which is one of the largest conferences in the US for supply chain and logistics solutions and software. For many companies within this ecosystem, this is the Super Bowl for their go-to-market teams where key customer conversations happen and a meaningful percentage of the year’s sales emerge. Supply chain is an industry where seeing is believing - especially when robotics are involved - and this year’s show was out in full force. The environment was electric (no pun intended): with a roster of over 50,000 supply chain enthusiasts, the conference organizers brought attendance back to levels not seen since the start of the pandemic. And with close to 1,000 exhibitors, including four within the Scale family (Locus Robotics, Plus One Robotics, Spot.ai, and Verusen), any attendee could feel like a visitor at the Louvre where an attempt to stop and see every exhibit is simply a lost cause. The Locus team at the \"Super Bowl\" of supply chain and logistics events. *(Credit: Locus Robotics) Spending time at industry trade shows can be one of the best ways to get a real time sense of what’s happening in a market. The confluence of old and new companies, buyers, industry leaders, and machine builders creates an environment where everyone is within arm’s length of new insights and data points at virtually all times. Especially within the world of robotics and automation, the ability to see dozens of live demos and speak to industry professionals about how these solutions map to the problems they’re facing is invaluable. In 2018, I attended MODEX (PROMAT’s sister conference held every other year in Atlanta) for the first time and then PROMAT the following year in 2019. Since then, in just a short amount of time, there have been noticeable changes that have taken place. What once felt like a conference for warehouse supplies and equipment now has a much greater orientation around software, robotics, and automation - which feels like a harbinger for where the industry is headed. Autonomy is making its way into more corners of the warehouse - from forklift operations, to depalletizing, to picking, packing, and trailer unloading. Several years ago, there was still a palpable skepticism around whether automation was here to stay. That mindset has now shifted: the same skepticism has been replaced by not just acceptance, but a series of questions around which automation solutions will be the right ones to choose. Among the innovative software and robotics solutions for the warehouse, there was a clear focus on platforms, with machine builders trying to create solutions that can help automate multiple job functions within the warehouse. In a world where there is infinite hype around the capabilities of robots, we have observed that viable robotic solutions are infrequently capable of completing more than one or two tasks very well. As such, more and more partnerships are beginning to emerge to tackle multiple use cases within the same site (one such example is Locus Robotics’ recent partnership with Berkshire Grey to offer a cross-platform solution for its end customers). As labor headwinds, wage pressures, and rising rent costs continue, warehouse operators will continue to drive margin improvement by automating the tasks that are expensive, time-intensive, error-prone (or all of the above). Going forward, the question will be whether industry operators will purchase from many different point solutions (as it is today) or whether buyers will be able to look to just one or two full stack vendors to help them automate. Industry experts believe that between 75% - 80% of all warehouses have not yet brought on their first automation solution and that the vast majority of the market still relies singularly on human labor. With more than 160,000 warehouses worldwide, there is still a huge amount of whitespace and a massive opportunity for emerging automation solutions within the industry. I’m eager to watch the developments happen within the ecosystem over the coming year - and if you’re an entrepreneur building software or automation that is bringing more intelligence to the built world, I hope that we can connect at MODEX in 2024. More scenes from PROMAT 2023"
    },
    {
      "title": "Stream processing becomes mainstream",
      "body": "Like counts, a ranked feed, or fraud alerts are examples of user-facing analytics and the result of complex transformations on vast amounts of raw data that capture user activity. Although user-facing analytics are increasingly present across modern applications, most companies still struggle to deliver them in real-time and at scale. As a result, many businesses miss the chance to deliver a personalized user experience and react to changes quickly. Event streaming platforms like Kafka, Pulsar and Kinesis are already able to capture raw real-time data at scale. They have gained widespread adoption in the context of microservices, whose proliferation has made point-to-point communication between systems inefficient. Using event streams to centralize data exchange–while important–only scratches the surface of the value that they can provide. We believe that we are at an inflection point where stream processing will become the standard that powers user-facing analytics. The growing importance of the use cases enabled by stream processing is being met with solutions that are increasingly easy to adopt. As investors in this space, this is a trend we’re following closely. A new architecture for user-facing analytics Traditionally, companies wanting to deliver user-facing analytics have had to compromise by choosing an imperfect architecture. Here are some common examples: The application can query the OLTP database (e.g., PostgreSQL). → Problem: The OLTP database cannot handle this beyond a few records at a time (not scalable). The application can query the data warehouse (e.g., Snowflake). → Problem: The query takes too long to run when dealing with vast amounts of data (high latency). A job (e.g., Spark) can periodically pre-compute values and reverse-ETL the results to a NoSQL database (e.g., Cassandra) for serving. → Problem: Reading from the database is fast, but due to batching the data seen by the user is stale. Although no two organizations are the same, we have observed that companies who deliver best-in-class user-facing analytics have generally adopted an architecture that resembles the following: Let’s go through these components one by one: Ingestion: This layer is responsible for detecting the state changes associated with events and producing them to the event streaming platform. Developers can build ingestion into their application by using low-level APIs or high-level, configurable tools (e.g., event trackers, CDC systems). Event streaming platform: This layer provides a highly-scalable, fault-tolerant system that ingests multiple event streams, stores them, and enables other systems to asynchronously consume them. As mentioned earlier, this is a mature segment with established players like Kafka, Pulsar, Redpanda and Kinesis. Stream processing framework: This layer consumes raw events from the event streaming platform, performs analytics, and produces usable events back to the platform. Stream processing is the focus of this blog post. NoSQL database: This layer enables serving. A common workflow is to compute metrics in the stream processing framework and then perform upserts on a table in the NoSQL database as the events are produced to a stream. This is also a mature segment with big players like MongoDB, DynamoDB and Cassandra. OLAP database: Raw events can be streamed directly into an OLAP database, which is designed to run sub-second aggregations on vast amounts of data. This layer can therefore complement stream processing by enabling use cases where the metrics required by the application are not known in advance, i.e., when it’s better to compute on demand rather than continuously in advance. In many cases, it can also replace the NoSQL database to act as the serving layer. This is a growing segment with powerhouses like Pinot, Druid and ClickHouse. Multiple catalysts are transforming the market The trend towards more user-facing analytics delivered in real time was pioneered by large tech companies, who wanted to deliver better user experiences and react to change quickly. Although real-time architectures have been notoriously hard to adopt, we see a number of catalysts playing which will pave the way to mainstream adoption. First, the current wave of solutions focuses on SQL and Python, which are familiar to the key constituents in the development of user-facing analytics: data scientists. By contrast, the original stream processing frameworks were built on top of JVM languages (Java and Scala). Second, the transition from self-managed deployments to SaaS stream processing platforms has made it possible to adopt these technologies with limited operational pain and at a smaller scale. Previous iterations required significant resources to manage the many moving pieces needed to deliver high availability (e.g., ZooKeeper) and imposed a large minimum footprint to amortize the cost of a solution designed for a very large scale. Additionally, for organizations not willing to adopt SaaS, similar gains have been made possible with the use of Kubernetes. Last (but not least), the introduction of tiered storage across event streaming platforms has decoupled storage and compute, providing infinite retention and eliminating bottlenecks in data retrieval. In the past, companies would have had to use batch (e.g., Spark) for backfilling alongside the real-time layer. Operating two systems and maintaining two codebases (the lambda architecture) was prohibitively complex and expensive. With tiered storage, it’s possible to fully eliminate the batch layer and rely exclusively on a real-time stream processing framework. Mapping the market Developers and data scientists who want to adopt stream processing can follow different approaches depending on their requirements. The most important decision is how the application runs: Libraries simplify the development of stream processing applications by providing higher-level abstractions on top of the low-level produce/consume APIs exposed by event streaming platforms. Among others, in particular, libraries incorporate stateful operators that enable applications to produce aggregates. Under this model, the developer embeds the library into an application which is deployed and scaled by them. Cluster frameworks provide a fixed infrastructure that takes responsibility for persisting state and scheduling applications that execute as long-running jobs. As a result, the operator managing the cluster is responsible for ensuring that it meets the demands of the application. Cluster frameworks themselves tend to fall into two categories: Stream-to-sink frameworks do not persist data within the cluster itself (other than internal state). In this model, output is piped back into the event stream or, in some cases, a different sink (e.g., blob storage, data warehouse). An independent serving layer (e.g., NoSQL database) is required. Stream-to-table frameworks also create tables (or materialized views) that can be queried externally. Depending on the implementation, tables might be stored only in memory or persisted to disk. This doesn’t preclude the developer from also outputting to a sink and/or optionally adopting a standalone serving layer. Despite these rather clear-cut definitions, the truth is that many stream processing solutions do not strictly fall under one definition–within each category, there are also major differences between the different implementations and providers. This “cambrian explosion” of stream processing architectures suggests that many designs are being evaluated against different customer requirements, which will lead to exciting progress in the space. For the purposes of this blog post, we have categorized existing products and vendors as follows: The future of stream processing There are two key reasons for founders and investors to be excited about the stream processing space: The first is that event streaming platforms like Kafka solve stock (storage) and flow (network) problems; but stream processing provides analytical capabilities (compute). Because the resources spent on compute tend to be larger, we think that stream processing has the potential to be an even bigger market. The second reason is that event streaming platforms expose standard produce/consume APIs, which enables stream processors to be developed independently. This decoupling means that it is possible to build a stream processing system that works with the various event streaming platforms. As a result, stream processing systems can be developed as standalone products, leaving room for new entrants and innovation. We’re really excited about how the next generation of stream processing providers is unlocking access to real-time user analytics. If you’re a builder in this space and like our thesis, we’d love to hear from you!"
    },
    {
      "title": "Fixing Your Cost Structure While Preserving Your Upside",
      "body": "Earlier this week I had the chance to sit down with Box CFO Dylan Smith and discuss how Box was able to dramatically improve their cost structure while continuing to innovate and grow in the face of tough competition. The learnings Dylan shared from the journey Box embarked on in 2020 are broadly applicable to the challenges tech companies are facing going into 2023. I invite you to listen to the recording of our conversation above. A couple of quick highlights: In late 2019 Box had a clear growth strategy based on customer feedback, but at the same time had to significantly improve their operating profile and buy time for the strategy to take effect. Box turned around from growth of 8% and barely breakeven operating margin in 2020 to 17% growth and 23% operating profit in 2022. Clear scenario planning with the Board and management team was key to gaining trust and confidence (e.g., “if we only get $X in NNARR we’ll do Y, otherwise we’ll do Z”). Box cut overall headcount by ~10%, largely driven by rigorous sales performance management on the GTM side and focus on sales efficiency. Offshoring roles to Eastern Europe had little cost savings after accounting for travel and other costs, but was a significant talent lever. Box limited compensation increases while restructuring their cost profile. Retrospectively, this led to some unwelcome attrition of top performers - which could have been addressed by cutting headcount a bit more while keeping merit-based comp increases. Box adopted a “coverage vs attainment” framework for tracking sales rep productivity, which was used quarterly with both the board and management to track performance improvement: You can find more details around Box’s journey in this blog post. Also, remember to check out our Annual Planning Hub for more helpful tools and resources."
    },
    {
      "title": "This Is What Winning In SaaS Looks Like In 2023",
      "body": "The Box team has always done things early, and in 2020 Box went through a transition that most SaaS companies are about to go through in 2023. After more than a decade of explosive growth, with cheap capital to finance that growth, Box faced a slowing business environment with increased competition. Revenue growth declined from 27% in 2017 to 11% in 2020. At the same time there was increasing pressure from shareholders, including activists, to get profitable quickly. The challenge for Box was not just about getting to profitability. The real challenge was continuing to invest in building a compelling enterprise-grade product while at the same time making progress towards best-in-class profitability. How Box handled that pressure can be a model for what most CEOs and CFOs are dealing with in 2023. In late 2022, I talked to Dylan Smith, Box’s CFO since its founding in 2005. In the webinar, we focused on the practical steps Box took on the cost side to get from an operating loss of 11% of revenue in 2017 to an estimated operating profit of 23% of revenue in 2022 and profitable on a GAAP basis. We talked about what they chose to cut and what they chose to keep and how they made those decisions. We discussed what worked and what didn’t. Hopefully some ideas will help teams wrestling with the same dynamics in 2023. For Box the good news is it worked. The chart above shows the stock price from Jan 1st 2020 to today. The company underperformed the market in 2020, as it had to focus on profitability while many companies were benefiting from COVID revenue tailwinds. However, as the markets pivoted to valuing profitability, or more accurately, since the market started hating losses, Box stock has outperformed its peers. The rest of this post is some context from my perspective as a board member of Box from 2010 through to 2020,what I learned from that journey, and how it has influenced my thinking on other deals in 2023. I am long off the board and don't speak for management but this is what I took from that experience. On Monday we will see what Dylan thinks! Box's Journey: 2010 to Present Box went all in for growth in the period starting 2010. There were a slew of competitors in the enterprise file sharing market and the risk of Microsoft getting there was ever present. The only way to win was to get big fast and we did. From 2010 and to 2016 the CAGR was 87% and the company grew from $5M run rate to over $400M. Growth rate went up, efficiency went down but we established the enterprise management category, got public and became the winner in that category. Most of our early competitors failed or were acquired for small dollar sums. Over time growth rates began to decline, not just in absolute terms, but relative to other similarly sized companies. With increased competition it was getting harder for Box to generate growth and that growth was costing more. At some point that model doesn’t work and the period from about 2018 to 2019 was the hardest. The company still wanted to be a high-growth company but the return on investment just was not there. Box was stuck in \"no man's land\" with the expense profile of a high-growth company but that growth was on the come. Basically we were in the middle of a strategic transition but our financial profile had to change for us to survive through that transition. In the 2020 to 2022 time period that is what the Company did. Box was barely break even in 2019 and today the company is tracking to an operating profit of 23% of revenue for 2022. Growth dipped to a low of 8% in 2020 and is currently running at about 17% a year (constant currency) at a revenue run rate of $1Bn. This financial performance was only possible because of the continued evolution of the product, and the investment in building the features enterprises value most. Most impressively, Box is still an independent founder-run public company that has outperformed the market since the SaaS crash, is profitable and has control of its own destiny. That is a fate a lot of SaaS founders would like to share in 2023. This simple chart shows the entire journey. It shows growth rate (relative to similar sized peers) on the y axis and burn multiple (or eventually profitability) also relative to similar sized peers on the x axis (check our tool here to plot your own path). The entire journey is clear, from explosive growth, to de-acceleration to profitable and re-accelerating growth (using analyst estimates for 2023). I will say it again, this is what winning looks like in SaaS in 2023. Lessons Learned Some lessons that I learned as a board member from this journey are: When the facts change, you gotta change. As a board member who had been part of the hyper-growth part of the journey I was slow to realize those days were over. There was a temptation to give it one last try to see if growth could come back but it didn’t, and that consumed resources that we could have used later. New voices around the table can help. We recruited some amazing new board members who came unencumbered by the past and forced us to address cost issues that were long in the making. From the outside we had lots of forceful comments from activist shareholders. I didn't always agree with their solutions but they were right to call attention to the problem. Driven adaptive management teams, especially founder-led teams, are a company’s superpower. The ability of the founding team at Box, led by Aaron and Dylan, to adapt and play a different game is what made the difference. It is very hard to go from a growth at all costs mindset to pinching the pennies. It sucks to be unwinding investments you made only a year ago but they did it. At the same time, they held fast to what matters and continued to invest for the upside. The founding team believed in the opportunity for a standalone enterprise content management company that could compete and win against the largest software company in the world. The evidence today says they are right. To be that company, Box continued to invest in R&D; and Customer Success throughout the entire period to build a differentiated product. They cut Sales and Marketing spend in absolute terms while broadly holding R&D; flat but making it far more efficient. The results speak for themselves. When I hit save on this blog post it will save to Box. It will not save to some bland embedded product in a Big Tech product suite mish-mash. For that, I will smile and think fond thoughts of the whole Box team. I believe the Box experience in 2020 will be the universal SaaS experience in 2023. Growth is going to slow across the board and every company will have to dust out the “get profitable while preserving the upside” playbook. Hopefully these learnings from Box can help. All views above are my own and not representative of Box, Inc."
    },
    {
      "title": "Some Thoughts on Annual Planning 2023",
      "body": "Last week we hosted an event for our portfolio CFOs, where Scale Partner Rory O'Driscoll talked through some of the key considerations when approaching annual planning this year, especially in the context of today's volatile market conditions. In addition to the video recording of Rory's talk above, you can view the slides that were presented below. A couple of quick highlights: The CFO Dilemma this year is balancing valuation and cash while preserving the upside and dealing with economic uncertainty Q3 was a tough quarter, but there was variance across individual companies, with some being hit hard while others did OK Don't limit your upside by being overly conservative in planning for next year. That said, keep a Plan B in your pocket in case the downturn continues to get worse Depending on your customer base, there's a possibility for a churn explosion next year that you should factor into your plan Going into 2023, executives need to be on the same page with the Board on the plan for next year. To aid this, it's important to be aligned internally and ground your Board in the high-level facts Remember to check out our Annual Planning Hub for more helpful tools and resources."
    },
    {
      "title": "Generative AI Index Use Case Glossary",
      "body": "A quick glossary of our Generative AI Index use case definitions Accent Modulation & Voice Augmentation Some products focus on normalizing more distinct accents. Others have a broader focus on enabling a voice to be changed in a broad range of ways (e.g. make a male voice sound female). Ad Generation & Brand Building Products helping early stage brands further develop their brand identity. This includes AI generation of ads, of derivative marketing collateral, and of PR-style story pitches for publicity. AI Editing for Video, Photos, and Audio Companies are introducing AI to the editing process in myriad ways. For example, some products enable video editing from text commands or from a transcript, and others introduce diffusion models to a Photoshop-like image editor. The target end users range from zero video experience to professionals. Think of this as how generative AI expands the capabilities of the Adobe Creative Cloud suite. Build Apps from Text Interface This is similar to 'Natural Language Frontends,' but in a narrower problem space. Natural language LLM embeddings are mapped to specific tasks that can be completed using code-generating LLMs. Business Intelligence These tools empower business users to query databases otherwise accessed through SQL or Python with natural language. Generation for Code, Terminal, and Querying A number of companies are building on top of coding-, terminal-, and querying-specific LLMs. Particular focus is going into the engineer's experience engaging with the underlying model. Unlike many business users, engineers are very aware that they can download an open source LLM trained on code and work with it directly, but are choosing to engage with an application that offers a slicker experience and features specific to their use case. Contact Center These are products built specifically for contact center operations. These largely help employees access knowledge faster or type a text response faster. Tenyx, meanwhile, seeks to fully automate call center operations. Call centers are also target customers for the Accent Modulation & Voice Augmentation category. Content Marketplaces Draft and Pepper both started as SaaS-enabled marketplaces for accessing copywriting talent. With the rise of AI copywriting tools serving their talent pool, these companies have begun building similar AI in their own product. We expect incumbents will look to embed AI into their own products, especially where off-the-shelf foundation models are most applicable. Content Repurposing A key marketing best practice is to repurpose one 'parent' piece of media (e.g. a recording of a 3-hour panel) into content for various use cases (e.g. blogposts, Tweets, Instagram posts, TikToks, etc). These products leverage the strength of foundation models in matching genres to pick the key bits of content from a parent post and 're-jigger' them into various other genres. Design Tools Tools offering design primitives and design assets. Documentation Writing There is perhaps opportunity to build an app around Markdown best practices with a wedge feature in AI generated documentation. Figma to Code These products generate code to replicate what is created in front-end design tools, like Figma. Foundation Model Providers These are companies building their own foundation models, often at the leading edge of research. Some additionally offer end user products. General Purpose Communication Writing These products feel like the next-gen Grammarly or GMail autocomplete. They are primarily browser plugins, not applications. Generation for Copywriters & SMB Marketers This is an early category of startups built on top of a foundation model. These products empower copywriters to engage with GPT-3 through a \"skin\" on top of OpenAI's API that abstracts away some of the complexity of engaging directly with the playground environment. Some productized prompt-tuning occurs under the hood as part of the application layer. A number are now pushing into image generation and are also providing more workflow type tools for editing (a la Canva) and pushing content to the platform it will be published on. MLOps, Infra, & DevTools These products make foundation models accessible to software developers, not just AI engineers. They make it easy to select the right model, finetune that model, and deploy it. Music AI generated music, which is typically rights-free. Natural Language Frontends This is among the most ambitious use cases for LLMs. The embeddings of a purpose-built LLM will map to tokenized actions that can be performed by the model (similar to how OpenAI Five was trained to play the video game Dota 2). Proficiency across a broad range of tasks will be enabled by a Mixture of Experts approach. Product Display These companies apply diffusion image models to problems of visually branding and/or displaying products (e.g. putting clothes on fashion models). Sales Coaching These products play in the same broader category as Gong. They provide increasingly real-time and automed support & coaching. Sales Tools These tools serve enterprise sales teams, helping their sellers do higher quality, more personalized, and more efficient work leveraging with foundation model enabled products. Search Engine These \"search, reimagined\" products take the theme of Google's Knowledge Hub a step further. They use generative AI techniques to provide \"the answer\" rather than just links to possible answer sources. Synthetic Talking Heads It is newly possible to generate synthetic talking heads (although the work to offer such a product remains much more build-it-yourself than many of the other categories mentioned here). Use cases are still being explored by this cohort, but learning & development and personalized sales & marketing videos are two areas of early focus. Synthetic Voices & Dubbing It is newly possible to generate Synthetic Voices & Dubbing that sound almost as good as a live human voice. (The work to offer such a product remains much more build-it-yourself than many of the other categories mentioned here). These companies are productizing such models, largely thru an API offering, and sometimes with an app-layer product, for a variety of end markets. Task Automation & Virtual Assistant These companies offer RPA either directed through or automated through LLMs. Tools for Artists A catchall bucket for applications to help further (less-profit-oriented) creative processes. Training Data These products generate synthetic data for training AI models. Translation While AI translation has been around for some time, LLMs enable it to happen on a more realtime basis, expanding the potential use cases. Vertical This is a catchall buckets for products being built for specific verticals. Most tools remain horizontal. Voice of the Customer Analysis These companies use the embeddings of LLMs to bring quantitative techniques (statistical analysis) to an otherwise qualitative problem (understanding text-based feedback). Voicebot Today, many 1-800 numbers use voice-activated \"call flows\" as a first line of defense for directing traffic and weeding out some volume. These flows can be quite frustrating, given their reliance on keywords and the inflexibility of a rules-based approach. \"Voicebot\" products are offering a brand-new experience that replaces such flows with a combination of transcription, knowledge base access, text generation, and synthetic voice to power flexible conversation with a knowledgeable, problem-solving AI. Semantic Search By applying LLM embeddings to a vector database, and matching these to the embeddings of plain language queries, semantic search is enabled as an alternative to keyword search. Knowledge Bases / Semantic Search Apps Semantic search infrastructure enables a new class of application for use cases like cross-app search and knowledge management."
    },
    {
      "title": "Introducing: the Scale Generative AI Index",
      "body": "Over the last several years, two of the dominant constraints in applying and commercializing artificial intelligence have been that: ML models were primarily adept at tasks of understanding (classification, entity extraction, object recognition, etc) across disciplines like speech, language, and images, as opposed to generation. Individual startups were responsible for “do-it-yourself ML” – gathering & labeling proprietary training data and building their own ML models. But over the last 18 months, the capabilities of machine learning have dramatically expanded, as: Models with real-world-useful generative capabilities have emerged. Foundation models, offering off-the-shelf capabilities to software developers, have become available. We have previously outlined our thesis on why we believe that foundation models are the new public cloud. We are closely following the momentum of these overlapping but distinct trends. And to track our own work and share some of the knowledge we’ve accumulated, we’re introducing the Scale Generative AI Index, a list of nearly 200 companies in the space and details about what they’re building. We’ll keep adding to this market map as our research progresses. Our index includes companies that are building on top of foundation models (both for generative and non-generative use cases), companies building generative products with proprietary models, and MLOps/Infra products important to this ecosystem. A breakdown of each of these buckets is below: Foundation Models Foundation models (like GPT-3 and Stable Diffusion) are extremely large models trained on broad datasets that can be adapted to a wide range of downstream tasks (Stanford HAI). Foundation models are analogous to the public cloud in making a powerful, new technology (in this case, AI) accessible to developers who do not have specialized machine learning skills. The low cost and ease-of-use of these models is causing the evolution of AI-apps to accelerate as more engineers jump into building with artificial intelligence. Today, foundation models (e.g., GPT-3 and Stable Diffusion) are frequently adapted to build generative applications, as that’s their most “wow!” capability. But they also can be applied to more traditional ML use cases such as classification and entity extraction, and importantly, they minimize (but not completely obviate) the need for startups to gather proprietary training data, label it, architect complex data transformations, tune hyperparameters, and select the right model. Generative Applications These are companies utilizing generative AI for its namesake purpose: the creation of net new output in various media types. Needless to say, this is by far the most prolific category and thus comprises the majority of companies on our index. We are seeing startups here that are both building directly on top of existing foundation models, as well as those that have chosen the route of building their own models from scratch, particularly in domains where foundation models don’t exist (e.g. speech). Non-Generative Applications Built with Foundation Models While generative use cases are the most popular application of foundation models, many emerging products highlight that generation is only part of the story. Another set of powerful and newly-feasible applications have taken advantage of their embeddings. More effectively than ever before, text, image, and even an application’s set of possible actions can be represented by embeddings. Semantic search (e.g., what underlies Mem-X) and text-based interfaces (e.g., RunwayML’s newest feature) are two fascinating applications of foundation models. MLOps, Infrastructure The prolific innovation we are seeing in the space has also created a need for support infrastructure and frameworks that cater to these new use cases. Companies here come in many forms, but are mainly centered around: ML Ops: Makes selecting, implementing, and fine-tuning foundation models as easy as possible. Vector databases: Databases optimized for vector-based information retrieval There are other category-of-one companies in this space as well, like Hugging Face. In sum, the creative abilities of Generative AI enable software to transform a variety of creative fields ranging from the world of voice actors to videography, while Foundation Models are enabling more rapid experimentation with wholly new use cases for AI. Amid Much Hype… Where’s the Substance? We would be remiss to ignore that this market map piles onto what feels like an endless stream of hype in the space. And cynics are right to seriously question both the attention span and herd-like behavior of the VC industry in general! While we stand by our conviction that foundation models are the new public cloud, the recent pushback over the outpouring of hype in the last few weeks is very much warranted. In fact, many skeptics bring up a number of good questions that we ourselves are wrestling through. Where Is the (Enterprise) Value? Generating an image of an avocado playing guitar may be fun, but, with very few exceptions, is likely not a good business. However, more meaningful use cases do abound even if they are not quite as entertaining. Generated with yours truly, DALLE-2 Generative models have implications that reach far beyond our beautiful avocado art. Investors don't need to believe that AI will create the next Star Wars or that Hollywood should just throw in the towel to get excited about this category. There is plenty of whitespace in simply building applications that automate the repetitive tasks that many humans loathe -- writing sales emails, finding a document without the right keywords, or the manual rotoscoping of an object from video frames -- the list goes on. The Border War Still, valid questions remain on which players will come out on top here. There is an inevitable “border war” brewing between the foundation model providers themselves and the companies that are built on top of them. After all, the companies which build the foundation models need some way to extract value, yet currently, the lion’s share of revenue in this space lies with the companies who build on top of these foundation models. Over the next year, we’ll be watching to see the extent to which a great application layer/UI offers companies sufficient differentiation in a competitive space. We’re also thinking about whether improvements driven by domain-specific fine-tuning will really give startups enough lift when future foundation models (e.g. GPT-4) inevitably become a whole lot more powerful. And of course, there is the age-old question of: in which circumstances do generative capabilities create new standalone companies versus become features embedded within incumbent applications. These unknowns, however, do not undermine the innovation we are seeing every single day. After all, VCs have always asked themselves “Can this company add value where the big guns cannot? Is your offering defensible on a technological level? On a product level? Will the incumbent decide to simply integrate your offering and squish you?” Generative AI is no different. Welcome to AI’s brave new world! This article co-authored by Jeremy Kaufmann, Max Abram, and Maggie Basta."
    },
    {
      "title": "Observations from the Field: The Future of Generative AI",
      "body": "We believe that in this moment of generative AI hype, nothing is more valuable than hearing directly from entrepreneurs and product leaders building in the Generative AI space. That’s why last month we hosted a panel with a group of entrepreneurs and AI practitioners to discuss the key challenges entrepreneurs are facing when it comes to building category-defining generative AI companies. We began the panel by discussing why the technical performance of generative AI in the last year is approaching human baselines and how the common wisdom that we would have self-driving cars very soon but that creativity was out of reach for AI was wrong. We then transitioned to discussing the most monetizable opportunities for entrepreneurs building in the space, paying particular attention to the potentially fraught relationship between applications built on top of foundation models and the companies building the underlying foundation models and how startups can navigate this potentially tense dynamic. We (dangerously) asked the panelists to conclude by making some predictions around whether generative AI and foundation models act as centralizing forces, and delve into the question to what extent will this technology accrete value to large tech incumbents vs. allow startups to prosper. Our panelists were: Andrew Carr - Senior Applied Research Scientist, Gretel AI Lisha Li - CEO, Rosebud AI Ryan Seamons - VP Product, Latitude Srinath Sridhar - CEO, Regie.AI For a more in-depth outline of our panel discussion, we turned to a startup called Contenda, which can take in a video input and output a summary of that video in text. Contenda identifies topic themes and narrative direction given existing content using Large Language Models (LLMs) and topic modeling. New content is created that matches the accuracy and tone of the existing content. Unlike other AI copywriting tools, you don’t even have to provide any guidance or details to the model. Here's what Contenda wrote for us: In our recent AI panel, we discussed the big picture trends and technological innovations in generative AI. We also talked about what entrepreneurs can really do with this technology – what can they build on top of these models? And lastly, we touched on some esoteric questions about foundation models. The Future of Generative AI A lot of people are underestimating the creativity of AI, which is leading to some big mistakes. For example, the common wisdom 10 years ago was that we'd have self-driving cars, but people didn't think that creativity was really within the reach of machines. In 2012, Google showed that deep neural nets could be used to recognize cats in Youtube videos. This was a significant progress on a problem that was completely open until that point in time. In 2015, we had AlphaGo which could play Go. This was a progress in the creativity domain. In 2017, we had sketches that could be converted to images. In 2018, we had GPT-2 come out for large language models. And in 2020, we had GPT-3 come out. Generative AI is an exciting new technology that has the potential to revolutionize many industries. Self-driving cars are one area where generative AI can have a major impact. However, the technology is still in its early stages and has yet to reach its full potential. The next big breakthrough in generative AI will likely be in multimodal learning, which involves combining text, images, and audio/video. Google's Second project is an exciting development that uses huge language models to instruct robots. The project is getting closer to being able to generate plans and understanding of the world. Some of the things that have been promised for a long time are finally getting closer. So what can entrepreneurs really do with this technology? What can they build on top of these models? The Impact of AI on Entrepreneurship It is now widely known that self-driving cars are not going to take over the world anytime soon. The technology just isn't there yet and there are still many complexities that need to be worked out. However, the potential applications for Generative AI are vast and exciting. We are only just beginning to scratch the surface of what this technology can do and the possibilities are endless. Recently, we've seen great progress in AI with text generation (GPT-3) and image generation (DALL-E and stable diffusion). The next big breakthrough will be in multimodal learning, which combines text, images, audio, and video. This will allow for a more realistic and lifelike experience for users across a variety of platforms. In the video space, it is getting better to control the semantic content of images and make them temporarily consistent. This means that the solution is incredibly within reach. It just requires more resources to scale the models that we have to get better control within the representation. Even in the last week, there have been a lot of results released in video. As Artificial Intelligence (AI) and Machine Learning models become more advanced, entrepreneurs are finding new ways to use them to build products and businesses. One of the challenges is to design products that are easy to use, while still harnessing the power of these complex models. GPT-3 and other similar foundation models provide a great opportunity to do this. By building on top of these models, entrepreneurs can create products that are both easy to use and offer powerful capabilities. The Impact of Open Sourcing With the availability of large, general models for machine learning, the demand for machine learning engineers may change. Some startups may outsource everything to the foundation model, and hire different types of people instead of ML engineers. The open sourcing of NVIDIA's foundation models will increase demand for more specialized models and services to help users fine tune those models for their specific use cases. This is good news for NVIDIA, as it will increase the appetite for startups to build on top of their platform. There is a growing demand for machine learning engineers, both in terms of developing new foundational models and in terms of applying and managing existing models. This demand is driven by the need for ever-more sophisticated AI applications, which in turn is driven by the ever-growing capabilities of machine learning."
    },
    {
      "title": "Doubling Down on Cognitive Apps: Announcing Scale Fund VIII",
      "body": "Today we’re excited to unveil our new fund, Scale Venture Partners Fund VIII. This $900M fund gives us the opportunity to continue to invest in the category-defining technology companies that are being built today. We want to thank our amazing founders and investors, new and returning, for making this fund possible. It’s been two years since we announced Fund VII, which we raised in the throes of early-COVID, and a lot has changed since then. We celebrated the IPO of WalkMe and the acquisitions of portfolio companies Lever, Solvvy, and Agari. We invested in 16 new companies, including AppOmni, Viz.ai, Comet, and Archipelago. Many others in our portfolio came out of COVID stronger, and are seeing strong growth even in today’s challenging environment. Investing in the Future of Software We believe it’s always a good time to build an innovative company. But now is especially good; rapid digitization during the course of COVID made the market hungry for software solutions that deal with the new way we all work. The last two years have been a time of unparalleled growth in artificial intelligence and machine learning capabilities, making building intelligent applications easier and faster than ever. This combination of market opportunity and technological capability means that even in the face of economic headwinds, great companies will emerge, and we are excited to help build them. In fact, we’ve seen this movie before. In the last downturn, Scale was particularly focused on investing in enterprise cloud-based SaaS applications, at the time an emerging category that transformed the markets in less than two decades. For several years now, we’ve been focused on how AI-powered applications transform traditional SaaS applications, by augmenting manual work and offering data-driven predictions. While the last two decades were largely defined by the shift from on-premises to cloud software, we believe that the next decade will be defined by the addition of machine intelligence to software, a trend we’ve labeled cognitive apps. We previously articulated our cognitive apps thesis when we announced Fund VII, and we’re happy to share that nearly two-thirds of the investments we’ve made from that fund have some form of machine learning core to the product. Looking ahead, we only expect this trend to continue as the reach of AI permeates our focus markets like cybersecurity, devops, data infrastructure, fintech, productivity, vertical SaaS, and many others. One area where we are increasingly devoting time is an emerging breed of Generative AI applications, which expand the reach of machine learning beyond merely understanding to creating. These applications tend to employ foundation models, allowing entrepreneurs to build software more quickly and easily than even a year ago. Investments in Textio, Datagen, and most recently Regie AI, show our early conviction in this space. Scaling Scale As our portfolio companies do, we’ve been busy maturing Scale. We’ve grown our team, expanded our platform, and deepened the support for our current and future portfolio companies. We are now seven partners strong, with recent the promotions of Eric Anderson and Jeremy Kaufmann to Partner. Sam Baker was promoted to Principal, and John Gianakopolous and Noah Gross were promoted to Vice President. Javier Redondo recently joined us as a Principal focused on infrastructure and business software with a particular interest in open source and fintech. He joins Scale from Anyscale, where he led product efforts for the development of a managed compute offering and was a regular contributor to the Ray project. We’re joined by a class of talented associates that represent backgrounds ranging from Product to Machine Learning to Investment Banking at companies like Rubrik, Morgan Stanley, Deutsche Bank, and QuantCo. Rounding out our Scaling and Marketing team, we welcomed Craig Rosenberg as Chief Platform Officer and Hillá Watkins has joined us as Chief Marketing Officer. Craig joins us from Gartner, where he was a Distinguished VP. He joined Gartner via the acquisition of Topo, an advisory firm he co-founded that focused on GTM for early-stage technology companies. Hillá joins us from Pendo, where she was VP of Brand, responsible for all content, community, and creative marketing. Hillá created ProductCraft, Pendo’s global community for product professionals. Supporting Scaling with Go-to-Market Expertise and Insights We made our first major investment in our Scaling Platform back in 2015 when Dale Chang joined our team. Since then, we have built capabilities to deliver direct advisory and playbooks to our portfolio companies. This work has been supported by data from our proprietary analytics platform, Scale Studio. With the addition of Craig and Hillá, we continue to extend our capabilities to help our portfolio companies transition from founder-led growth to a repeatable go-to-market machine. They will continue to build and manage our Scaling Platform, which delivers targeted expertise to help our portfolio companies drive efficient hyper-growth. Since joining, they have increased the breadth of advisory capabilities, introduced more playbooks for our founders and expanded the Scale Go-To-Market Network. The expansion of the network gives our portfolio companies access to recognized experts and leaders in Marketing, Sales and Customer Success. This network builds on the success of our functional communities which has helped our portfolio executives learn from and collaborate with their peers. In support of our Scaling Platform, we continue to invest in Scale Studio, our proprietary analytics platform. As companies shift their focus from growth-at-any-cost to efficient growth, they can use Scale Studio to understand how their growth, efficiency, retention, and burn rate compare to more than 1,000 private and public companies. It’s worth repeating: we believe it’s always a good time to build an innovative company. But now is especially good. Both in the data and in our community, we see that innovation hasn’t slowed and we don’t intend to slow down either. If you’re a founder working on something great, we would love to meet you."
    },
    {
      "title": "Foundation Models Are The New Public Cloud",
      "body": "This article on Generative AI was co-authored by Jeremy Kaufmann and Eric Anderson with contributions from Max Abram and Maggie Basta. Imagine it’s 2002 and you are an early engineer at Salesforce. There is no Dreamboat offering free hotel rooms, no Dreamfest featuring famous musicians, and not even a single Dreamforce cocktail party to add to your social calendar (gasp!). Instead, you spend the majority of your time building and maintaining data centers that span the globe, finagling hand-crafted orchestration layers to interact with the various data centers, and in the few remaining hours of the day, designing a complex SaaS product that sits on top of this messy stack. Hard to fathom, right? While it’s almost trite to dredge up the historical evolution from on-prem to SaaS at this point, many of us have forgotten that there was in fact a short-lived interstitial era of “roll-your-own SaaS.” This transition from on-prem to cloud-based SaaS didn’t just happen overnight, rather there were seven years between the founding of Salesforce (1999) and the launch of Amazon Web Services (2006) where startups building SaaS-like experiences had to do it all on their own, without any help from the public cloud vendors. Which explains why being a Salesforce engineer in 2002 was way less glamorous. So what does this bygone era of roll-your-own SaaS reveal about the world today? Replace “SaaS” with “Machine Learning,” and we’d argue we are broadly witnessing that very same evolution today. Over the last few years, building an AI startup used to require “do-it-yourself AI,” which consisted of gathering training data, labeling it, architecting complex data transformations, tuning hyperparameters, and selecting the right model. It was a herculean task, similar in complexity to the workload of the Salesforce engineer above. But in the last year or two, foundation models have emerged as a time-saving shortcut that enable entrepreneurs to do more faster. These foundation models aren’t specific to particular AI use cases, but are largely general and have something to offer almost anyone. Entrepreneurs can now decouple parts of the training data and model (which comes pre-packaged in a foundation model) from the application layer, which we at Scale call a cognitive application. Looking Backwards: What We Got Right and What We Got Wrong Two years ago, Scale’s Andy Vitus first introduced our Cognitive Applications thesis which detailed how machine learning would change the way we both build and use software. Reading that piece now, we’re struck by how strongly we continue to believe that adding intelligence to software represents the dominant paradigm in enterprise investing over the next decade. And also by just how understated the tone of the original piece seems in retrospect. It did not envision a future where ordinary people would generate art with textual prompts, or where natural language would begin to function as a credible UI for interacting with software. So what exactly did our prior thesis leave out? For one, it missed this transition from the era of “do-it-yourself AI” to foundation models, much like a Salesforce engineer in 2002 would have never imagined how much of her job would be outsourced to the public cloud vendors by 2010. But it also missed that the fundamental use cases for machine learning would vastly expand beyond understanding the world to generating the world. The Transition From the Era of Understanding to the Era of Generation It’s been truly amazing to watch over the last year just how rapidly machine learning has expanded beyond “the era of understanding” into a new “era of generation.” Over the last several years, ML models largely performed tasks of understanding (classification, entity extraction, object recognition, etc), across disciplines like speech, language, and images. Compare that to the emerging generative AI applications where computers actually create novel text and images. In the era of understanding, there was basically one mode of operation. We called it inference, because it was about converging on a conclusion, usually in the form of applying a label. It was basically all “Hot dog or Not Hot Dog.” In the era of generation, the mode of operation is divergent. Pretty much any medium can be transformed into any other in a variety of ways. And the same inputs can produce an infinite number of equally valid variant outputs. Text can be generated, summarized, or translated into other text. It can cross formats and be spoken audibly or visualized as an image or video. The reverse is also true: audio can be transcribed, videos can be captioned, and imagery can create more imagery. All these can be combined into compound transformations. Never has the phrase “the possibilities are endless” felt so apt. Caption: Alternative book cover art created by generating images from a generated summary of the book’s text. The same process could be applied to any book. Credit to GPT-3, DALL-E 2 and our very own Maggie Basta (as well as E.B. White?) Foundation Models Transform How AI Entrepreneurs Build Companies For AI entrepreneurs, the era of understanding was characterized by “do-it-yourself AI.” Until very recently, the individual startup was responsible for gathering training data (laborious), carefully hand labeling the data (expensive), and then building their own model (requires ML engineers). There was no way to harness the magic of AI without this effort. This is why AI investors historically focused heavily on both the quality of the underlying training data (see “Data is Not the New Oil” by Zetta VP and “The Empty Promise of Data Moats” by A16Z) and the raw talent of the machine learning engineers. Foundation models offer a new way of adding AI to an application and expand what it means to be an AI entrepreneur. Models like GPT-3, DALL-E 2, and Stable Diffusion have been pre-trained on a massive corpus of data and can be used as a platform for an enormous breadth of AI powered products. Yes, entrepreneurs may still need to fine-tune these general models on their particular domains. But instead of requiring hundreds of thousands of specialized documents and AI expertise, they may only need a thousand such documents. Every AI-entrepreneur now holds a “golden ticket” to the world’s data in their back pocket. The market has already proven out the power of foundation models for entrepreneurs: one of the largest and fastest growing startups of the generative AI era, Jasper AI, exploded to significant revenue before even hiring a full ML team. The real significance of foundation models is that they can encapsulate all accessible human knowledge and do so in a new way. Google’s search engine accomplishes its mission to “organize the world's information and make it universally accessible and useful” by viewing everything possible and saving important bits and pointers to source. The result is “well over 100M GBs in size.” While these foundation models also ingest everything they can (Stable Diffusion looked at 600M or so images) they don’t save copies of them. Instead they maintain a single representation that is influenced by each image. Where Google’s index is estimated to be 100M GBs in size, Stable Diffusion is just 4GB and can fit on a DVD (I know we just dated ourselves). Where is the Money in the Era of Generation? One of the biggest unanswered questions in this era of generation is who will ultimately extract the lion’s share of the value created by these generative processes? In the previous era of understanding, we often saw a fierce “border war” between the existing system of record (e.g. Salesforce or ServiceNow) and the newly built cognitive applications that sat on top of it and attempted to wedge their way into the stack by automating various processes (think of customer support chatbots like Solvvy and Ada Support, or ITSM automation companies like Moveworks sitting on top of ServiceNow). The analogous situation in this era of generation is the “border war” between the companies which have built the foundation models themselves (the infrastructure layer) – like Cohere, Open AI, and Stability AI – and the crop of startups sitting on top of these complex models and integrating into various business processes like Jasper AI, Runway ML, and Regie AI. After all, the companies that have invested millions of dollars in building these incredible foundation models need to figure out how they can best monetize their creation, while the Runway MLs of the world start off with the advantage of being closer to the end user. Grab some popcorn and get ready for the inevitable border wars. Strategies for Startups Building In the Era of Foundation Models So what strategies are available to entrepreneurs to capture the tremendous value created by these generative apps? Below we outline several approaches we’ve seen to date, recognizing that this list is in no means comprehensive and that entrepreneurs fighting in the trenches of the generative AI border wars will likely make use of multiple strategies. In fact, these blueprints are listed in descending order of the new cognitive apps stack: starting with the application UI, then at the intersection of the application and model, and lastly at the level of the foundation model itself. 1\\. Building a more accessible UI This allows business users to interact with the underlying model. For example, companies like Regie AI and Jasper AI recognized that sales professionals and marketers looking to generate sales cadence emails and blogposts would likely find it too complex to interact with the interface of GPT-3, which is built for a developer. These companies help business users interact with generative models in a more structured and guided way, eliminating the need for a non-technical user to have to make model tuning decisions around “temperature,” “frequence penalty,” and “presence penalty”, all choices that show up in the Open AI interface. In fact, one of the most powerful strategies seems to be taking business users out of the complex world of prompt design and prompt engineering, helping ordinary people structure their inputs in a way that is most likely to lead to a good model output. The magazine Cosmopolitan describes the hours they spent trying to engineer a DALL-E prompt that would produce an up-to-snuff magazine cover. The winning prompt “a strong female president astronaut warrior walking on the planet Mars, digital art synthwave” is not exactly the kind of thing a novice could generate. At the end of the day, prosumers and professionals interested in using foundation models for their real world applications are unlikely to be interested in learning to prompt engineer or tune a model. Broad adoption will require applications that productize best practices and fit nicely into existing workflow. 2\\. Fine Tuning a Generative Model on a Particular Dataset Open AI first began allowing developers to create GPT-3 models tailored to the specific content in their apps and services back in December of 2021. This process is called fine tuning, and results from passing specific examples to adjust the billions of weights within the network to make it more performant for the domain in question. Fine tuning simply enables the entrepreneur to build a more accurate domain-specific product. For example, Regie AI built a fine tuned model trained specifically on thousands of sales emails. Fine-tuning means that Regie knows the best performing tone and length to take in sales emails. Moreover, Regie’s generative tool has been exposed to the weird nuances that exist in the world of sales emails, such as whether or not to suggest a time for a meeting to a prospect, and the very human power dynamics that implies. One of the biggest question areas for entrepreneurs to watch going forward is the velocity of improvement in subsequent versions of foundation models. Today, fine tuning on domain-specific data is one of the key ways to properly “steer” the model and stop it from generating gibberish. The extent to which fine tuning is needed in the future will closely parallel the rate of improvement in underlying models like Stable Diffusion and GPT-3. We’re watching this pattern closely as it will determine the extent to which the locus of value moves from the vertical to the horizontal, and whether fine tuning is a credible route to defensibility in the generative AI space. 3\\. Building (and even open-sourcing) a model It is easy to see how proprietary models like DALL-E and GPT-3 can make money, but what about Stability.ai, the open source darling that helped create Stable Diffusion? Stability, as we’ll call it, was only published six weeks ago and it has rocked the developer community. It turns out there is much more demand for experimenting with foundation models than proprietary models allow. Developers are flocking to Stable and extending Stability’s influence into new areas like image compression, animation, infinite video, textual inversion, alternative web UIs, support for mac GPUs, and support for intel CPUs, faster than proprietary vendors can build products. As these efforts mature into actual products, many will look to Stability to define the standard version, one they can offer as a service. As users’ apps go into production, ops teams will want a vendor to ensure model security, uptime, support and we bet Stability will be the first place they look. In every heavy engineering domain, be it systems (Unix, Linux, Docker, Kubernetes), or in data, (Hadoop, Spark, Kafka, DBT) builders have innovated faster in open source, formed communities and built big businesses. But don’t count out the others. If history is any indication, open source standards in big markets like this typically exist alongside a proprietary leader: Snowflake/Databricks, iOS/Android, Windows/Linux. How Company Building Evolves With the Rise of Foundation Models Moving beyond the fight over value capture in the generative era, it's worth reflecting more deeply on how building and scaling cognitive applications will change as foundation models become a standard part of the entrepreneur’s toolkit. Increase in Speed to Market: Foundation models enable entrepreneurs to hack together the MVP for an ML-enabled product more quickly and cheaply than ever before. With a supersized portion of the world’s training data sitting in the back pocket of founders, these newer companies built on top of these foundation models have a massive head start relative to all the labeling and manual configuration that marked the do-it-yourself AI era. The Rise of the Bottoms-Up GTM Motion In ML: Historically, many cognitive applications in the era of do-it-yourself AI sold tops-down at higher price points. This was to compensate for the fact that time to value in ML has historically taken longer given the need to access vast quantities of company-specific data and then build company-specific models. In the new era, foundation-model-enabled companies sell directly to individual users with lower price points and then upsell with enterprise-wide purchases. It’s not surprising that when a larger share of the training data now comes from the foundation model itself vs. the customer specific data, time to value can be improved, thus enabling the classic software bottoms-up sale. Model ≠ Product: Nat Friedman and Daniel Gross are correct to highlight all the challenges that still remain in the generative AI space, particularly the vast gap that still exists between using these models in academic research vs. actually embedding them in business workflows without causing user frustration. We agree that ultimately entrepreneurs must remember that “the model is not the product. It is an enabling technology that allows new products to be built” and that “entrepreneurs need to understand both what the models can do, and what people actually want to use.” Just because you have access to an amazing model, doesn’t mean you get a pass on understanding the needs of your user. With Generative AI, Software Might Actually Eat the World Generative AI is quickly becoming a canonical example of how automation allows software companies to enter markets where software has never gone before. More than two decades after the founding of Salesforce, cloud software has already eaten a large chunk of traditional on-prem software spend and advances in AI are now giving software companies the ability to directly feast on previously untapped categories of spend. Our partner Rory O’Driscoll previously described an upcoming era he termed the SaaS Hunger Games, writing that in a world of more cloud-on-cloud competition, the next generation of software companies should focus on reaching deep into the “real world” and earning dollars by automating, or eating, the work that today is outside the scope of current technology spend. Generative AI is simply one of the most successful examples of this approach we’ve seen to date. Today, human voice actors are in direct competition with the text-to-speech capabilities of synthetic speech companies like Wellsaid Labs and Resemble AI. Traditional corporate videographers are going head-to-head against synthetic video companies like Synthesia and HourOne which produce corporate training videos at 1/60th the cost of traditional videography. With very little advanced warning, AI is in the midst of digitally transforming creative industries many had incorrectly assumed were safe from the long-term trend of software eating the world. While the societal and artistic impacts of this phenomenon are certainly worth a much longer discussion amid all the attention they are getting from the popular press (check out the New York Times’ Kevin Roose’s piece and Humberto Moreira’s Crafting the Hyperreal), it is clear that the generative AI era is only in its first inning and there will be no going back. In the midst of such exciting innovation, Scale couldn’t be more thrilled to double down on our focus on machine learning with a new $900M fund and play an active role in supporting the founders building companies on the frontier of foundation models and generative AI. Thank you to our colleagues Max Abram and Maggie Basta for their many contributions to this piece and for their thought leadership. Max wrote an early draft of this post and was an essential editor, while Maggie is well on her way to becoming a full-fledged AI artist! For further questions, please reach out to Jeremy Kaufmann (jeremyk@scalevp.com) and Eric Anderson (eric@scalevp.com). If you are a builder in the generative AI space, we’d love to hear from you!"
    },
    {
      "title": "What Comes After Adyen and Stripe? The Future of Payments Orchestration and Optimization",
      "body": "Turns out, phones are quite good payment platforms. This poses a challenge for traditional consumer payments. Debit cards, credit cards, and other legacy payment methods are making way for Apple Pay, BNPL, QR code payments, Venmo, and Zelle — and that’s just in the U.S. Outside the U.S., super apps are beating Visa and Mastercard to the punch. China is a prime example, where WeChat and AliPay process more USD equivalent volume collectively than Visa. Payments industry, interrupted Traditionally, Visa and Mastercard served a useful function for merchants by aggregating a large number of different consumer experiences (e.g. credit card, charge card, debit card) into one simple data and money stream. But the bounds of payment processes are expanding and demanding new solutions in terms of payment organization and orchestration. This poses a challenge for merchants, who now have to incorporate a myriad of different contracts, chargeback rules, reconciliations, and timing considerations into their payment processes. Essentially, merchants went from managing a single contract to 20 contracts, and that’s before they ever expanded their commerce into global markets, which many of them are. The payments industry hasn’t been asleep at the wheel, though. First, we saw the unbundling that gave us the alphabet soup of MSP, PSP, PayFac, ISO, etc. This, in turn, gave way to re-bundling, as these services were aggregated into a single vendor for online and offline transactions. Such payment gateways became known as acquirer-aggregators. They created an evolution which is now fronted by the popular Stripe (SMB-focused / US-heavy) and Adyen (enterprise-focused / Europe-heavy). The benefit of these solutions, as Rohit Sharma writes in Monetary Musings, is that “merchants have one best in class provider that works seamlessly across all their use cases. One provider to interface with, one provider to negotiate with, one provider to get reporting from.” Payment aggregation, but at what cost? There are, however, downsides to a future dominated by the acquirer-aggregator payment gateway. Consolidation and streamlining are great, but they often sacrifice technical flexibility. We think this will spur a new payment orchestration and optimization solutions and products. What if your payment gateway doesn’t support a certain payment solution? What if the gateway is limited to certain geographical locations? What if you can negotiate better pricing directly for certain transactions, volumes, or geographies? What if your business has complex transaction rules or parameters, or necessitates data attached to the transaction, that the payment gateway does not support? The convenience and universality of a single payment gateway comes at the cost of flexibility many enterprises need to meet their customer or reporting needs. Our conversations with some of the largest companies in the world show customers increasingly wrestling with these questions. The Next Wave of Payment Orchestration and Optimization We think this next generation of players will have these distinct characteristics as enterprise customers demand that vendors are capable of meeting their business needs: Unlimited integration flexibility: Vendors will need to have architecture that can react to a wide range of payment complexity without meaningful engineering with each iteration. The value in these new solutions will lie in their ability to reduce the complexity of engineering to business integrations. Customers are increasingly demanding the ability to integrate multiple acquirers to maximize approval rates. While players like Adyen have world-class APIs, what happens when a customer's systems can’t handle APIs? Even if a customer can handle APIs, what happens if Adyen or Stripe’s risk management system isn’t tuned for specific geographies or industry types? As more old-world businesses modernize, these challenges are increasingly the reality in the enterprise space. Unlimited payment flexibility: Simplicity is the primary objective of modern payment technology. Acquirers – like Ayden – have evolved to aggregate many systems globally. But as new payment types explode, enterprises need the flexibility to integrate payment types that don’t go through an acquirer (e.g. QR codes, Affirm) as well as bi-directional payment types. It used to be that outbound payments were just checks, but today they come in many different forms with many different types, especially as you move across industries. Vertical focus: To deliver unlimited flexibility, vendors need to understand the industries they serve and their specific demands. While a horizontal solution is aspirational, our belief is that vertical vendors will have the edge on delivering the experience enterprise customers need. Vertical focus will allow for standard system and software integration. We are already seeing the emergence of vertically-focused players in e-commerce, marketplaces, financial services, and insurance. Flat fee payment models: The transactional fee is a cornerstone of the traditional merchant acquiring model and success of payment gateways thus far. This has multiple benefits, including alignment and transparency. The opportunity for this new generation is to position themselves as technology vendors sitting at the integration level, accessing new budgets and mitigating the risk of double-fee layering as enterprises may continue to work with transactional fee payment vendors. So, who’s making noise? It’s becoming clear that the “true democratization” of payments is no longer a discussion point but an inevitability. This will be achieved by optimization and orchestration solutions which offer new levels of flexibility and integration based on businesses varying needs. No doubt, the frontline of this trend is being fought in e-commerce. Intuitively, this makes sense since e-commerce has the most sophistication in transacting online relative to other industries, frequently combined with a brutal conversion or margin structure that forces fierce competition on marginal innovation. Venture funding supports this insight: Primer out of the UK has raised over $75M, and Gr4vy and Pagos in the US have raised over $25M and $10M, respectively. While quite early, investors are speaking with their wallet. Other verticals are more nascent, with Apexx Global doing well in travel, and Imburse landing early customers in insurance. This is probably a reflection of the evolution of the trend, and that players will emerge across a broader set of industries, you could see specialists in healthcare, real estate, or logistics in the future. While it would be nice for a single vendor could serve these industries, it is likely that the integration needs and payment habits vary widely enough to require verticalization. Predictions for the world of payments Our suspicion is that we will see these orchestration and optimization models demonstrate initial stronger growth outside the U.S., where fragmentation around payments is greater than in the U.S. We also suspect that the long-standing Visa and Mastercard reign may finally be seeing cracks. In terms of market share, consumer choice is becoming priority one and merchants are equipped to address the consumer where they want to be. The future of payments orchestration and organization is going to be increasingly focused on the outcome for the consumer, rather than trying to compact every payment process possible."
    },
    {
      "title": "Steering with a Madman in the Car",
      "body": "Can you imagine steering a car and getting directions from a manic depressive in the back seat? Most venture-backed CEOs can. Can you imagine how stressful it would be if you had to play nice to that person because they have the money to pay for gas? Most CEOs know already. The venture industry, and the capital markets more broadly, are that manic depressive back seat driver. In November we said your startup was worth over 50 x revenue. Today we are saying 10 times is too much, and you need to get to cash flow breakeven right now. The obvious question: how much should you listen to advice from people that got it so spectacularly wrong six months ago? Listen but Filter Should you do exactly what we say? Hell no. Our stock in trade is valuations. When valuations are up, we are happy, when valuations are down, we often panic. Can you ignore us entirely? No, because we have the gas card. The reality of running a high-growth startup is that you are almost certainly going to burn money, which means at some point you need to know what the manic guy is willing to give you. What you should do is listen but filter for what matters to you. Right now, the default venture missive is a combination of fact (valuations are down), prediction (we are going to have a recession), and advice (you should cut the burn). The fact is true, the prediction is probable, but the advice needs a lot more massaging to be useful. Here is my mental model for what is going on: Fact: Tech Valuations are down (a lot) Public Tech valuations have just done a massive two-year-round trip during Covid. From peak to today public SaaS multiples are down over 61% to just below the long-term median of 6.5x and high-growth public SaaS multiples are down an even larger 76%. Private company valuations are by nature harder to track and slower to correct, but the anecdotal peak 100x ARR valuation of 2021 could be due for an 80% correction. The Covid Valuation bubble is well and truly over. When investors overpay by 5x it takes two years of 123% growth to “grow into the valuation” as the phrase goes. The table below shows the math. This growth rate is not impossible, but it is not the norm, and so many companies will face the (not awful) prospect of a down round. We have just lived through one of the great epic bubbles of financial history. Even if nothing else was wrong with the world, cleaning up this mess would be a rolling problem for the next two years. My guess is also that the venture angst right now is less about a firmly held prediction for 2023 GDP and more about a slowly dawning recognition of what a big hole the industry has dug for itself in the last two years. It’s that moment you wake up after a wild fun evening and start to come to grips with what you did at the party. Prediction: A recession is coming (but what flavor of recession?) If the plan is to grow our way out, the bad news is that pundits, including venture investors, are predicting a recession. Predicting recessions is hard, which does not seem to stop everyone from trying. Macro forecasting exists, as Galbraith said, to make astrologers look good. However, thinking about a “recession” may not be the most useful way to think about what is going on now. It implies a one size fits all world, but we are not seeing a one size fits all reality. Look just at how 2022 revenue guidance for these public companies has changed between Dec 2021 and June 2022. This is the period when we went from a recession being perceived as unlikely to being seen as almost inevitable. In that period, the Coinbase 2022 consensus revenue number was reduced by 42%, Amazon’s revenue estimate came down by 5%, Q1 GDP was down 1.9% and some of the best software companies are predicting higher revenue for FY 22 as of June, then as of last December. We should take all predictions with a grain of salt and there is probably bad news to come but it is the spread of the predicted impact that is noticeable. We are seeing the same story in our portfolio Companies selling labor-saving automation to corporate America are seeing revenue acceleration as labor shortages drive the need for automation. For core SaaS companies it depends on the health of the end customer. SaaS companies selling to other venture-backed companies are seeing significant slowdowns driven by the valuation reset above. Companies selling to larger corporate customers outside the tech ecosystem are doing much better as corporations continue to invest in technology. Companies with exposure to, or directly based on the most speculative or venture-fueled sectors (crypto, consumer lending, rapid consumer delivery, etc.) are seeing absolute revenue declines. I am not Pollyanna. An extended cyclical downturn will impact all startups but the key is that it will not impact all start-ups anywhere close to equally. How we got to this point was weird (Covid itself, unprecedented Fed expansion, a tech valuation boom, lockdowns, shortages of goods, inflation) so it makes intuitive sense that the unwinding will be weird also. One of the mistakes forecasters make is trying to group and compare different downturns as if there were only a few flavors of bad, when in fact, every downturn is unique and different. This is not Global Financial Crisis (II) or even Dot.com (II), this is yet another completely different way in which the world has become screwed up. It will unfold in its own special way (my gut is a shallow but long grind v a short deep shock like the GFC). Tech investors may not be best positioned to assess the wider economy right now. There is a clear linkage in tech from the valuation collapse to a slowdown in tech revenue growth. Companies that are laying off employees to avoid raising capital, do not simultaneously invest in new software, so one startup’s expense cuts become another startup’s revenue slowdown. This is the tech reality, but it is not the reality (yet?) in the 90% plus of the economy that is not tech. Put another way, I think over half of the “recession pessimism” coming from venture is based on a justified pessimism about the revenue consequences of the valuation reset and not from a dubious ability to predict the wider economy in 2023. In practical terms, it means that management teams should not focus on a generic macro prediction about a recession. Instead, they should focus on having a firm grasp of what is happening to their customers and in their end markets. Macro forecasting is meaningless, Revenue forecasting is what matters. Don’t worry about the money supply, worry if your customers have money. Advice: Prioritize Runway over Growth, (maybe?) Prioritizing cash runway over growth is the standard venture advice right now. As a practical matter, it is good advice about 50% of the time, half right 25% of the time, and utterly wrong 25% of the time. This is not a bad hit rate, but I think we can do better. The way to do that is not to start on the runway question, “how long do you have” but instead start with the more fundamental question “how strong is your business? “ What is “Good” Growth and an “Acceptable” Burn The financial profile of any software startup can be characterized at a high level based on revenue growth and either burn rate or burn multiple. This gets complex because what “good” looks like for both metrics, varies enormously by size. A 30% growth rate at $100MM with a burn multiple of .5 would be compelling, but a 30% growth rate at $1M would not be interesting to venture investors almost regardless of the burn multiple. Using data from over 500 companies we have built a set of benchmarks by size from 1MM to 100M+ for both metrics as explained here in detail. We then make a 2x2 quadrant that shows a specific company’s performance versus the benchmarks. The chart below shows an example, showing two years of numbers (which could be either 2021/22 or 2022/2023). Growth and Burn Multiple — 2-year Trajectory Year 1 Year 2 Please enter values in USD thousands Beginning ARR Year 1 Beginning ARR 20,000 NNARR Year 1 NNARR Year 2 NNARR Ending ARR 20,000 38,000 OpInc Year 1 OpInc Year 2 OpInc Growth Rate 100% 90% Growth Percentile Top Quartile Top Quartile Burn Multiple 2.0 1.4 Add your numbers above to plot your 2-year growth and multiple trajectory. Different Profiles require different plans Once you know how your financial profile benchmarks versus other startups, you can make better decisions about managing cash runway. Bluntly this is because you can also figure out how to manage us, the venture industry, the madman in the car. Some companies can ignore venture advice on extending runway but still attract venture capital dollars easily. Some companies have no choice but to optimize for runway because there is no prospect of raising new capital. In the middle, the vast majority of companies have to manage runway as a binding constraint against growth as the long-term objective. Looking at each quadrant in turn: Companies with high growth rates and a low burn multiple (HGLB) can continue to grow aggressively and do not need to optimize for runway. These companies will be able to raise capital if required. There is always room in the portfolio for a great deal. Current venture advice is least relevant for companies that are the most interesting for venture investors, which is an odd result. Companies in the bottom right quadrant, (LGLB) need to aim for cash flow breakeven, ideally without having to raise more capital. If capital is needed the growth profile may not be compelling enough to raise a venture mega venture round, but a smaller round or venture debt should be enough to get the company to break even. It is also possible that post this market craziness, growth ticks up again, so companies at the top end of the quadrant can look to reaccelerate into the top right quadrant. In the bottom left quadrant (LGHB) the choice is clear. Now is the wrong time to raise money with this profile, so you have to extend your runway regardless of the impact on the business. It may not be possible to build a realistic plan to get all the way to breakeven but what you can do is get time in which to alter the performance of the business and get out of the low low quadrant. We do see companies move quadrants up about 30% of the time on an annual basis. Burn is not Burn Multiple The whiplash quadrant is the High Growth High Burn one. Because venture is rightly biased towards growth, companies in this quadrant have been able to easily raise venture money in the last few years often at amazing valuations. Indeed, by pushing capital on companies, the venture industry has effectively forced efficient companies into this quadrant. Now in the space of six months, the rules have completely changed. The advice now is, capital is scarce, cut the burn. Cutting the burn is simple in a recurring revenue software company. Headcount is by far the largest expense and so the main way to reduce expenses is to fire people. Given the recurring nature of the existing revenue, the burn is automatically reduced. Changing the burn multiple is much harder because it involves reducing expenses without reducing new ARR. The chart below tells the story. First comes the realization that the FY 22 plan has changed. The growth rate is going down, and unless operating expenses are changed the burn multiple will also go up. The dot has moved from 1 to 2 and the company is going backward on both dimensions. (Ugh downturns are hard). This drives a replan discussion. The decision is to cut expenses, but the big unknown is what is the impact on new ARR. If all the spending being cut was wasted spend, there will be no impact on growth and in terms of the quadrants above the company would just move across to the top right (point 3). This would be a great result but is not likely. If all the spending that was cut was vital for growth, then the impact of the cuts is to move the company directly down to point 4. Obviously, this would be a disaster. The typical result is a blend where the company improves on burn multiple but also slows down, with the dot gliding slowly down to the right (5). The question is how to evaluate the tradeoffs between runway extension v a reduction in growth to a place that makes the company hard to fund. Cut too little and you run out of cash early, cut too much and you survive but will struggle in the future for relevance and capital. These discussions get contentious because there are no easy choices. Investors are looking for growth to bail them out on valuation (remember that 123% revenue growth rate to grow into the valuation above) but at the same time, they want to extend the runway to avoid having to raise money. If extending the cash-out date comes at the cost of lower revenue growth, then it gets harder to square that circle. To avoid confronting this reality investors often resort to the immensely irritating venture advice “sell more and spend less”. Sometimes I think half the money we pay our CEOs is to listen to this s*&t; without murdering someone. On the management side, there is always a natural reluctance to admit how much things have changed and how much spending is now wasteful. Wishful thinking meets anchoring. Tiebreakers: Cash and Valuation The first tie-breaker is the cash balance. If there is enough cash on the balance sheet to fund a slower march to breakeven, management can get the go-ahead for a less maximalist burn reduction plan in the hope of preserving a higher growth rate. If cash is tight the pushback is often to cut regardless of the impact on growth. This shows that for companies in the HGHB quadrant, the role of luck is real. Two companies with the same business profile can end up with dramatically different realities simply because one was lucky enough to raise last year and one was not. This is not fair, but life is unfair. The other tie-breaker is valuation. Price clears all markets and the right way to solve a cash runway issue may not be to extend the runway but instead to raise more money, even in a dramatic down round. The job of management is not to preserve the last round price, it is to build a company. The team at Klarna just did what they had to do to preserve the upside for all investors and even those last-round investors are better off because of the raise. Yes, Softbank looks stupid in the short term but the dilution from the down round is only 10%. If the upside from $50Bn was real before, they still have 90% of that upside. The down round reveals the past valuation error but preserves the chance of future upside. That is the right long-term choice for all shareholders. Do you like your advice decisive or will useful do? This is a far less decisive approach than the simplicity of “always cut the burn to be a fully funded plan”. It is a harder approach to implement (different companies get different guidance), and it dials up the risk (follow-on financing, risk of missing revenue). That is the price to pay for preserving the upside. I think the price is worth paying. Maximizing the runway is not the objective, it is the constraint. If you run out of money you are dead, but if all you do is not run out of runway why bother? The objective is to build a large, sustainable, high-growth business. The challenge right now is not just to survive, it is to survive while preserving enough upside to make the surviving worth the effort. It is also a humbler approach to giving advice. Investing should teach humility, but venture investors can be slow learners. The lesson of the last two years is how incredibly hard it is to predict valuations, the economy, Covid or pretty much anything. In the face of this, the right advice is situational, conditional and should recognize the risk of bias from the advice giver. Finally, I would say good luck managing right now. You have hit the curse of living and managing in interesting times. For additional background on the data and methodology behind the numbers in this post, take a look at this post summarizing some of our key decisions and how we arrived at them, as well as more detailed benchmarks."
    },
    {
      "title": "Natural Language Generation",
      "body": "Great writing is a 10x tool with a powerful higher calling. Warren Buffet’s annual letters (that he drafts with his sisters' names in place of “Dear Shareholder”) are mythologized in the business world for the clarity with which he describes his strategy. Jeff Bezos publishes a similar annual letter at Amazon – and the company requires executives to draft a 6-page memo where peers might use powerpoint. The narrative structure required by great writing benefits both the author and the audience, and it’s a profoundly human skill, in that great writing has rhythm that deviates from the general/expected. But this higher calling is a fraction of the business writing domain. Produced in much higher volumes are marketing copy, emails, performance reviews, code documentation…. the list goes on. In the business domain, most wordcount is a nesting of predictable niceties for the key point(s). Writing this content is a desk worker’s low-skill labor, demanding little skill but lots of time. (The quality of GMail autocomplete, introduced in 2018, illustrates just how repetitive business writing is.) It’s this attractive target that makes Natural Language Generation (“NLG”) products so exciting, because this new technology has finally grokked the patterns interwoven in our prose. NLG products are newly feasible, enabled by linguistic “transformer” models like GPT-3 from OpenAI and Jurassic-1 from AI21 Labs. These models are trained on enormous repositories of text drawn from the internet. As the AI models train, it breaks text up into “tokens,” which largely correspond to legos (“tokens”) for words. When a user prompts a model to author, it executes this by evaluating the text that comes before the cursor and probabilistically calculating what word most fits next (--in ongoing succession). When GPT-3 was first released in May 2020, the technology’s promise seized the minds of hackers. Two years later, the demand from the broader population for NLG products is resounding. Marketing copywriters have been the quickest to adopt NLG products. It’s a highly predictable genre, and one in which the tradeoff between volume of content and thoughtfulness of content leans most toward the former. (Think about the typical “Top 10 Tips” SEO blogpost as an example.) Existing linguistic models can do this job out-of-the-box and perform especially well with a little “prompt engineering” of out-of-the-box models. Early winners in serving this marketing copy market, like Jasper.ai, serve this use case with an inviting user experience, tools for prompting the underlying model, and a strong onboarding experience. Increasingly, NLG is being built into broader systems of engagement as a hook or differentiator, as end-user adoption of the technology expands from early adopters to the early majority. For example: while copywriting tools best serve NLG's early adopters, content marketplaces like Pepper and ContentFly are bringing NLG to the early majority by providing Jasper-like features to already active users of a broader suite. (These companies are two SaaS-enabled marketplaces for buying copywriting services and conducting the draft/review/feedback cycle; each has begun to offer their own GPT-3 based NLG product baked into the marketplace-accompanying product.) Here, the value of the NLG product is synergistic with the value of the broader marketplace and SaaS product suite. I expect that NLG tools for new use cases will successively deeper integrate with relevant systems of engagement to serve broadening adoption, as well as better serve demands for (1) adherence to a genre and (2) programmatic personalization. By genre, I mean the structure that pops into your mind if I say “cold sales email” or “legal motion” or “employee performance review.” Interweaving with the system of engagement describes bringing NLG tooling to the place where the buyer currently does –or can do– most or all of their task. Personalization is the weaving of an intended recipient’s identity into the composition. What’s a next step from marketing copy? It might be email writing, where a few cool products are emerging. One example is Flowrite, a Chrome-plugin for “supercharging daily communication,” enabling the user to select a genre and tap out a few key points which the product turns into a composed email with all the right niceties and nesting. These genre selections, a sort of productization of the priming process, are available in a pre-stocked gallery that can be further enriched by the user. The personalization element is handled here through the few commands a user inputs, like “Interview next tue at 4p via Zoom?” Autobound is another startup totally focused on sales emails, so its problem domain is personalization at scale (--the genre is fixed). The product pulls third party sources for company news and prospect biographics and feeds this, as well as CRM data, into the underlying NLG model. Various cold email drafts are output and provided to the salesperson with in a sidebar within their sales system of engagement (e.g. Salesloft or Outreach). As product builders look to move into increasingly specialized domains, it will require “fine tuning” (customizing off-the-shelf models with additional text repositories) to get deep into the domain’s genre and lexicon. But with such training, it’s easy to imagine really cool use cases. Maybe a tool for writing legal motions that are quotidian but not so template driven as to just be fill-in-the-blank. Perhaps a product that takes as input… Motion to Seal Rationale is that the document in question contains trade secrets The document in question was shared with the counterparty under NDA Cite Acme Corp vs Beta Corp, 62 F. Supp. 2d 463 (D.P.R. 1999) …and produces a submit-able motion with certain portions highlighted for human proofing. Equally cool as a next step is leveraging these natural language models’ capacity to summarize as an input itself. One example that grabs me is Stenography, a tool that reads programmer-written code and generates accompanying plain-English commentary. Viable is another example; the product transcribes and then summarizes user feedback in a voice tailored to C-suite, product, or CX/UX teams. Natural language generation is a particularly cool “wow” example of the way that software is increasingly doing work (with a human in the loop) where it had once just served as the place to do that work. It’s easy to imagine many of the biggest tech exits a few years from now baking language models intro a broader problem-solving software product to make their users even more more efficient and productive. Originally posted on Friday Meeting Scratchpad, Max Abram's Substack."
    },
    {
      "title": "The Opportunity in Usage Based Billing Infrastructure",
      "body": "Though usage-based models are not new—having first been championed by AWS (2005) and Twilio (2008) — there’s been a notable increase in their adoption in recent years. This trend continues to accelerate as the landscape for usage-based models quickly evolves. In 2019, 19% of the Scale portfolio operated usage-based revenue models. Today, that has climbed to a significant 40%. AWS and Twilio first launched usage-based models to enable below-the-app-layer products (with no seats to sell) that were also initially most resonant with startups (whose small size made them poor fits for term licenses). Since then, usage-based models have been much more widely adopted – first by a growing number of similar developer APIs and utility-like products, but also by a broader set of application-layer companies. Most app-layer companies previously used a seat-based subscription model when it was the best choice in town for reflecting value (versus term licenses), but acceptance of charging on different metrics can be a game changer. Consider Zapier, an application-layer product where the value is best reflected by a usage metric (number of automations created) as opposed to by seats (number of users creating automations – usually no more than a handful). Zapier’s go to market motion is built around low cost adoption, with growing ticket sizes with customers who built a lot of automations – often through one or a handful of power users. We’ve spent a lot of time as a firm helping our portfolio understand the differences between operating a usage-based revenue model versus a traditional, subscription-based SaaS model. In an April 2021 conversation of our portfolio CFO working group, we together identified four particularly challenging areas for the FP&A; teams of usage-based companies: revenue forecasting, customer usage projection, enabling ARR-derivate SaaS metrics, and appropriate compensation of sales teams. Our CFOs have continued to evolve their approach to these problem areas, but have noted a lack of established ‘rules of thumb’ and benchmark best practice vendors. These pain points called out by our CFOs make clear the opportunity to sell tools that enable the implementation of usage-based revenue models — tools that are reminiscent of (and potentially bigger than) those developed by Zuora, Chargify, and peers as the SaaS industry first matured. We are particularly excited about the scale of this opportunity for those companies addressing the pain points of usage-based revenue infrastructure. Metering usage and calculating revenue are the primary tech primitives that companies tackling this problem space are focusing on today, though we expect this to enable and expand into tools for sales compensation, cost analysis, and forecasting as the pervasiveness of these challenges persists. Following our April 2021 discussions, we saw a clustered launch of both specialist metering solutions and products that sit downstream of such metering (but often have built in metering functionality today). A lookback on software monetization models Before SaaS (in the 80s and 90s), business software was a shrink-wrapped product sold in a perpetual license model and owned by the customer. The only practicable method of getting software code to its destination was by physical means (e.g. floppy disk, CD, or hard drive). Once it was in the customers’ hands, it remained theirs. Without the capacity to gate access to the product on an ongoing basis, a software company could only entice additional revenue through software updates, new versions of the product or billable support services. In this world, software billing was not particularly a concern. There was no need to link ongoing billing systems to the product, and standalone invoices were easily generated from the ERP. By the late 1990s, increased roll-out and accessibility of high-speed internet carved out a new path for software delivery. This innovation was pioneered by Application Service Providers (ASPs) — a category of some 300 companies that hosted third-party applications and first “rented” software . Soon, the first SaaS companies would emerge by iterating on this model with proprietary, cloud-native software. Third-party application providers accelerated their revenue by simultaneously hosting the software they facilitated subscriptions to. The first ASPs controlled restrictable and revocable access to the software at a primary level — enabling the subscription revenue model at its core. By simultaneously hosting the software being accessed, an ongoing cost structure was created which ASPs also controlled and benefitted from. This ongoing cost structure acted as a catalyst for the profits of the subscription-based revenue model that ASPs were still busy establishing. The subscription model also created new challenges surrounding its execution. racking billing and revenue recognition events (on a separate schedule from sale events) were two primary pain points. As the subscription model permeated, ‘subscription management software’ that enabled its implementation began to emerge. The earliest tools were built for telecommunications and would later be repurposed to charge for SaaS (Monexa, Aria Systems, and Vindicia). They were later followed by SaaS-specialist tools from Zuora (2007) and Chargify (2009). Early blog posts from Zuora — which discussed the initial pain points around subscription-based models — are similar in tone to the discussions around usage-based models happening amongst our CFOs now. “One of the big disconnects in the on-demand market has been the constraints on product packaging imposed by billing systems. The on-demand model takes a product and renders it as a service, but existing billing systems more or less still bill for the service as if it were a product. Confused? Think of it this way: Conventional software sells product by the seat in a one-time transaction, while on-demand currently sells by the seat each month. For most situations, that works pretty well, but it works largely because it’s the only game in town. What happens, for example, if you negotiate a different seat price? Conventional product-oriented billing systems would have issues with that.” – CRMBuyer.com, May 2008; reposted on the Zuora blog; Link. Zuora has since become the leading name in subscription management software, and excels in serving FP&A.; Their functions — such as invoicing, collections, and recognition of subscription revenue —all reflect a primary focus on the billing and processing of recurring payments. Engineering organizations have never been a core constituency for Zuora and peers, for whom a historic depth of integration into product is the identity layer that keeps billing in sync with access. Despite the recent proliferation of usage-based models, execution remains painful and an unsolved problem. The motivated solution-buyer in FP&A; (--and their peer in engineering) cannot turn to the same subscription management providers that orchestrated billing at her last SaaS companies. These solutions were built for subscriptions and are too rigid to effectively integrate any deeper than the identity layer. Enterprise level companies have resorted to building in-house billing infrastructure that tackles this issue. Twilio and Snowflake both employ 50 engineers on such internal products. Among more early-stage companies, exporting this data to Excel is common but a source of anguish. When usage data is demanded in the CRM, CPQ, ERP, data warehouse, and customer dashboard Excel operations begin to buckle at the knees. These tedious, manual exports draw employees away from high-value work. Mid-market attempts to recreate the in-house systems created by Twilio and Snowflake often fail given the unprecedented and “submerged” complexity of the problem. The architecture of a usage-based billing system The architecture of a usage-based billing system consists of three fundamental components: event metering, aggregation of those events to billable metrics, and alignment of billable metrics to a customer’s pricing plan. It’s in the details of those three components that submerged complexities begin to unravel. Event metering: The core of any usage-based metering platform is an API that captures usage events as JSON objects and uniquely identifies the customer, timestamp, event type, and provides any explanatory details of the event that may be necessary to downstream consumers of this datafeed. An event is any discrete measure of usage that could result in a charge (e.g. an API call or a daily snapshot of storage used). In its API docs, Metronome paints the picture of a gate agent at a theme park equipped with a tally counter when explaining the tracking of such events. An event metering function must accurately ignore duplicate events, bind together aliases for a single customer, and be resilient to common failures such as network issues. A loss in revenue may result from any shortcomings in these areas. Building a standardized framework that intelligently defines discrete events as ‘usage metrics’ ” is the biggest challenge the usage-based model faces as a differentiator from incumbent SaaS billing providers. However, if successfully developed this model will provide a broad flexibility that will challenge the rigid, out-of-the box SaaS subscriptions that have enjoyed monopoly over software billing until now. Aggregation to billable metrics: granular usage events must be convertible to predefined billable metrics. While it’s possible to charge customers per API call, a tiered, volume-based, or stair step billing model may be more feasible. Usage-based billing platforms must be capable of applying pre-constructed billable metrics to usage data. Logic functions and arithmetics to carry out such conversions would need to be developed to do so. Combinatively, those functions and their successful interpretation of robust usage data will result in successful event metering functionality. Pricing: pricing plans should produce final billable amounts from such metrics, formulae and processes.t. Billable metrics may not be an infinitely fixed unit — active revision by engineers is possible and will enable the flexibility that is essential to the usage-based model. Pricing engines should enable flexible pricing plans that are based on approved, consistently monitored billable metrics. A pricing plan's relationship to the cash cycle must also be defined i.e. whether charging is based on prepayment credits, or charging to an AR tally. With these three functions, metering products enable the construction of flexible pricing plans, credit prepayment plans or payment determination processes. Go-to-market teams may be freed from the constraints of predefined pricing plans in the prospective stage of sales and marketing. Careful collaboration across the firm will be needed as finance, engineering, sales, and product attempt to unlock the unprecedented benefits of usage-based billing. In order to overcome the pain points that Scale CFOs have highlighted to us, usage-based revenue systems must federate all of the relevant data detailed above into the cornerstone financial systems of the company. The billable metrics and events — and their summary pricing plans and corresponding transactions — must be translatable to central systems across finance (the ERP and forecasting tools), sales (CRM and CPQ), customer success (Gainsight and challengers) and customer dashboards. It’s our expectation that the current pioneers of these functional areas of usage-based solutions are well positioned as the solution to this increasingly broad, and high-value problem. Acknowledgements: Thank you to my teammate Alex Niehenke, Partner at Scale. He was instrumental in creating this post."
    },
    {
      "title": "Short Circuiting Construction: Scale Leads Dusty Robotics' $45M Series B",
      "body": "We are excited to announce that we have led the Series B in Dusty Robotics, which is building autonomous robots (or field printers) that assist in automating the layout process for mid- to large-scale construction projects. This announcement comes at a serendipitous time, shortly after an upgrade to our internal investment process at Scale. Describing innovative software and technology is hard. That is why we are including more visuals and videos in our presentation cadence. Videos of Dusty, with the little robot almost smiling at you, darting around a construction site and seemingly creating the impossible before your eyes, energizes a wonderful enthusiasm for the future. Like all of our investments, it’s worth rewinding the clock several years. We have looked to increase our construction exposure since our investment in DroneDeploy in 2016. Construction represents more than $12 trillion in annual spend as one of the largest segments of our economy but is plagued by workflows that have seen little (if any) evolution over the past few decades. We believe that construction is ripe for automation as well as an increase in precision that manufacturing has undergone. What has been missing is the right technology. Simultaneously, we are excited about innovation in robotics. Advances in the underlying software and connectivity combined with decreasing sensor costs have led to our investments in Locus Robotics and Soft Robotics. We particularly like robotics plays that replace skilled labor (vs unskilled) in markets that are growing or under high labor demand (e.g. e-commerce, construction) Tessa Lau, co-founder and CEO at Dusty, is an early and well-respected pioneer in robotics. She developed startup scars building products that didn’t quite match product needs. That encouraged her to spend countless hours digging into the construction industry, interviewing dozens upon dozens of industry professionals, long before the first Dusty robot was developed. The result was a solution that met almost immediate product-market-fit. Thousands of industry professionals, inspired by those same videos, are reaching out to Dusty Robotics to explore how they can improve layout on their job sites. Construction layout is the crucial step in any large construction project where the digital plans are translated into the physical world. The status quo today is days, sometimes weeks, of time, where trained (and expensive!) professionals use chalk and hair spray (chalk washes away - can’t make this up!) to mark the plans onto poured concrete guiding the work that construction crews will then follow. And this is precisely where Dusty comes into play. Dusty bots can help construction firms that are strained to find talent to deliver layouts on their construction sites in a shorter amount of time for a fraction of the cost - and most importantly - at a level of precision that few humans can replicate (1/16th of an inch!). Risking the danger of hyperbole, Dusty is the Gutenberg Press moment for construction layout. Scale Studio data shows that Dusty’s early uptake in ARR is top decile for its stage, and with an average customer contract size second to none, these data points speak to the immediate value that the team is already delivering. And in many ways, it’s still very, very early days for the business. Like many of our best investments, the Dusty team has a very big vision of how its product can expand from here to solve incrementally bigger problems for its customers. Onwards and upwards, Dusty Robotics. We are incredibly excited to be working with you. And thank you to our new syndicate partners and friends at Baseline, RootVC, Canaan Partners, and NextGen Partners."
    },
    {
      "title": "Hypergrowth Go-to-Market: Toss Out the Playbooks and Start Experimenting Sooner",
      "body": "Billy Sheng is the VP of Inside Sales at Esper, a DevOps platform for managing dedicated devices at enterprise scale. Esper’s first sales leadership hire, Billy has been a central figure in that company’s explosive growth. Below he shares how Esper catalyzed top-tier growth by pursuing multiple GTM motions simultaneously. Product-Led Growth is rightly the go-to sales motion for startups tackling large addressable markets where your product can effectively do the early selling. As the first sales leadership hire at Esper, where we tackled category creation around “DevOps for Devices”, we decided to develop multiple sales motions far earlier than conventional wisdom says you should. Our belief was that PLG alone wasn’t going to be enough to fuel top-tier growth from $0 ARR onwards. In a world where 5x is the new growth rate standard from $1M in ARR, I think of our approach as “PLG Plus” where we leaned on dev-focused self-serve while also simultaneously experimenting with many other sales motions. Infrastructure software for hardware (dedicated device fleets) can be a slow moving sale, so a lot of our work focused on how to accelerate the customer journey knowing that things like testing and buying decisions simply take longer. This blog is a recap of what worked for us. The Standard Playbook Is Evolving with the Times It’s a credit to HubSpot that their GTM strategy remains the standard playbook for traditional SaaS business models. Start with an inbound motion fueled by content to fill the funnel, then sell them something. Optimize percentages. At around $10M in ARR, add another motion like enterprise outbound. For many companies that pursued this approach, the model worked well because teams perfected a single motion before moving onto another. Early personnel decisions were also easier. You just needed to land on a sales formula and hire people who could execute. Your investors were overjoyed if you could triple sales each of your first years (T3D2). The ratcheting up of growth expectations to that 5x level resets the playing field. Companies need to jump into experimentation earlier, especially on their enterprise sales motion or partner channels. It can be harder to hire your first sales lead because you’re going to need them to be effective with two or three or four sales motions. PLG Plus Helped Esper Achieve 5x Early-Stage Growth The decisions we made at Esper came from the fact that we’re creating a new category around DevOps infrastructure for enterprise devices (think of kiosks in retail locations or order tablets in restaurants). This was actually an advantage because from the beginning the founding team was open to creative thinking and new ideas. What started as sometimes painful trial and error turned into a flexible new GTM approach. Here’s what we learned. Smart experimentation. If PLG alone won’t drive enough activity, you need to take other actions to generate a healthy ecosystem in the funnel. Start with a clear understanding across the entire leadership team on: How many sales motions you’re going to attempt Which ones Start by looking at the collective experience of your founder(s) and your sales lead(s). Those motions are going to be the natural place to start. In Esper’s case, our CEO knew first-hand how engineers test infrastructure products from his time at Amazon and Microsoft while our COO had Biz Dev, OEM, and Partner channel experience. I added the traditional inbound and outbound skill set. The key point is to leverage your founding team’s existing expertise. Let that determine how many experiments to try in the early going. I emphasize “experiment” because not all of them are going to work. Timing is everything. For instance, we halted the BizDev motion early then returned to it later when we found the right executive to own that motion. If you find you’re not gaining traction in an area, don’t hesitate to move on. Finding the right sales lead. The traditional sales talent playbook says that hiring a VP of Sales who leans enterprise can slow down early growth. Esper’s playbook of early GTM experimentation meant that a startup veteran VP of Sales might not have seen some of the later stage sales motions first hand, and thus might struggle later. What worked for us was leveraging the founding team’s expertise to fill in the gaps. Remember to make sure that you have your founder’s commitment to dedicate the time to sales. They’re playing more of a role here than they need to using the standard inbound playbook. Know when to cut bait. Understanding why something isn’t working can be hard too. Not every sales motion we tried gained traction right away. It’s critical to know when something isn’t going to work with your current team and/or product maturity. Here is my mental model for making decisions on whether to commit to a particular motion: Try a new sales motion. If it doesn’t attract someone who feels strongly and wants ownership, move on. If someone on the team doesn’t feel that they can make a living with it, cut it. But not (necessarily) forever. Revisit those “failed” experiments as your team evolves. Esper hired a really experienced enterprise sales exec later who came in and resurrected one of our halted channel efforts. That changed the calculus entirely. Marketing’s more complex role. One implication of early GTM experimentation is that Marking has a lot more sales motions to support. We had two big ah-ha moments: Hire Marketing early. Just as your Sales team looks different in this model, so too will your Marketing team. Bring great people early because they are going to have a lot of work to do getting up to speed. To take just one example, you’re going to need custom content to complement each sales motion. Staffing levels will be different. Esper chose to structure teams within Marketing focused on a particular sales channel. Our Marketing team headcount was almost 1:1 with Sales. The inbound playbook would see Sales at 2x or 3x the Marketing team. We were careful not to hire until we had a good sense a particular sales motion was taking off. By front loading our investment in Marketing, we were much more nimble early on. This was another important factor in Esper achieving 4x hypergrowth right out of the gate. Every company faces a unique set of challenges when it comes to GTM strategy. I hope hearing a little about what worked for Esper – solving the challenges of category creation in a high growth world – will help accelerate your own GTM out of the gate."
    },
    {
      "title": "The Secret to Selling Software Is Not Selling Software",
      "body": "The secret to becoming a successful software company might just be to not sell software. Ramp just raised at $8B and claimed to be the fastest growing business software company ever - except they don’t sell software. They give companies a free expense and accounting solution. Just take a look at the pricing page. Ramp replaces software that companies were reluctantly paying for but now don’t need to. Whew, that was close. People hate paying for software. This search for free is what has defined startup business models for the last decade. Many of the biggest exits are associated with having discovered a way to make money without charging for it, at least not directly. This is just the latest iteration of the ongoing evolution in the ways software is sold. Before we realized software was special, we sold it shrink-wrapped at retail, just like any other widget. Software needs updates but businesses didn’t want to pay for them, so the perpetual license plus support model became standard. It took operating a service (SaaS > software) in order to normalize subscriptions. From there, things have gotten much more interesting. You’ll see a clear theme emerging. Consumer internet. When Google and Facebook launched there was this odd question of how to make money when no one was going to buy the software itself. What seems obvious now, wasn’t entirely clear then. Content and streaming. There was a time when people thought you might buy Winamp premium AND content separately. That didn’t last long. Netflix came along selling movies. Spotify, music. Full stop. Even then, can you think of a way not to sell the content and still make money indirectly? (Hint: YouTube, Instagram, TikTok.) Cloud computing. Amazon, Google, and Microsoft sell core-hours, meaning access to hardware. There is a whole lot of software people are getting but they sure aren't paying for it. They have a bunch of products, but most are there to generate EC2 revenue. PLG and Developer tools. Users/Engineers choose and use the software for free, occasionally dropping a corporate card when they need a pay-walled feature. As the cancer spreads, managers and IT wonder if anyone has done a security review. There IT learns that SSO and other governance features they’ve been mandating require $$$. They pony up real cash because at this point, if anything goes wrong it is on them. Open source. An entire global movement to give away software for free, forever! Eventually some of it is deemed so important that IT or Engineering decides they should probably buy some support, hosting, or compliance. Notice even when they “buy software” they aren't buying software but rather support or hosting. They would never actually pay for this stuff. RPA / robotics / automation. The magic of RPA was that consulting firms realized they could sell automation not in terms of their costs (human consulting hours) but in terms of the customer’s savings. \"Bots\" were positioned as human worker replacements that do the same thing but at half the price. They aren't buying software, they are hiring efficient digital workers! Hardware. There is way more software in a Tesla than most startups are producing but customers never pay for it. No, they are buying a luxury sedan that saves the environment. Remember when you used to pay for an operating system upgrade? Apple provides all kinds of software, including the all important iMessage, for free as long as you buy their computers. Fintech. The largest category on the Unicorn list is there because startups discovered it is easier to extract value via a small cut of a payment or transaction rather than directly as a solution. Marketplaces. A spin on the Fintech transaction is the marketplace connecting parties and taking a cut. Scale advisor Niall Wall recently wrote about how you’re better off not charging for your marketplace (software) anyways. Crypto. Many think crypto's fatal flaw is the ponzi scheme, but surely if the equity-style revenue model didn't exist, and its accompanying requisite evangelism, no one would care about crypto. This alternative \"revenue\" model is the category's superpower and potentially its staying power. What's the moral of this story? If you want to create a big software company you might just need to sell something else. More practically, can you position your offering in terms of a not-software alternative like a car or a core-hour? Even better if your tech-enabled alternative is in fact cheaper (bots). Alternatively, can you charge for nothing and extract revenue indirectly? You might point out there are plenty of enterprise software companies that do just fine selling software. This is true, but there are also plenty that try and don’t. It turns out that distribution is more important than unique technology and the combination of new tech that brings new means of distribution is where something special happens."
    },
    {
      "title": "Business Models for Platform Ecosystems",
      "body": "Niall Wall is a veteran of Box’s hyper-growth phase, where he built and led the Business Development, Corporate Development, Channels, International, and Platform businesses responsible for 40% of company revenue. He then went on to lead the Global Partners, Channels, and Business Development organizations at Workday. These days he advises early-stage founders on go-to-market as part of Scale’s Executive Network. This article is about the business models available to startups that want to build healthy platform ecosystems. Long-term success in this area begins with the mix of business model and monetization decisions. From my direct experience at Box and Workday and from dozens of conversations with software companies investing in platform businesses, I’ve concluded there are three broad categories of platform business models. Understanding at the outset which one you should pursue is one of the most important strategy decisions you as a founder will make. There’s no one-size-fits all approach to platforms. But as we’ll see below, only two of them actually work over the long term. The Why and How of Platform Ecosystems What do we mean by a platform? At its core, many companies build a first-party solution, either as an app or an API that serves specific end-customer needs, then license capabilities to third-party developers to extend functionality of that first-party solution. This makes the first-party app more valuable to end customers, which attracts more developers to build more functionality and…Voilà, you have a flywheel that creates significant value over time. Slack, Stripe, and Snowflake are all great examples of platform companies with self-sustaining flywheels and massive valuation outcomes. While it may be obvious, companies should pursue a platform strategy because: Integrations into other applications increases the stickiness of the core first-party app It is a competitive moat that grows wider with each new third-party integration Ecosystem developers build functionality faster than the company can alone All of which leads to a sizeable boost in company valuation; all things being equal, a platform company commands a premium valuation to a non-platform competitor Given the winner-take-all advantages of platform businesses, there are many key decisions, from product strategy, API investments, Dev Rel, and Platform Marketing investments needed. Every one of these is worth an in-depth discussion. But in the early going, it’s important to focus on the optimal choice of business model to build a sustainable platform ecosystem. Different types of platform ecosystems are best built using specific monetization strategies. The Three Platform Business Models Successful platforms have a clear alignment of incentives between the platform company, the third-party developers relying on its API, and end users. There are different ways to achieve this alignment, so platforms come in different flavors based primarily on business model decisions about monetization of the ecosystem. This alignment does not happen automatically. In fact, there are very large, successful software companies generating revenue from their platform ecosystems despite what I consider misalignment (see the Misaligned Marketplace Trap below). I’m arguing that long-term success is best accomplished by pursuing either of the two platform models where incentives throughout the ecosystems are properly aligned. Platform Model #1: The Free Market(place) Approach This type of first-party-only platform is centered on a killer first-party app that the platform company makes easy to integrate into third-party apps. However, the company – by choice or by friction – does not monetize the platform activity as a separate business. Examples Box, Slack, Notion, Airtable Foundational Product Decisions A key early product decision for this type of platform is building core APIs and leveraging those APIs within the first-party app. This tends to mean: Core app is highly extensible and easy to integrate Heavy focus on developer relations Company has a dedicated Platform Engineering Team It’s important here to have absolute clarity on long-term product strategy, especially on whether to allow competitors to build first-party apps on your platform even if these would compete directly with you (as with Salesforce and Veeva). GTM/Business Model These types of platforms usually pursue end-user-based monetization. To give an example, while I was at Box we enabled over 1,600 apps to embed Box content into third-party apps via the Box API, for use cases like sharing secure HIPAA-compliant medical content or financial services apps that shared sensitive financial documents. Internally, we had many vigorous debates as to whether to monetize the APIs that powered these highly valuable use cases. Ultimately we erred on the side of a free marketplace, and helped third-party app builders leverage Box APIs and promote those solutions to Box customers, which drove higher adoption and stickiness of Box within our enterprise customers. The secret was ensuring we found a mutually aligned GTM motion whereby our monetization came from end customers only while consumption was driven by third-party app builders on our platform. We purposefully didn’t charge the app builders for access to customers via the Box marketplace, but instead monitored and charged for API consumption and integrations in premium Box pricing plans. Marketplace Model The decision here is whether to launch a marketplace and then whether to monetize it. A marketplace gives app developers access to customers, and customers ways to discover new functionality. My advice is that marketplaces are effective, but the decision to monetize activity on the marketplace is misguided. Airtable is a great example of a marketplace that contextually promotes free and paid resources like third-party templates; critically, Airtable does not take a cut of that revenue. It is a free and open marketplace. Pitfalls The biggest pitfall in an open marketplace is getting greedy and looking to take a cut of app builder revenue, which runs counter to the end goal of a killer first-party app that is enhanced by a synergistic ecosystem. It is also extremely difficult to build the metering, licensing, app certification, and runtime environments to monetize third-party app activity. In fact, doing so can create a “marketplace trap”. Platform Model #2: The Misaligned Marketplace Trap This type of hybrid platform looks like the Free Marketplace above, but with a critical difference: the platform company takes a cut from third-party developer sales via its marketplace. This model dis-incentivizes the third-party developers (revenue loss) and the end users (higher prices). The model often fails over the long term because it stifles the growth of a healthy platform ecosystem because its incentives are misaligned. Examples SAP, Salesforce, ServiceNow, Atlassian Foundational Product Decisions It is a massive investment to not only harden APIs to enable builders on your platform, even as the back-end processes to meter/charge for it are not necessarily accretive to end customer value. Factors like runtime, distribution, and licensing add further complexity. You also may need to invest heavily in your marketplace to promote these apps and build related processes like curating and certifying third-party apps. GTM/Business Model Typically we see both the end customer paying for the first-party app as well as third-party developers paying for the right to access APIs; the latter is monetized either through Percent of Net Revenue (PNR) or consumption (e.g. number of API calls, network throughput, etc). Marketplace Model Here the platform company monetizes its platform ecosystem by charging app builders to sell on its platform. I saw this at Box, where Salesforce took a healthy cut of Box ARR even though there was minimal value to Box customers for the consumption of Salesforce APIs. It gave Box a disincentive to promote the utility of the Box for Salesforce solution at scale. While companies like Salesforce, ServiceNow, and Atlassian are powerhouses, and nobody would argue they have not been effective in building successful platform businesses, there is a definite trend away from this “taxation model” today, as evidenced by Atlassian’s recent decision to significantly reduce their take rate for cloud apps built on Forge: today 95% of the value goes to the builder. Shopify, Google Marketplace, and others appear to be going in a similar direction by significantly reducing the percentage fees charged to builders on their platforms, and instead monetizing their first-party apps. Pitfalls This is where the importance of a mutually-aligned business model is so clear. Platforms in this category like Salesforce and ServiceNow are charging app builders a fee or “tax” to access data from the System of Record – which is misaligned with the app builder business and thus monetization does not happen to the end customer. Absolutely try to avoid this misalignment if at all possible. It’s a dead end. The future is elsewhere. Platform Model #3: The True Platform This is the “cleanest” platform model because you start out as a platform and a platform is what you sell. This could mean you are in the business of creating an API with clear value for third-party app builders to build on top of, and you monetize only when your API is consumed. It could also mean you start out building a first-party app but start with a consumption model. Examples Twilio, Stripe, Snowflake, Plaid, most of the Hyperscalers Foundational Product Decisions Find an API (e.g. payments or SMS messaging) that has profound value if embeddable into other apps. Make this API highly performant, secure, easy to consume, with extensibility as well as backward compatibility. GTM/Business Model Here, too, we see the perfect manifestation of a mutually-aligned business model: you enable third-party app builders to leverage your APIs, end customers pay the third-parties for those derivative apps, and you monetize based upon mutual end-customer consumption. This business model is almost always consumption based. Marketplace Model These companies will likely invest heavily in building a marketplace that makes it easy for end users to discover and buy third-party apps. The Snowflake App Marketplace is a great example. It shows the company is consciously choosing to not monetize the marketplace in favor of the long-term benefits that come from highly-motivated app builders who have clear incentives to build for the platform. Snowflake monetizes via a usage-based, per-second pricing model. Apps built on the Snowflake platform drive compute and storage which end customers pay for, which creates this mutually aligned GTM motion and ultimately a platform flywheel. Pitfalls Issues to think through include: Relying on monetizing APIs on a consumption model only without building a first-party app may not be defensible over the long term, unless you can create significant barriers to entry quickly via a robust third-party ecosystem. This may have motivated Twilio’s move to build first-party apps on their own platform (e.g. Flex). Customer reaction to consumption model: Will end users agree that consumption is the best way for them to consume your solution? This becomes more difficult if there are comparable solutions using a pure subscription model. Predictability of revenue at scale. When you are a pure consumption business, especially given the competitive dynamics of consumption models, as evidenced by Snowflake’s recent 30% drop in consumption pricing. While arguably the right thing to do, pure consumption models are more difficult to predict. Starting Down the Right Path to Platform There is no one-size-fits-all approach to becoming a platform. But there are a few first decisions that any platform aspirant will need to make: Figure out your platform strategy while you are establishing product-market fit Determine whether there is a credible platform ecosystem play within your chosen market. If so, understand what your platform ecosystem business model will be Then establish the licensing model (end-user-based or consumption-based) that is aligned to that platform business model. Thinking backward from where you want to be at terminal velocity is a great way to ensure that your platform strategy and business model are aligned and powered by complementary incentives between your company, your users, and your third-party app builders. Clarity about what options are available to you from the get go puts you on the path to platform success – and the premium valuation that comes with it."
    },
    {
      "title": "Rule of 40 Does Not Compute for Early-Stage Startups",
      "body": "About two years back, my colleague Sam Baker published a Primer on the Rule of 40, the rule of thumb that at scale a company's revenue growth rate plus profitability margin should ideally surpass 40%. It’s one of the more popular articles on our blog – which is interesting when you consider our advice to early-stage founders: As investors in early-stage enterprise software companies, Scale doesn’t use the Rule of 40 to evaluate prospective investment opportunities. We understand that most early-in revenue, venture-backed startups tend to favor growth vs. profitability. In this context, looking too closely at the Rule of 40 may drive entrepreneurs toward misleading conclusions. The article has an interesting perspective on the lack of standardization in the ways public companies measure and report the Rule of 40. That’s especially relevant to companies as they approach IPO readiness. I’d like to say more about why startups should ignore the Rule of 40 in the early going. It comes down to two factors that together are pretty convincing. The Case Against the Rule of 40 for Startups Around here, we believe that performance metrics should ideally have two characteristics: simplicity and standardization. We’ve called our approach to metrics the Four Vital Signs of SaaS, which is a set of metrics that are easy to understand, talk about internally, and benchmark against other similar companies. This gives us standards to evaluate the Rule of 40. Data from Scale Studio on Rule of 40 performance makes one problem clear. Look at the spread between median RO40 and higher-performing 75%ile companies: The median does tend to converge towards 40% as companies grow larger and achieve scale. But the numbers don’t carry a lot of signal–early-growth companies can do quite well even when they deviate from the 40% benchmark early on. When a metric doesn’t correctly capture the underlying processes it’s meant to measure, you might say it does not compute. And that the case for early-stage companies and the Rule of 40. Any internal planning or goal setting using it is doomed to fail. What metric does give you more signal? We look at actual net cash burn figures. But it’s tough to benchmark. So to normalize, we use Operating Margin because it's both simple and comparable across companies. Here is Scale Studio benchmark data for Operating Margin: The trends here match what your gut tells about the path from the early, high-burn stages to the later, nearing-profitability stages. As you’d expect, the performance tiers converge on cash flow break even in the later stages. Operating Margin is a metric that your teams will “get” far better than the Rule of 40. It’s a good foundation for more sophisticated measurement of how efficiently your spending generates growth–that’s a topic for a future blog post. Check out these related articles on SaaS P&L; and startup burn rates: A Founders Refresher on SaaS P&L; Calibrating Cash Burn A Quick Primer on the Rule of 40"
    },
    {
      "title": "Make Room for Partners in Your Go-To-Market Strategy",
      "body": "Georges Arnaout was the first Customer Success hire at CloudHealth prior to its acquisition by VMWare in 2018. He remains at CloudHealth where these days he is the Global Head of Customer Success, leading an organization responsible for $130M+ in ARR. Georges advises founders on go-to-market strategy as part of Scale’s Executive Network. Direct sales isn’t the only go-to-market sales strategy available to early-stage startups. Where to sell your product is a fundamental decision that founders make early in their go-to-market planning. And while a partner sales channel strategy is right there on the table for practically any enterprise software startup, in my experience most founders give it little or no serious consideration. Direct sales is the assumed default and there’s no real discussion otherwise. This article seeks to clear up a lot of the misconceptions about Managed Service Providers (MSPs, though I’ll just call them “partners” here) as a go-to-market strategy and argue it is valid and available to any early-stage startup. I’ll do so by sharing exactly what worked at CloudHealth, where in 2015 I was the company’s first Customer Success hire. The strategy was simple on paper: invest in Partner Success to drive overall success in the channel. It took some experimentation to get it right but once we did, Partner Success drove improvements in growth rate, churn, net retention, and NPS. Here’s what I learned along the way. Misconceptions About MSPs Early-stage founders often delay developing a partner channel sales strategy because it seems overly complex or inefficient. Arguments against it go something like: Partner sales channels take too much time to develop and enable. The direct model is simple, fast, and easy. Partner sales are great until you lose a partner who then takes a bunch of end customers with them. I don’t want to give up that control. Partner sales means flying blind. We’re not able to monitor the pipeline like we can with direct sales, making it unpredictable. You can probably sense the start of a self-fulfilling prophecy here. What often happens is that even when founders or sales leaders activate the channel model, they tend to under-invest in partner-dedicated Customer Success resources and/or fail to properly adapt organizational structure. Then when the channel underperforms, it reinforces their initial hesitation about investing in the first place. Incentives at the Customer Success level can get misaligned as well. When the channel strategy is seen as a secondary priority (which can be the case when it is activated years after the direct model), the CS leader will naturally prioritize investments in the customers that have the highest ARR or highest potential for NRR growth. Inevitably, that’s not the partner channel. You can prevent all of those downside scenarios by investing in the success of your partners in the same way you invest in the success of your inside sales team. Making the Right Investments in Partner Success We shouldn’t go much further without first defining Partner Success. Partner Success is the technical and commercial partner enablement that creates offerings / services your partners will want to sell. It requires coordinating go-to-market and CX. And it provides broad support for how partners use your platform in ways that ensure the outcomes of their customers are met. While CloudHealth did invest in a channel strategy pretty early on in its journey, the investment in Partner Success did not come until several years later. Once we made the investment, however, we saw immediate ROI in terms of Partner revenue growth, net retention, gross churn, and satisfaction. A lot of hard work and experimentation went into the following list of best practices. Here’s what I found worked well: It is imperative to think of Partners as an extension of your Customer Success team and not as customers. This means that you have to invest in enabling them, providing them with the training, tools, and processes that you would use with your direct customers Separate Direct CS from Partner CS (but under the same CS org). As the vendor, doing the above does not mean enabling your Partners on your Direct GTM processes (i.e. traditional GTM). The definition of success varies between Partners and Direct customers. Your direct customers care about achieving the desired outcome that they purchased your platform for (ideally captured in the sales process). Partners care about revenue, gross margins, Gross Churn, NRR, NPS, etc. the same metrics that you track for your own CS team in your direct GTM. If you enable them correctly, they will be able to provide the desired outcome of their own customers and make the services they create successful. Be indirectly involved. The customers of your Partners are indirectly your customers. While it is not recommended to work with them directly (this defies the point of scaling with Partners), you want to ensure you are guiding your Partners along the customer journey. This can be done by sharing platform usage metrics with Partners on the health of their customers, conducting joint roadmap sessions, conducting joint EBRs with their top customers, etc. Equip partners with the tools for success in a Partner Program. While creating a partner program is not specific to Partner Success, it is important to ensure that all the content and (pre- and post-sales) material needed to support your partners is available in one place. This is also a place where Partners and your sales team share leads and co-sell together instead of competing for customers - something that will quickly end your relationship with the channel partner. Establish clear roles and responsibilities. Establish clarity with the Partner and their customers on who owns what. For example, you will be working with three core groups on the Partner side: The team that will create the service [PM or PMM] The team that will sell the service [Sales] The team that will support the service [CS, Service Delivery, Support] It is important to establish the right connections between those core teams and your team. Wrap Up: Partner Success By the Numbers I’ve talked about Partner Success strategy and tactics. Here’s the impact we saw by the numbers. Prior to the introduction of Partner success in 2017, the revenue originating from the channel represented 17% of the overall CloudHealth revenue and annual Gross Churn Rate was close to 11%. There was also a fast upward trend of the Churn (1% in 2015, 5.7% in 2016, and 10.6% in 2017). After the introduction of Partner Success, the annual average Churn Rate dropped to 1% and has been hovering between 1% and 2% since. Only two years after inception, the channel business grew by 200% and now represents almost half of overall CloudHealth revenue. Net Retention remained consistently above 136%. From a customer satisfaction perspective, the Partner NPS was consistently higher than the Direct NPS and remained consistently above 47.1. By deliberately investing in Partner Success resources, we demonstrated that an MSP-driven GTM strategy could work – and in doing so contribute to faster growth during CloudHealth’s early-in-revenue period when every growth tailwind really mattered."
    },
    {
      "title": "Growth Time: $70M in ARR Left on the Table?",
      "body": "“Perfect go-to-market execution” gets said a lot about how a particular startup came to dominate a giant new software category. Often on the day its founders ring the opening bell on the NASDAQ. I wanted to flip that around and show what (near) perfect go-to-market execution looks like from the vantage point of $1M in ARR. Because as you see below, the difference between perfect go-to-market execution and middle-of-the-road go-to-market execution is about $70M in ARR over just five years. We recently wrote about the first leg of that journey being the new 5x gold standard for growth. A good way to visualize the gap between hypergrowth and median growth is using a 5-year projection chart based on $1M in ARR. Our tool calculates ARR levels for each of the next five years using benchmark data from Scale Studio. I like to remind people that “top decile” growth rates aren’t idealized projections – they’re based on the actual growth rates of the (rare) startups that nailed go-to-market right out of the gate then kept growing. Those are real numbers. Here’s how it shakes out: By just Year 2, top-performing startups are bearing down on $20M in ARR, about double the level of median performers. And median growth is still pretty impressive – you’re moving from $1M to nearly $8M in just two years. You can’t do it without technology that provides real value to customers, in a large addressable market, with a solid plan for getting in front of the right buyers and closing deals. But the hypergrowth path is something different entirely. It’s driven by founders and senior executives constantly making critical decisions and getting almost all of them right. When you factor in the pace of product development and hiring, hypergrowth companies are a whirlwind of activity truly on another level from their peers. My point isn’t to say “grow fast” because, well, of course. That’s the central purpose of a venture-backed startup. Instead, focus on the discipline it takes to ensure the things you choose to do (or not do) are additive to your growth rate. When you unpack “perfect execution” you find a lot of smart decisions along the way. Growth Resources: The New Gold Standard for Growth Scaling: Seed to Series A Content Hub From $0 to $1M: Compounding Growth From $0 to $1M: Extending Your Hyper-Growth Period"
    },
    {
      "title": "Web3 Weekend",
      "body": "Moxie rocked the boat this past weekend when he published his first impressions of web3. It’s worth a read as it comes from someone who knows their stuff (creator of Signal), is outside the web3 crowd a bit, and took the time to build actual things with MetaMask and OpenSea then use that experience to discuss questions of centralization and decentralization. The whole episode has interesting implications for venture investing and for entrepreneurs looking to understand how web3 fits into their roadmap and strategy. Many of the responses to the post seemed to rush to address the technical concerns he raises, mostly that the front end/client/application layer is not well decentralized. But these responses, at least the ones I’ve read, including from Vitalik, the ETH founder, miss some of the broader points Moxie was making. Namely that, relative to centralization, decentralization is fundamentally: Harder, meaning more work to build; Results in worse UX, as measured traditionally for the same amount of work; and Evolves slower, as standards require discussion to form and are really hard to change. Collectively these mean that for any application there is a way to build it centralized and one to do it decentralized, and the former will be faster and have a greater UX, from a traditionally measured perspective. But these won’t necessarily always be the case. The first point can probably be addressed in time with tooling. While there is an existing set of web2 tools, you can imagine that web3 tools could emerge to fill the gap and make development as easy or even easier than web2. The second point could become normalized. Passwords aren’t fun, but we accept them as necessary and put up with them. If a decentralized app is superior to a centralized app, users might simply accept the UX trade off. The third point, though, seems tougher to address. He points out that decentralization is organizationally slower because of committees and standards, etc. And that once they emerge, they evolve slowly. In an ecosystem where speed to market seems critical, with all else being equal, the centralized team will launch before a decentralized one. A pre-launch team needs to decide if they want to launch a new feature in a decentralized way or a centralized one faster. In crypto’s short history we’ve seen the above principal play out a few times where value has quickly accrued to the first-mover platform, and that platform has used centralized development to give it an edge. As Coinbase grew in size, many criticized it for not being a DEX (decentralized exchange). Most accepted that we could get decentralized later and put up with Coinbase for now. Today there are dozens of DEX’s that you haven’t heard of and for good reason, they can be slow, jankey, and unproven. Similarly, MetaMask and OpenSea are centralized apps that gobbled up adoption and became standard faster than alternatives. This is frequently explained as: “It is early, and we can’t decentralize everything overnight. We’ll get to that later.” This is what many of the responses to Moxie post, including Vitalik, focused on. But what is driving decisions about what to decentralize now versus later? It seems the most critical thing, where the gold rush is, gets centralized. Why? Because decentralization is harder (going to take more work), will have a lesser UX, and will evolve slower. Does this mean web3 won’t work? Not necessarily. It does mean that 100% decentralization will be hard, take more time, and evolve slower than we’d expect – in part because several parts of the stack will be centralized for at least a while. It might also mean that web3 won’t work… if users don’t really want decentralization. As Vitalik points out in his response, lots of devs are excited about decentralization and he claims “users generally accept defaults given by developers”. This is a powerful argument. It feels true to me as long as users want the value of decentralization enough to put up with the hard, long, slow combo. As many have stated, web3 could have some cool properties (privacy, digital goods that move with me, etc.) but the value of that coolness needs to be worth the sacrifice of hard, long, slow or we’ll never get there. Instead we’ll accept compromises along the way and be satisfied with them for the long haul. There will always be a new frontier for innovation, and there will always be the option to fill it first and well with a centralized solution. As a venture investor, this has investing implications inside and outside web3. Alongside all the cool decentralized tech from purists, it’s important to also watch for the critical gaps and the fast-moving pragmatists who do anything to solve users’ needs. The users will ultimately decide how much decentralization they want or need."
    },
    {
      "title": "From $0 to $1M: Renewal Rate vs Retention Rate",
      "body": "SaaS metrics can be tricky when different people use the same terms differently. “Renewal” vs. “Retention” is one of those where the two terms are often used interchangeably. Below we look at how they’re used to clear us their differences. In both cases, we’re looking at customer cohorts, or groups of customers that share specific traits. But we’ll set up our cohorts differently and thus learn different things about customer behavior: Renewal Rate uses customer cohorts based on the end date of each customer’s contract; Retention Rate uses starting cohorts, or customers that signed up during the same period, and looks at their behavior over the long term. We created two sample spreadsheets to show how you work with each. Renewal Rate. Use it to find out how many customers re-signed at the end of their contract. To get an accurate view of this, you set up your cohorts by listing all the customers that renewed during a period, ignoring contract start date. The Renewal rate is the best count (customer or $) of whether customers said yes or no when their contracts came up for renewal. As with other churn metrics, you can and should calculate both a customer count number and a dollar-based number. Here’s a look at how to set that up: Retention Rate. Use it to understand how well you’re retaining groups of customers that signed up at the same time. One key insight is looking at the dollar-based calculation to see how much an initial $1 in sales grew over a specified period of time. Here’s a simplified view of retention tracking: The table illustrates something common with early-stage startups: the outsized impact on metrics of losing an early customer. Dive deep into this topic in our Customer Retention series: SaaS Metrics: To Understand Customer Retention, Start with Churn SaaS Metrics: Getting More Signal from the Churn Metric SaaS Metrics: Diagnose Customer Loss with Renewal and Retention Rates"
    },
    {
      "title": "The Next Frontier of Customer Success Software",
      "body": "Customer Success is becoming one of the most critical functions in the modern business world, and also one of the most rapidly evolving. This shift will be marked by an emergence of new technology, a change in the way users interact with software, as well as the insights and workflows that machine intelligence can help generate. Customer Success: The Central Hub of Information Flow No team sits closer to the customer than Customer Success (or “CS”). This is especially true in the world of SaaS, cloud, and other products that are delivered on a subscription basis. CS is one of the key determinants of a new customer's success with a product. It's a case of starting conditions that determine outcomes, informing the product or service that a business ultimately delivers. Post sales workflows are the center of gravity for Customer Success orgs. These include, but are not limited to, customer onboarding, professional services, account management, support, customer experience and education, and in some cases, renewals. But a CS team’s scope often extends much further, overlapping with other organizations, such as sales, marketing, product, or engineering. As such, CS teams often become the central hub of information flow within many modern organizations. Demand For Customer Success Is High (And Not Just Within Tech) Customer Success Managers (or “CSMs”) are now one of the top positions for hire within the US job market. According to LinkedIn, Customer Success ranked sixth last year within the emerging jobs category with 34% annual growth in new openings. According to a recent survey by Totango, 91% of CS leaders have reported growth in their team size within the last twelve months, and this increase has been even more pronounced since the emergence of COVID-19. CS teams are not just expanding within hyper-growth technology startups. The CS hiring wave is hitting some of the largest companies on the planet. Take for example, IBM’s recent move to boost its CS department from 300 to more than 1,000 reps following its $34B acquisition of Redhat. An estimated 72% of CS roles still sit within tech, but a rising percentage of job openings are surfacing within Advertising, Financial Services, Real Estate, and other non-tech segments. Demand for CSMs is being driven in part by the rapid growth of the subscription economy. Subscription businesses are on the rise and starting to surface outside of tech and within some segments of the market that might surprise you. Worthy of note: the three largest providers of cloud computing by revenue - Microsoft, Amazon, and Google - are now all primarily sold on a subscription basis. This ongoing shift is requiring businesses to engage with their customers on a more continuous basis. Not only are modern businesses processing more data today versus at any other point in history, but the ways in which businesses are interacting with their customers are also following a similar trajectory. Aside from the ability to engage in person, customers are now able to reach out to businesses via phone, email, text, IM, social media, as well as many other third-party channels. The byproduct of this setup is that CS teams now have more surface area to cover and a higher volume of requests to field than at any other point in time. We’d be foolish to ignore the impact that COVID-19 has had on every segment of the economy. At no point has the push towards digital transformation been more apparent, and the customer is - by definition - at the center of that journey. In a world where all but a very limited number of professionals have been able to meet in person, companies have needed to figure out how to keep their customers engaged, better understand how their product is working, and where there is room for improvement. Much of this discovery had to be done through digital mediums. The transition from a physical to virtual (or hybrid) world has not just created a window of opportunity for subscription-based products, but also placed more burden on delivering tangible value. Products where ROI was clearly understood were kept. Those where value was more fuzzy were let go. To respond, GTM orgs were reoriented, with the spotlight shifting (and shining more brightly on) CS than it ever had before. The Next Frontier of Customer Success Software As the demand for CS continues to rise, leaders will be forced to confront new challenges. Growth initiatives will need to be balanced with an increasing exhaust of new communication channels and information flows. One of the ways that leaders will be able to unravel this complexity will be to adopt technology that can help make more sense of the world around them. Leaders want to know what their customers are saying, where they should focus, and what actions they should take that will have the greatest impact. Emerging CS software may have some of these answers. We believe that the next wave of CS software will build on top of the foundation that was laid by some of the early movers in the market. Gainsight, for example, was one of the first entrants in the category and built a deep foundation that wound up informing the way that many CS leaders still structure their teams today. The Company created awareness for the CS category as a whole and revealed the need for systems that were distinct from those used by other GTM leaders and were specifically designed for the CS departments. Below are some of the key themes that we believe will start to take shape as the next wave of CS software emerges. One of the most acute pain points that we’ve heard from CS leaders is that their teams use too many different systems to get their job done. We believe that one of the central themes surrounding the emerging CS tech stack will be the consolidation of disparate systems and the data that they help generate. Whether the tool is powering ticketing, telephony, surveys or an internal knowledge base, the next wave of CS tools will help consolidate many of these tools into a single pane of glass. Systems like Catalyst, Planhat, or Quala help pull in data points from other parts of the organization, aggregate customer-level information, and deliver insights that would otherwise have to be stitched together from separate systems. Consolidation will be a crucial driver in helping CS leaders develop a better understanding of the entire customer lifecycle. More connected systems will thread together what may otherwise be distinct segments of the customer lifecycle. Instead of just focusing on implementation, customer health, support, or renewals, CS leaders will leverage tools that deliver a more complete view of their customer’s end to end journey. These tools will help leaders identify who their users are, how they became customers, how they’ve changed the way they’ve engaged with the product, and whether they are doing so in a way that’s helping them drive tangible business results. The key here will be to allow CSMs to merge data from tools that gather information at different moments in the customer journey and use these insights to make more informed decisions around where they focus their efforts. Part of assisting leaders in charting the entire customer lifecycle is the ability for systems to help provide product feedback at scale. One crucial dimension of CS that is often overlooked is the handoff of information from customer interactions to product teams. It is not uncommon for customers to relay important feedback to Customer Success teams, only to have that information get lost in the mix of other initiatives. Companies like UserVoice, Canny, and Productboard help CS and Product teams collect and consolidate customer feedback, and help incorporate leanings from customer conversions that can translate into product improvements. The sheer volume of data within CS will create an opportunity for machine intelligence to deliver insights and optimized workflows. One of the main critiques that we’ve heard from leaders within the space is that it is more challenging than it should be to make sense of the heaps of data that sit in front of them. We believe that machine intelligence is going to help make meaningful improvements in this capacity - not only by helping CSMs (and their managers) prioritize where to focus their time (i.e. serving up predetermined workflows or in-product notifications), but also spotting early signals of churn risk before customers decide not to renew. Companies like Retain.ai, Involve.ai, and Hook.co are taking interesting spins on some of these dimensions, and are helping CS leaders create more proactive workflows that drive adoption and increase the potential for customer growth. Customer Success is no longer just an enterprise-focused motion. An increasing volume of CS leaders are catering to scaled populations, where customer engagement models can be quite different versus their enterprise counterparts. It is unlikely that a single tool will be perfectly suitable for every CS organization, but we do believe it will be important for the next evolution of tools to be flexible (or customizable) enough to accommodate different (or multiple) engagement models - both in terms of how CSMs engage with their customers and how they analyze those interactions. Arrows.to, Rocketlane, and Baton are ushering in more flexibility within the customer onboarding process and Assembled is helping streamline scheduling, performance, and forecasting for both large and small support organizations that want to take a more agile approach. And finally, no matter how effective or intelligent a system may be, none of it matters if the solution is impossible to stand up. We’ve heard countless stories of companies making a switch to a new platform, only to confront the harsh reality of a failed implementation months (or even years) later. The next generation of CS software will be in part judged by its ability to get up and running quickly, and to the extent possible, will rely more on software (vs. people) to fine-tune the way the system works over time. Customers should always anticipate at least some level of complexity during implementation, but we expect that the next generation of CS systems will help reduce some of this friction. Turning The Page Customer Success is one of the most people-centric functions in the business world and one that sits at the center of the information flow in many modern companies. While there is a huge wave of emerging technology heading towards the ecosystem, we believe that the next generation of CS teams will be optimized (rather than run) by software. The era to come will be marked by the harmony of people and systems working together, where the systems will assist CSMs and their leaders to make better sense of the world that they live in - turning massive heaps of data into better workflows and smarter decisions. If you are an entrepreneur building a system to improve the lives of those in the world of Customer Success, we’d love to talk to you. Please don’t hesitate to reach out. Acknowledgements: My teammates Stacey Bishop and Max Abram were instrumental in collaborating on the work that went into this post. Also, a special thanks to my better half, Katie Leighton, whose insights from a decade of CS experience were invaluable in thinking this through."
    },
    {
      "title": "From $0 to $1M: The New Gold Standard for Growth",
      "body": "It was a typical Tuesday and as I was scanning through my Twitter feed, I came across an interesting post. Gokul Rajaram stated that ARR growth rates weren’t what they used to be and instead have been reset at much higher rates. Tripling from $1M to $3M ARR in a year was no longer the “Gold Standard” and instead, top-performing companies were rocketing from $1M to $5M ARR in the same period of time. This concept of tripling at $1M ARR has been written about extensively and used as the first step in Neeraj Agrawal’s post on Triple Triple Double Double Double (or T2D3). I wanted to know whether that 5x figure was correct and if so -- have growth expectations changed? So, using Scale Studio’s data on thousands of quarters for hundreds of companies at $1M ARR, we went back in time and looked at what top-decile growth rates have looked like over the past several years for $1M ARR companies. (As an introduction - or refresher - on Scale Studio, it is our analytics platform that we use for analyzing and benchmarking startup performance. You can read more here.) Two things came out as truths: Historic Triple Confirmed: It is a fact that top-decile companies have historically, from 2013-2020, grown 3x in 12 months once they hit $1M ARR. The Growth Rate Reset: It’s also a fact that, in the past year, top-decile companies have grown 5x in 12 months once they hit $1M ARR. This is the new gold standard for growth at that scale. This data is useful to your annual planning for 2022. If your goal is to attain top-decile growth in 2022, the bar is officially higher -- and along with it, so are investor expectations. On the flip side, the 5x club is a very exclusive one given the historically high growth required for membership. It comes with bragging rights. Read more about the trends in $1M startups in “Seed to Series A” on our blog."
    },
    {
      "title": "From $0 to $1M: Stepping on the (Scaling) Pedal Too Soon",
      "body": "No single metric tells you exactly when to step on the gas and scale, scale, scale. Getting the timing right is hard and one of thorniest decisions founders make in the early going. There's no cookie cutter answer either, but you can do better than just guess by understanding the link between Growth and Sales Efficiency. Intuition tells you that spending more on Sales & Marketing should increase growth (and burn rate) but lower your Sales Efficiency metrics (more dollars in the numerator). Growth and Sales Efficiency are a trade off: you spend more to grow more. To test that intuition, Scale analyzed more than 2,000 quarterly data points on SaaS startups from $1M to $200M in ARR asking: When is the right time to go all-in on scaling your business? Quite surprisingly, our analysis found that: Faster growing companies have HIGHER Magic Numbers than their lower-growth counterparts. What gives? The answer is lengthy (though informative!) and comes down to this: the highest-performing companies have a flywheel effect at work. It's so powerful that it lifts both Growth and Sales Efficiency at the same time. High-growth companies are high growth because they find product-market fit, target a large addressable market, hire the right people throughout the company, and so on. That lets them raise capital more easily, and more of it. Which in turn means they can continue to invest in Sales & Marketing operations and talent. Which does in fact create more growth. The lesson from our analysis is that you shouldn’t rush to scale until you’re sure that your Sales & Marketing engine will generate the highest possible level of new sales growth. If you’re seeing your Magic Number and other metrics bounce around quarter to quarter, it’s probably too early to step on the gas. But when the flywheel starts turning, go for it. There’s a lot to know about sales efficiency metrics. Start here: A Primer on Sales Efficiency A History of the Magic Number The Magic Number Chain Reaction The Wonder Years of SaaS: Balancing Growth and Sales Efficiency"
    },
    {
      "title": "From $0 to $1M: The Magic of 0.7",
      "body": "We recommend that early-growth companies use the Magic Number as their primary sales efficiency metric. Because it’s GAAP-based, it’s easy to track and allows for benchmarking comparisons between companies -- so you can compare how you’re doing relative to the competition. What is a “good” Magic Number? In our Scale Studio dataset of more than 1,000 enterprise SaaS companies (from $0 to $100M+ in ARR), the long-term median Magic Number is 0.7. It moves higher and lower during short time periods, but over the long haul returns to that 0.7 level. It’s, well, magic. A Magic Number of 0.7 means that Sales & Marketing delivers $0.70 in recurring revenue for every dollar invested in those operations. SaaS may not be a perpetual motion machine, but that’s a solid ROI. And of course 0.7 is a good reference point to keep in mind when you’re tracking your startup’s sales efficiency. As long as you’re measuring the GAAP-based Magic Number, you can always compare your most recent results to the 0.7 long-term median or the various Magic Number levels by performance tiers. As with all SaaS metrics, it’s helpful to read about the nuance with Magic Number (or its ARR-based cousin Net Sales Efficiency), which we’ve packaged in the following series: A Primer on Sales Efficiency A History of the Magic Number The Magic Number Chain Reaction The Wonder Years of SaaS: Balancing Growth and Sales Efficiency"
    },
    {
      "title": "From $0 to $1M: The Magic Number",
      "body": "First-time founders, finding themselves running a fast-moving SaaS software company, often run headlong into a particular knowledge gap: the endless number of SaaS performance metrics and the endless number of opinions on which matter most. The metrics available for tracking Sales Efficiency are no exception. So we thought we’d share the efficiency metric we use ourselves when analyzing potential investments, and for counselling our portfolio companies as they grow. It’s called the Magic Number. First the basics: The recurring revenue business model allows for all sorts of useful tracking and measurement. Sales efficiency metrics track business activities related to winning new customers. The Magic Number is a GAAP revenue-based sales efficiency metric that allows comparison between companies (where other sales efficiency metrics do not). Magic Number tracks the amount of new revenue generated for every dollar invested in selling and promoting. From your first dollars in revenue, you’re going to want data on how well your business is attracting customers. How much growth is generated by the dollars you’re investing in Sales and Marketing? We prefer the Magic Number for that metric because (1) it tells you just that and (2) allows benchmarking between companies. That means you can use a tool like Scale Studio to see how your sales motion compares to other startups at similar revenue levels. Which tells you whether or not you should step on the gas and direct more dollars into S&M.; The Magic Number formula: Basically, you’re calculating annualized new (GAAP) revenue as a percentage of last quarter’s Sales & Marketing expense. One assumption here is that the prior quarter’s spending impacts the next quarter’s sales growth. As you accumulate more and more quarters’ Magic Number data, you’ll give yourself a quantitative basis to evaluate all those processes and people hard at work bringing new customers in the door. For further reading, check out Scale’s series of articles on sales efficiency: A Primer on Sales Efficiency A History of the Magic Number The Magic Number Chain Reaction The Wonder Years of SaaS: Balancing Growth and Sales Efficiency"
    },
    {
      "title": "From $0 to $1M: What’s the Difference Between Churn and Renewal?",
      "body": "Customer retention is the business activity focused on making customers so happy that they stick around long-term. Renewal and Churn are metrics that track that business activity. But how and why do you use one versus the other? Let’s take a look. Think of Churn and Renewal the yin and yang of customer retention metrics. As the names indicate, Renewal rates track the number of customers (or dollars) that re-engage with you when their contracts are up. Renewal rates are measured using the “end state” of a time period. It tells you how many customers/dollars renewed as a percentage of those that could have renewed. It’s your contract renewal batting average. Churn is the inverse of Renewal: it tracks the number of customers (or dollars) that stop doing business with you at any time up to their contract-end dates. It’s that simple, though there are a lot of nuances about how to best measure and track churn to get the most value out of it. Now for the why: Why use one or the other metrics? From your first dollars of revenue, you should be tracking both Churn and Renewal rates. You’ll share Churn data to coordinate Sales, Marketing, and Customer Success. And when (if!) Churn starts trending against you, you’ll lean on Renewal rates to tell you a lot about “why?”. For further reading, see our series of primers on customer retention metrics: To Understand Customer Retention, Start With Churn Getting more signal from the Churn Metric Diagnose Customer Loss with Renewal and Retention Rates"
    },
    {
      "title": "From $0 to $1M: A Common Mistake with Churn Metrics",
      "body": "Customer retention matters a whole lot early on when the loss of a single customer has an outsized impact. Early-in-revenue startups need to get sophisticated fast about customer retention metrics. But where to begin? Start with Churn, but after understanding a key thing about Churn math. Churn is one of those concepts that everyone in SaaS understands intuitively, yet there’s no real convention or common definition for how it is calculated and used inside a company. So to get started right, remember this: When your contract period equals your measurement period, your churn rate gives you an accurate count of the $ disappearing from your ARR base during that period. If you have quarterly contracts, measure churn quarterly. Monthly, monthly and annually, annually. Because when your Churn measurement period is different than your typical contract period, you get somewhat arbitrary results. We did a long explanation of why but it comes down to ensuring the base amount of ARR up for renewal in a period matches the dollar amount of business that churns. This one fix means your churn metric properly compares the churn that does happen against the actual windows during which customers could churn in the first place. For further reading, we created a series of primers on customer retention metrics: To Understand Customer Retention, Start With Churn Getting more signal from the Churn Metric Diagnose Customer Loss with Renewal and Retention Rates"
    },
    {
      "title": "From $0 to $1M: Tracking the Growth of Every $1 in Sales",
      "body": "Annualized Net Retention takes longer to say than to explain: It’s the best metric for early-stage startups to understand how every $1 in sales grows over time. The formula annualizes the net difference between churn and expansion dollars. The more net expansion, the higher your Annualized Net Retention metric. Annualized Net Retention (ANR) is a very powerful metric because it reflects the overall effectiveness of all operations that interface with the customer: Sales, Marketing, and Customer Success in particular. Each of these teams has its own set of metrics to track performance, but Annualized Net Retention sits above those metrics to provide a single view of how well your company retains and expands. As you can see from the “Good, Better, Best” benchmarks below, there’s a large gap between good and better performance and top-tier performance. The 162% number for top percentile means those startups turn every $1 in sales into $1.62. You can think of that “extra” as a growth booster -- after all, upsell and expansion make their way into the revenue line. Note that in many cases, median performers tend to struggle more with churn, which cancels out some of the success with upselling. That operational problem is then a drag on those startups’ growth rates. We recommend that founders get started early building ANR tracking into their internal performance metrics. We’ve written a lot about how to get started with churn and retention tracking: To Understand Customer Retention, Start With Churn Getting More Signal from the Churn Metric Diagnose Customer Loss with Renewal and Retention Rates"
    },
    {
      "title": "From Seed to Series A: Growing from 11 to 20 Engineers",
      "body": "This Guest Post was authored by Rob Zuber, CTO of CircleCI. CircleCI is the world's largest continuous integration and continuous delivery (CI/CD) platform. Below, Rob continues from Part 1 to look at finding product-market fit during the very early 10-20 engineer stage. The article below is an excerpt from The Startup Founders' Guide to Software Delivery from CircleCI, available in full here. At 10 to 20 engineers, you probably don’t have a person dedicated to developer efficiency or tooling. You might, but it’s not common. It’s more likely everyone is contributing to the cause. And your team will benefit from deferring creating a dedicated Dev Experience or Tooling team. Why? Because at this stage you will be building this mindset into your culture precisely by deferring the dedicated team. You get further by having everyone be responsible than by relying on a single person. Then people are creating tools, processes, and code that can be shared across teams. At 10-20 engineers, you have some practices in place, you’re keeping your software simple. You’re finding product-market fit. That’s the benchmark. Once you get there, you can invest. Before product-market fit, everything you build has a high probability of being thrown out – the goal is to build as simply as possible so you can pivot or throw things out easily. Once you have fit, you can start making investments. This is the awkward stage where you are trying to manage your ability to work independently without creating unnecessary complexity in your operating environment. Should you break into teams? There are too many people for one team, but your priorities are still shifting. But it does feel cumbersome to be working on the same thing. At 10-20 engineers, you’ll be tempted to build services, but it’s too early. You’d be adding unneeded deployment complexity that you’ll pay for later. Rather, create boundaries and stability with components and libraries. Whether you end up using libraries or just well-defined boundaries between your monolith (i.e. a modular monolith) – you can still pull it all together and deploy in a monolithic fashion This will give you room to operate with more independence, while keeping your deployment simple. Breaking your codebase into services will add a whole new layer of decisions: What kind of inter-service communication should you use? How should you do service discovery? Or retries? There is a massive amount of cost and complexity that comes with your first service. So put it off. You will want to change your continuous integration (CI) process without changing your continuous delivery (CD), or in other words: change your build model without changing your deploy model. What to Do Keep deployment simple. Keep your build and deploy as simple as you can. Resist the urge to create services or overly complicate anything you can defer instead. Maintain efficiency and productivity as you grow past the 2-pizza mark. Look for more fluid ways of creating independent work streams without concrete team definitions. Expose engineering to customers' feedback. You want an engineering team that understands why customers want a given feature and what it is solving. There is a time and place for large product management teams that curate information and focus the team around core principles and maintain consistency across a product portfolio, but this isn't now [early in GTM]. As you hone product-market fit, you can best take advantage of your small team by optimizing for direct channels of communication between customers and engineering and maintaining tight feedback loops. What Not to Do Microservices. Building services where you could use components or libraries instead will create deployment overhead that a team of this size shouldn’t have to deal with. Defer. Read the complete The Startup Founders' Guide to Software Delivery from CircleCI."
    },
    {
      "title": "From Seed to Series A: Growing from 1 to 10 Engineers",
      "body": "This Guest Post was authored by Rob Zuber, CTO of CircleCI. CircleCI is the world's largest continuous integration and continuous delivery (CI/CD) platform. Below, Rob examines best practices for growing a dev team from the very beginning. Part two covers 11 to 20 engineers. The article below is an excerpt from The Startup Founders' Guide to Software Delivery from CircleCI, available in full here. Software delivery on a team of 2 people is vastly different from software delivery on a team of 200. Over the growth of a startup, processes and tool choices will evolve naturally - but either optimizing too early or letting them evolve without a picture of where you’re headed can cost you in time and agility later. That’s why I want to talk to you about how to evolve your delivery process with purpose. The optimal approach to software delivery is tied to your software architecture, which, as we know from Conway’s Law, is in turn related to your organizational structure. Throughout this article, I’ll be offering insights on how each of these factors play a role in setting you up for either turbo-charged growth or mounting roadblocks as you scale, depending on the decisions you make at key inflection points. Founding Stage: 1-10 Engineers This is my favorite stage of company development. You’re excited. You’ve got an idea. And the last thing you want to work on is tooling. At a certain point, you’re deploying your 20th change to your initial environment by hand. You might be thinking “this can’t be the right way to do this.” And you’d be right. Even though you technically can operate like this, this is the time to put CI and CD in place. At this early point in a company, you might not even be in the right business, but rapid delivery and confidence in your releases will be instrumental in helping you get there. This exploratory phase demands simplicity. Coordination costs are low at this size so use that to your advantage: a single team, a monolithic codebase, a basic (automated) deploy. You don’t want to stop and ask your cofounder: “Hey, remind me the Capistrano command?” or “Did I just push on top of your push?” You don’t want your laptop to be the “build laptop” where when someone wants to push code, they have to come to you. With the click of a button, you could have CI/CD. Don’t make it harder for yourself. CircleCI has a free plan. Use it. Just this one single move, even if you don’t “need” it right now (and I’m going to try to convince you that you do), this one move will set you up to be leagues ahead later down the road. When things get real and you’ve got a security problem that’s blindsided you, or you actually have customers and are tackling issues of scale, you can just ship a fix. And that is way better than “Rob’s at lunch, we can’t deploy.” Early-Stage Dev Team Do's... Put CI/CD in place now. Your application is changing a lot and you want to learn as fast as you can. Don’t burn time trying to remember how to safely deploy. When your systems do start to necessitate more complex (and I will explain later why you should defer complexity as long as you can) you will have the practices and tooling in place to handle complex systems. Keep it simple. It’s so easy to have CI and CD in place. Get Google Analytics and set up CircleCI. At this stage, that could be all you need. But not having them is a huge problem. By not having these basic tools, you give your competitors a massive edge. Building a company is all about execution; acquire the tools you need to execute reliably, and put off the rest. Create placeholder implementations. This basically means to defer complexity, but jumpstart processes that support complex parts of an engineering org early). Traceability is a great example of a process that’s relatively easy to set up in a monolith, but much harder to put in once your application has scaled to include services. Why bother? Having traceability drives certain behaviors and decisions. Because there is even a simple implementation, you’ll think about traceability in everything you build. Putting these practices in place early will actually change the way you code. You will end up with fewer asynchronous handoffs in your codebase, or you’ll design them in a way that you’ll be more likely to understand later. Think, don’t act. Knowing what your future constraints are or might be doesn’t mean you have to build to them. Building is expensive, thinking is not. Think through your scaling roadmap before you make decisions so you can make conscious tradeoffs about what you’re deferring to the future. Prioritize operability from the beginning. The first developer in a startup should already take responsibility for thinking about the operation of the software. It should be apparent in the first line of code. For example, the effort to select a logging library that can redirect output to a central system is negligible. While you’re at it, structure your logging and either remove or structure the log points you were using to debug as you built. ...and Don'ts Copy the big players. You are not solving the same problems as Netflix, Google, or Facebook. The advancements they are focused on are unlikely to even translate into your context, let alone be useful as a direct copy. Keep it as simple and flexible as possible. Hopefully you’ll get there someday, but today is not that day. Read the complete The Startup Founders' Guide to Software Delivery from CircleCI."
    },
    {
      "title": "From $0 to $1M: Does the Rule of 40 Matter Early On?",
      "body": "There’s a lot written about the Rule of 40 that applies to public companies and late-stage startups. Which begs the question, is it useful for early-in-revenue startups? Let’s take a look at the numbers. The Rule of 40 boils down to a rule of thumb that a company’s revenue growth rate plus its profitability margin “should be” ≥ 40%. Its formula is as simple as: We think the Rule of 40 is a fine rule, just not for venture-backed startups and certainly not for any company still perfecting its go-to-market strategy. But there are two things that founders should keep in mind about the Rule of 40: It becomes relevant when your company approaches IPO readiness; the Rule is used by some analysts and bankers to compare software company fundamentals. It’s a persistent reminder that profitable growth is a long-term goal; thinking about your company’s business model at scale is certainly valuable in the early going. There’s no getting around the simple fact that venture-backed startups need to optimize for growth not profitability early on. Conceptually, investors often see themselves as underwriting that growth because high growth signals a company has a high-value product and a large addressable market. With success (and scale) comes a natural slowdown in growth rates, and along with that a shift in focus to factors like market expansion or cash flow growth. Keep the Rule of 40 in your back pocket for when you start thinking about your company’s IPO. Further reading on SaaS P&L; and startup burn rates: A Founders Refresher on SaaS P&L; Calibrating Cash Burn A Quick Primer on the Rule of 40"
    },
    {
      "title": "The Third Wave of Restaurant Technology",
      "body": "My teammate, Noah Gross, and I grew up working in restaurants. Some of our first jobs as kids were hosting, waiting tables, running registers, doing inventory, and prepping for the next shift. Really, whatever it took to get food into the hands of our hungry customers. The work was hard and, at times, grueling. There is a certain grind involved in restaurant work that is different from other businesses, and this may always be the case. With that said, the restaurant industry (and food supply chain, more broadly), still relies on countless manual inputs. The industry as a whole has lagged behind other verticals where the uptake of technology has been more pronounced. There are many drivers of this behavior, but as more vertical-specific technologies are beginning to surface, the restaurant industry is starting to become even more digitized and even more automated. In a recent conversation, my teammate, Alex, put an interesting spin on how this shift seems to be taking place. It feels like we are in the midst of what is the third wave of tech transformation (wave 3.0) within the industry. The first wave (1.0) was simply establishing a digital presence (building a website; publishing your menu online, maybe even allowing customers to book a reservation). The second wave (2.0) was characterized by a rush of online information aggregators (e.g. Opentable, Tripadvisor, Yelp, Google, etc.) that created distribution for many restaurants for which it would have otherwise been too burdensome or expensive to generate independently. And then there was part two of this second wave (call it 2.5), out of which spawned online delivery platforms such as Doordash, Uber Eats, and Deliveroo, which instead of acting as an information exchange, literally provided a new conduit for the food itself. What we have recently observed in the market is a third shift that is engendered by a new wave of digital solutions that are returning control of the customer lifecycle (and all of the data and workflows that come with it) back to the restaurant owner (vs. a third party). We have been keeping a close eye on the shift that’s happening and, in the process, have spoken with dozens of emerging companies and restaurant operators to better understand some of the nuances in the market. We thought we’d share a few of our observations below: Setting The Table Everyone needs to eat, but few understand the many underlying complexities and challenges that exist within the ecosystem. According to the National Restaurant Association, there are more than 1M restaurant locations in the U.S. that, before the pandemic, collectively represented more than $1T in annual revenue. The industry is simply massive, but it is also characterized by an unusually long tail of small businesses, with 9 of 10 restaurants employing fewer than 50 staff members. As such, most restaurants do not have the luxury of implementing complex systems and automated processes that often come with scale. Profit margins within the industry are notoriously low (if you are a restaurant pulling in low double digit net income margins, you are probably quite happy). This setup historically hasn’t lent itself for much reinvestment in the business, let alone adoption of new technology. Even at greater scale where one is likely to encounter more systems and automation, replacement cycles can be long and disruptive - which is one of the reasons, for example, you still may encounter point of sale systems, e.g. Micros (now owned by Oracle) or Aloha (owned by NCR), which initially came to market decades ago. The industry is known for unusually high employee turnover, and in recent quarters, rising food costs. These factors, among others, contribute to one of the unfortunate complexities within the industry: a very high casualty rate. Around 60% of restaurants fail within their first year; 80% fail within their first five. A Year of Transition The COVID-19 pandemic in one sense had a catastrophic effect on the restaurant industry. At the end of 2020, the industry ended the year with total sales of $240 billion (27%) short of pre-pandemic forecasts. As of December, 2020, more than 110,000 (more than 10% of all US restaurants) were either temporarily or permanently closed. But in the wake of such challenges, opportunity emerged. The COVID-19 pandemic had a pronounced effect on almost every business within the space, but it also forced a reconstruction of the industry, whereby many owners began to view the adoption of new technology, not just as a business enhancement, but as a necessity. Innovation was no longer an existential crisis, but, for many, a life or death decision. The National Restaurant Association estimates that more than 40% of all restaurant operators adopted new technology during the pandemic to accommodate online orders, as well as contactless and digital payments. Toast, a rapidly growing point of sale solution for small and medium sized restaurant concepts, recently published that in 2020 82% of Toast customers placed their orders online or via mobile app (vs. 51% in 2019 before the pandemic struck). Even if online orders don’t stay at this high water mark, our suspicion is that there will not be a pronounced retreat. Both consumers and restaurant owners have grown comfortable with (and in some cases, greatly benefitted from) the dynamics of online ordering, and this is just one example of a new wave of digitization that is helping return data back to restaurant operators. The Next Course: What Will The Future of Restaurants Look Like? The wave of new tech that is passing through the restaurant ecosystem will bring with it the added benefits of process automation and systems cohesion. As we think about what restaurants of the not-so-distant future will look like, we offer below a set of predictions that we believe will take shape. Restaurants will universally become more digital, data-driven, and defined by automation. The digital properties of restaurants will become the center of gravity of the business. As many restaurants continue to experience some of their highest volume of online orders, operators will shift towards specialized hosting platforms that can accommodate workflows that are different from those in other verticals (e.g. frequent menu changes, specials, loyalty programs, gift cards, and back of the house connectivity). Companies like Popmenu, Lunchbox, and Bentobox are helping operators push beyond the basics of specialized web design and regain control of the customer journey in ways that were previously untenable. Operators will demand the ability to own their own customer data and manage their entire guest lifecycle. Many of the widely popular information marketplaces (i.e. Google, Yelp, OpenTable, TripAdvisor) enabled restaurants to gain mass distribution into new customer segments. But the tradeoff here was that they retained control of all of the customer data (page views, reviews, reservations, etc.) that users generated. A new wave of solutions, such as SevenRooms, is attempting to reverse this trend by returning the data flow back to the restaurant and providing more visibility into the entire customer journey (and, therefore, enabling more customized experiences). Restaurants will take advantage of the API economy that is enabling the connectivity of disparate systems that until recently were unable to communicate with one another. In the same way that Twilio forever changed the world of communications and Stripe did for payments, restaurant owners will be able to take advantage of connectivity that is weaving together workflows both in the front and back of the house, inside and outside the restaurant. Businesses in the likes of Bbot, OneDine, and GoTab are ushering in innovative ways of connecting online and / or in venue ordering modalities with other internal systems and workflows. Restaurants will have a greater reliance on online marketplaces to drive distribution, not just at the end of, but throughout their entire supply chain. Online marketplaces will extend to much earlier segments of the restaurant / food supply chain. Silo, for example, is transforming the way that farmers and growers match with wholesalers and distributors at the earliest segment of the food supply chain. At a later stage in the food supply chain, Rekki and Choco, for example, are abstracting away the painful process that chefs endure to order and reorder ingredients. And of course, online delivery marketplaces such as Doordash and Deliveroo have forever changed consumer behavior by bringing together restaurants and hungry tummies at the very last mile of the supply chain. Restaurants will continue to leverage intelligent systems (including bots and robotics) to assist with order management, reservations, preparation, delivery, and eventually, other customer interactions. Today, we wouldn’t count on your next Michelin star rated meal to be prepared by a robot, but the next time you call to make a reservation or place a takeout order, don’t be surprised if the person who answers isn’t a human. Restaurants of the future will begin to augment the human workforce with automated solutions across various modalities. Kea.ai, for example, has developed an automated bot that uses natural language processing to assist in taking orders over the phone, and Bear Robotics, has built autonomous mobile robots that assist with delivering food and drinks in many well-known quick serve restaurants. We suspect that it won’t be long before robotics find their way into other workflows such as prep, delivery, and clean up. Our Takeaway We continue to be compelled by the current digital transformation that is happening across the restaurant ecosystem. This shift is similar to that which we’ve observed in other verticals, whereby operators can drive better process automation and systems cohesion through the adoption of new technology. To be clear, tech will not be a solution for every industry challenge. There is an inextricable, human component within the food services category that, in some ways, may still be the most integral part of the customer journey -- and that should not be left unsaid. There will, however, be many gaps that technology can bridge, and if you’re an entrepreneur working on an interesting problem that makes things easier for restaurant operators, we'd love to talk to you. My teammate Noah Gross was instrumental in helping conduct the market research and analysis that went into this post."
    },
    {
      "title": "From $0 to $1M: Early Stage Burn Rates",
      "body": "It may come as a surprise, but Scale Studio data on hundreds of early-in-revenue startups shows that a sizable number are indeed profitable right out of the gate. Not many, and it’s not the norm, but they are out there. We pulled data from Scale Studio showing “good, better, best” performance benchmarks for Operating Margin in the $0 to $1M revenue range: Pulling back to look at a much larger revenue range, you’ll see that early profitability is certainly an outlier. From $1M onwards, even the “Best” cohort has a negative operating margin, reflecting the need for external capital to underwrite scaling and growth. The chart above shows Operating Margin, the GAAP-based metric that looks at Operating Income (or Loss) as a percentage of total revenue. It’s formula is simply: For startups, Operating Margin is a good proxy for net cash burn. It has several advantages as a single metric for measuring your runway: GAAP-based means it doesn’t require extra time and effort to track and calculate It’s possible to benchmark Operating Margin versus competitors and other startups It’s just one more step to calculate your runway in terms of time Managing your burn is critical for ensuring your runway is long enough to meet your goals. One easy way to measure your runway at any given time is to simply divide your total cash balance (including unused lines of credit) by the most recent quarter’s operating loss. That gives you your runway in quarters anchored to your current rate of operating loss. Further reading on SaaS P&L; and startup burn rates: A Founders Refresher on SaaS P&L; Calibrating Cash Burn A Quick Primer on the Rule of 40"
    },
    {
      "title": "From $0 to $1M: What’s the Best Growth Metric?",
      "body": "Metrics matter. The Scale team leans heavily on startup metrics for a lot of reasons, but first and foremost it's this: performance data tells a quantitative and qualitative story about a company. That’s a somewhat subtle point: a metric like Net Sales Efficiency is high or low because of realities like the company's sales motion, go-to-market strategy, how much value customers get from the product, and so on. Metrics allow investors and startups alike to gauge the effectiveness of a whole bunch of factors all at once. And speak a common language about the results. The challenge in SaaS is that there are an endless number of metrics, and an endless number of opinions on which metrics matter and when they matter and how to even calculate them in the first place. That’s true especially for growth metrics. But we can untangle some of that to answer a very reasonable question: What’s the best growth metric for founders to use in the early-in-revenue period? Here’s a summary table of three key measures of top line growth: iCAGR (instantaneous compound annual growth rate), NNARR growth rate, and Y/Y ARR growth rate. There are some trade offs. Tracking Y/Y ARR growth rate is easy, but because it’s backwards looking, it has little predictive value. Gathering all the data to track your NNARR growth rate quarter after quarter pays off big, but might be too big a lift during the early-in-revenue period when your Finance team is stretched. Our standard advice is this: just get started where you can, but have a plan for getting more sophisticated with growth tracking as your resources and capabilities expand with time. Dive deeper into growth rates: There’s More to Growth than Growth Rate The Engine of Future Success: Focus on the Growth of Your Growth The Growth Rate Mirage Future Growth in One Number: Scale Acceleration Factor"
    },
    {
      "title": "From $0 to $1M: The Crystal Ball for Growth",
      "body": "What is Net New ARR growth and why does it matter? It’s the growth metric that gives you the best view into your future growth. In a recurring revenue business model, annual recurring revenue (ARR) grows every year by the net difference between the additions (new sales, upsell, expansion) and the subtractions (churn, downsell). Net one against the other and you have your NNARR. And it matters because it’s the single best indicator of your company’s future growth. The formula for Net New ARR growth rate goes like this: We say “crystal ball” because NNARR captures all of the work your company is doing to win, retain, and grow customers. When NNARR growth rate is higher than your ARR growth rate, all of that work means your future growth will accelerate. With the concept out of the way, we can look at some real data to see what makes for strong NNARR growth. Start with a metric we call the Scale Acceleration Factor: divide your NNARR Growth Rate (for a quarter) by your Ending ARR Growth Rate for the quarter. A ratio of 0.5 is median performance where growth is decelerating at an expected pace. Above 1.0, your startup is maintaining or outright accelerating future growth. In our experience (and validated by Scale Studio data), only about 25% of companies are accelerating their growth trajectory. If you’re in that group, move that growth slide to the front of your pitch deck because investors are going to sit up and take notice. Founders’ resources on growth rates: There’s More to Growth than Growth Rate The Engine of Future Success: Focus on the Growth of Your Growth The Growth Rate Mirage Future Growth in One Number: Scale Acceleration Factor"
    },
    {
      "title": "From $0 to $1M: Extending Your Hyper-Growth Period",
      "body": "Take a look at the following charts. They show “good, better, best” growth rates for different revenue bands across different growth metrics. What do they have in common? Any way you slice it, growth tends to spike early then decline (hopefully slowly!) over time. It can be rough on first-time founders: you invest more time and effort, hire great people, and your best days of growth are behind you!? Yes and no. The reason growth rates decline over time is largely just math. Growing ARR from $100k to $1M in a year gives you a large growth rate percentage (900%). But that $900k in new ARR is what you’ll produce in a month when you’re at, say, $15M in revenue. Knowing that growth rates will decline frees you up to focus on extending your high growth period for as long as possible. We recently looked at the performance gap that widens over time when you compare a top-performing startup to a middle-of-the-road competitor. And that gap isn’t just a number. It’s an advantage in time (scaling faster), hiring (easier recruiting), and capital (investors want in). There are countless “median performers” that are ultimately successful, but they’re fighting a different fight than a hyper-grower. Growth decline is a universal law for startups, but the rate at which it happens isn’t. You can slow the rate of decline by doing things like: Selling into a large market. Diminishing returns in small markets are a thing -- if there aren’t a lot of customers to sell to, success will actually burn through your addressable market quicker. Balance providing value with the volume of companies that need your product. Pursuing new use cases. Don’t let new use cases be an accident. Make sure product development is intentional so that you’re continuously opening up new markets. Aligning internally around expansion strategy. Your whole company can contribute to a customer expansion strategy. Product teams should always be thinking about what features will bring additional users onto the platform. Customer Success should measure which levers lead to upsell and expansion. Sales should nurture relationships beyond the initial win. Align your entire organization around a documented strategy to get customers buying more. That list goes on and one. But the theme is this: know from the beginning that you’re not just fighting to grow quickly, you’re also fighting against slowing down too quickly. Build in this awareness from the get-go and you’re setting your company up for long-term success. Take a deeper dive into growth rate metrics: There’s More to Growth than Growth Rate The Engine of Future Success: Focus on the Growth of Your Growth The Growth Rate Mirage Future Growth in One Number: Scale Acceleration Factor"
    },
    {
      "title": "From $0 to $1M: Compounding Growth",
      "body": "Your startup’s growth rate is more than just a number. Or a feather in your cap. Your growth encapsulates a bunch of different things about your company, your people, and your product. Think of it this way, it’s highly unlikely you will grow quickly year after year if: You have the wrong people in key positions Your product doesn’t provide real, lasting value Your operations are a mess Turn that around, and high growth becomes a powerful signal of strength: a strong team, making smart decisions, selling into a large market hungry for your solution. So what counts as a “great” growth rate? There’s no single universal answer. But there is data. During the critical growth window of $0 to $1 million in ARR, here’s what good/better/best growth rates look like, using Forward ARR growth benchmarks from 300+ companies in the Scale Studio dataset: 90% percentile: 1420% 75% percentile: 678% 50% percentile: 365% This shows that the top 10% of companies in the dataset (the “best”) will grow on average 1420% during the $0 to $1M period. While the median (50%) company will grow at 365%. Remember up top where we said growth isn’t some vanity metric. Watch what happens when we compare three hypothetical companies with the good/better/best growth rates. We’re assuming for simplicity that each company ended 2020 with $500K in ARR. We then calculate out how that revenue grows at the 90th, 75th, and 50th growth rates. There’s some complexity behind how this is calculated, but we’ll save that for another time... ...because the outcome needs no explanation. A 90th percentile growth rate is going to open a lot of doors: investors, customers, recruits. The chart is also a good look at what constitutes hyper-growth right now today in enterprise SaaS. How’s your company doing? Scale Studio has Reference Benchmarks covering growth rates, retention/churn, sales efficiency, and cash burn at specific revenue levels. It’s a quick and easy way to understand your company’s performance using real-world data from companies like yours. Founders’ resources on growth rates: There’s More to Growth than Growth Rate The Engine of Future Success: Focus on the Growth of Your Growth The Growth Rate Mirage Future Growth in One Number: Scale Acceleration Factor"
    },
    {
      "title": "Here’s What CISOs are Building (Because No One’s Making Software for Them to Buy)",
      "body": "Scale conducts an annual survey of enterprise CISOs and senior security executives, covering topics from threats to budgets to strategy. We recently released this year’s report: Cybersecurity Perspectives 2021. The report is valuable reading for anyone in the cybersecurity space, and especially for founders (or future founders!) seeking white space market opportunities. The following breaks down the data, the larger trends, and the opportunities that CISOs themselves are talking about. How Enterprises Responded to SolarWinds and COVID The majority of enterprise security leaders (63%) responded to the events of 2020 — a year bookended by the pandemic and the SolarWinds cyberattack — by increasing budgets to fortify their organizations against security threats, with 45% of those nearly doubling spend. Headcount also expanded, growing by 40% last year alone. This data underscores a central theme in the report: Security departments have more resources and visibility inside their organizations than ever before. But those resources are stretched very thin. Therein lies the opportunity for new startups with targeted solutions. Build Versus Build: Where CISOs Are Investing Security’s growing influence and buying power coincides with data showing enterprises are needing to build security tools in several areas: 51% built in-house tools in the past year because they couldn’t find what they needed in the market. Topping the list of homegrown solutions were: Network security (35%) Operation technology (28%) Data privacy (25%) Security automation technology (23%) To put that in context, 100+ enterprises decided they needed to build custom network security solutions because they weren’t satisfied with what was available in the market (if anything). This runs counter to the conventional wisdom that all the basic problems in cybersecurity have been solved; when in fact opportunities remain in many fundamental areas. Another peek inside enterprises comes from the data on investment priorities. In the years that Scale has been running this survey, cloud infrastructure security, cloud application security, and network security have been mainstays atop the list of enterprise spending. Here’s a look back at the past 4 years: The rush to remote work accelerated cloud adoption and exposed new vulnerabilities to unsecured home networks and cloud services. Those areas are likely to remain enterprise priorities moving forward. The investment trends around data privacy and security technology automation, however, tell a more interesting story of opportunities on the horizon. Don’t Forget Data Privacy One surprise from the report was that data privacy experienced the largest year-over-year rise on the list of investment priorities, jumping from 5th in 2020 to 2nd in 2021. Even though regulations like GDPR and CCPA have been in place for a few years, enforcement has lagged, giving the industry time to better understand compliance requirements and the solutions available in the market. But clearly there’s a lot of work still to be done. In fact, the number of enterprise privacy technology solutions climbed from 204 in 2020 to 365 this year. With regulators, governments, and consumers watching how companies collect, use, and protect sensitive data, data privacy has been elevated from a security risk to an opportunity to create a competitive edge from security and reliability. Who’s buying privacy solutions? The survey showed that security executives who report to their CEO are more focused on high-level issues like data privacy, network security, and regulatory compliance. When asked who was ultimately accountable for security, 21% said the CEO, followed by other C-level executives, including the CISO (16%), CIO (15%) and the CTO (11%). The key insight from this data is this: knowing who your buyer reports to can tell you a lot about how that organization prioritizes different areas of security. Security Automation Solutions Are Needed Yesterday Enterprise CISOs are desperate for a more scalable approach to enterprise security management. For some time now, security leaders have responded to threats by expanding their arsenal of security point solutions and then adding headcount to manage the endless alerts those solutions generate. There are signs that things are changing. 3.5 million unfilled cybersecurity jobs means that security leaders can’t keep growing their teams at past rates -- yet they also can’t abandon many (or any) of their security tools. Security automation technology might be the answer and we saw it move up the list of investment priorities from 8th in 2018 to 6th in 2021. We think this trend is likely to continue as tool sprawl and the tight hiring environment means enterprises will happily open their checkbooks to buy security automation in practically any form. You can read the complete Cybersecurity Perspectives 2021 on our website."
    },
    {
      "title": "From Seed to Series A: Hiring Options in the Era of Global Teams",
      "body": "Guest blog from Eynat Guez, CEO of Papaya Global. Founders should know: Managing teams in multiple countries is complex because every country has distinct regulations. A key first step is properly selecting the types of employees your company needs. Contract workers offer flexibility, but many countries are cracking down on misclassification of these workers. The employer of record (EoR) approach allows you to get started quickly because a legal entity in the country isn’t required. For longer-term (5+ year) commitments to a country or region, establishing a legal entity provides greater flexibility. Even before the tumultuous events of the Covid-19 pandemic, companies were opening up to building a distributed workforce and expanding their operations overseas. That process accelerated dramatically over the past year as companies looked for ways to keep growing while keeping their employees safe. Implementing a work-at-home policy was the natural way to maintain operations during the pandemic, and many companies that never considered moving to remote work suddenly found themselves managing a distributed workforce. The experience convinced many corporate executives that they could hire anywhere in the world, not just within commuting distance of their central office. It remains to be seen how those companies will respond as vaccinations spread and the threat of the pandemic begins to recede. One thing, however, is certain. Remote work and distributed teams have become a “normal” part of the work culture. Companies have seen the advantages of the distributed model – hiring from a global pool of talent over not just local talent, entering new markets with feet on the ground, infusing their operations with new ideas and methods – and many will continue to build in that direction. At the same time, compliance on a global scale has never been more demanding. Labor laws in many countries, particularly in Western Europe, are growing in complexity every year. Tax codes continue to change everywhere. Managing payroll manually for global teams, even in just two countries, can be extremely challenging. Managing teams manually in five or even ten countries can be overwhelming. To be honest, running a global payroll is like trying to speak 20 languages at the same time. There are simply too many variable elements to keep straight through Excel sheets and calculators. Fortunately, we live in an age of rapid technological progress. New challenges spur new technology solutions. Global payroll platforms with automated workflows and advanced analytics can make it surprisingly simple to run a global payroll and deliver payments to employees across borders around the world, with all the proper taxes withheld and all authorities and stakeholders paid appropriately. Advanced global payroll software has answered the burning question of “How will I pay my global teams accurately and compliantly?” But before that question can even be asked, there is a more fundamental question to answer: “What category of employee is right for my company?” This article will shed light on how to approach that question. The Three Primary Employment Options There are three main types of employees: contract workers, those hired through an employer of record, and regular employees under payroll. Choosing the option that best serves the needs of your company is the first step in a successful overseas growth strategy. The process begins by defining what you are looking for in a global hire. There are a number of considerations that can help form the decision. 1\\. What is the nature of the team’s activity abroad? Different types of teams need different levels of flexibility. For example, if your primary purpose is to send a small sales team abroad to test a local market for your product, you might choose an option that is quick to launch without tying you down, in case the market proves untenable. On the other hand, if you are opening an R&D; center for an indefinite period, you might want to choose an employment option that offers more stability over flexibility. 2\\. How long is your commitment? The hiring option you choose will be influenced greatly by the level of commitment you have to the specific expansion project. Some companies want to enter a country quickly and stay for a short time. For them, opening an entity slows down the process and makes it more expensive and more complicated. Other companies are committed to staying in a market but want to build the team slowly. 3\\. How large of a workforce do you plan to hire? In many cases, especially in today’s era of remote hiring, a company might just want to hire a single person or a small team of two or three employees. In contrast, another company might have a large-scale expansion plan. Knowing how many you want to hire, both at the initial stage and how many you foresee hiring in the next 1-3 years will influence the hiring option you choose. Contract Workers Contract workers are self-employed independents who are hired for specific, short-term projects. Since they are self-employed, they are responsible for paying their own taxes. In most places, labor protection laws such as minimum wage and extra pay for overtime do not apply to them. Employers are not obligated to provide health coverage or pension. Contractors submit invoices for the work they do and receive their pay in a lump sum. For a company just starting out in overseas expansion, hiring contract workers may seem like an ideal way to keep costs low and the payroll process simple. For many tasks, contractors are indeed a perfect solution. Most companies of all sizes and structures outsource small projects to contractors now and then. However, there are risks in relying too much on contract workers. First, misclassification is a genuine concern, especially when a company relies too much on contractors. Specific restrictions vary from country to country but in general, contractors must be classified as employees if the company controls where and how they do their tasks, and if the jobs are long-term or regularly recurring tasks. Governments have been cracking down on misclassification in recent years because the practice exploits workers who should receive benefits such as health coverage and pensions, and it deprives the government of payroll taxes the employer should be paying. But even if a company is super-careful and avoids misclassification, there is another reason to limit the use of contractors. As temporary workers, they come and go as they please with no commitment to the company. To really grow, however, companies need people they can rely on from day to day. They need a permanent workforce for stability and continuity. Those regular employees can be supplemented by contractors, but over-reliance is risky, both for compliance and for growth. Employer of Record (EoR) An employer of record arrangement allows companies to hire workers in any country in full legal compliance, even if they don’t have an entity in that country. By working with an EoR or a vetted partner, the client company finds the talent and directs the employees in their day-to-day activities, as it would in a regular employment situation. The only difference is that the employees are officially employed by the EoR or its local partner. In many ways, working with an EoR is similar to outsourcing the back office of its payroll department. The EoR handles the administrative tasks of workforce management – collecting time and attendance reports, withholding taxes, calculating salaries and benefits, etc. Most importantly, the EoR, as the legal employer, assumes liability for the employees. An EoR is an excellent interim solution for companies that want to start hiring in the shortest possible time. It’s great for hiring remote teams wherever the talent is located. Hiring through an EoR arrangement does not require opening an entity, which is a great deal of work for the sake of hiring one or two people in any given country. It’s also ideal for companies looking to test a market without committing to it long-term, or for companies that plan a short-term presence in the country. For example, a company with a technology platform may want to send a team of salespeople into a market for a few months to sign up as many people as possible, then to move to another location. They can do it easily with an EoR. The solution works in many cases, but it does have limits. Some countries only allow companies to hire through an EoR for a limited time. Germany, for example, sets a limit at 18 months. As your company’s local presence grows to about 10-15 employees and you plan to stay for the foreseeable future, it may be time to consider opening an entity. At that point, you are probably paying enough in fees to justify the cost of an entity. Regular Payroll Through a Legal Entity If your company has a long-term strategy in a country with a commitment of at least five years, opening an entity may be the best solution. An entity is the most intensive solution in terms of time and money. It takes the longest to get started (unless you start with an EoR while the entity is processed) and costs the most. It places all of the liability on your company, making you responsible for ensuring that all employee taxes are handled properly, and compliance errors can result in fines. It also represents the most stable presence in any country. Some local companies may not want to sign contracts with a business operating through an EoR because they consider it an interim solution. With an entity, you are set up to engage fully in any kind of commerce and contracts. Putting People First An overseas expansion can be remarkably rewarding, but it is essential to remember that every country has vastly different regulations. The more you know about the local laws and restrictions, the more success you are likely to have. The vast regulatory difference between countries is often expressed through the boilerplate employment contracts that are common in each country. While it may be tempting to save time and money by adopting a “fast and easy” onboarding process through a “standard” template, it is highly advisable not to give in to this temptation. Each country is different and signing on a contract that may not suit the local regulations can prove troublesome in time. It is also greatly advantageous to you, the employer, to adopt a people-first attitude to global hiring. That means taking concrete steps to ensure a first-rate employee experience. That’s the best way to build a loyal and stable workforce that will work hard for your company. No matter how great your vision may be, it will never be achieved without a motivated workforce determined to make it happen. That can take many forms, but it is rooted in ensuring that employees are engaged, informed, and feel like equals – which can often be challenging when it comes to a remote team far away from the core office. Creating a policy of equal benefits for all, regardless of location, is a good place to start. An org chart that shows the whole company together as a single unit can also go a long way towards building a sense of belonging. It all starts with choosing the best employment type for your company. That’s how you ensure that the foundation is solid from the start – the key to success in today’s global world of work. Learn more about global employment decision making in Papaya Global’s Ultimate Guide to EOR. About Eynat Guez, CEO, Papaya Global With 20 years of experience in global workforce management, Eynat is a leading expert in HR and payroll management. She holds a BA from The Open University in Israel in Business Administration and International Affairs. She served as COO at LR Group, a global holdings group; founded Relocation Source, a DSP and Global Mobility Solutions provider; and founded Expert Source, an Asian PEO organization. In 2016, Eynat co-founded Papaya Global and became its CEO. Papaya, one of Israel’s fastest growing startups, recently became a unicorn with a valuation of more than $1B. Papaya is developing a global people management platform for all employment needs, from onboarding to payment, in 140+ countries. It has over 200 employees globally, grows 300% year-over-year, and raised over $150M from U.S. investors, including Bessemer Venture Partners, Insights Ventures, and Scale Venture Partners. Eynat puts great value on diversity, both by maintaining diversity within in the company, including a 50% ratio of men to women, and by volunteering at the Israel- Palestinian Economic Initiative led by the World Economic Forum."
    },
    {
      "title": "Cybersecurity Perspectives 2021",
      "body": "We're pleased to share Scale's sixth annual report on enterprise cybersecurity. This year we surveyed 300 security executives in the U.S. who are responsible for buying decisions, the success of security deployments, or the overall security of the company. Covering an extraordinary year for business, the report shows how CISOs responded to the pandemic, SolarWinds, and the many other forces shaping cybersecurity strategy and response. The latest survey includes new data on job satisfaction, org structure, automation, and how companies are managing so-called \"tool sprawl\". Click here to access the full report. Highlights include: The security impact of remote work How SolarWinds changed security operations The importance of having a \"seat at the table\" Automation and managing tool sprawl Continuing investment in data privacy Scale's cybersecurity research efforts date back to 2014, with the Cybersecurity Perspectives series produced annually since 2017. We've archived past reports on the blog: 2020, 2019, 2018, and 2017."
    },
    {
      "title": "Future Growth in One Number: Scale Acceleration Factor",
      "body": "The math of Net New ARR isn’t rocket science. However, when trying to understand the relationship between NNARR, ARR, and the growth rates of these measures, it’s quite easy to get tripped up. At any point in time, your company’s Ending ARR Growth Rate and NNARR Growth Rate are different positive or negative percentages. The relationship between these numbers tells you whether or not your overall growth rate is accelerating. It works out like this: If the Net New ARR Growth Rate is greater than the Ending ARR Growth Rate, the company is accelerating growth. This is a really good thing. It is easier (even expected) at early growth stages. And harder as companies grow bigger. If the Net New ARR Growth Rate is positive, but less than the Ending ARR Growth Rate, the company’s growth is decelerating, but at a typical pace as indicated by the predictable growth decay. Again, growth rates tend to slow as companies get bigger. If the Net New ARR Growth Rate is negative but the Net New ARR dollars added are still positive, then the company is decelerating quickly. So long as NNARR is a positive number, the company is still growing in absolute terms, albeit slowing down rapidly. If the Net New ARR number itself is negative then the company is shrinking. And for a subscription revenue company, this is a terrifying result. Wow, that’s a lot to remember. There has got to be an easier way! The Bright Line of the Growth Acceleration Factor To simplify the math into a rule of thumb that takes just two numbers, we devised the Scale Acceleration Factor. To calculate, divide your NNARR Growth Rate by your Ending ARR Growth Rate for that quarter. Congratulations if your Scale Acceleration Factor is greater than 1. That means that you are accelerating growth. But don’t necessarily fret if your Scale Acceleration Factor is less than 1 as that is what is fairly typical in recurring revenue software models. In fact, based upon our Scale Studio benchmarks, we have found that roughly 25% of companies in any time period are able to achieve a Scale Acceleration Factor of greater than 1, meaning that they are accelerating their growth trajectory. And companies able to keep the Scale Acceleration Factor above 1 for multiple periods fall into the top decile. The balance of companies behave as most do, with a predictable decay in their Ending ARR growth rate. The bottom quartile of companies typically will have a negative Scale Acceleration Factor which indicates that their growth is slowing down dramatically. Remember, we’re talking about growth rates not growth so, in these cases, ARR can still be growing but future growth is at risk. If we look back over the past 2+ years, we can see the impact of COVID on our portfolio of companies and how their Scale Acceleration Factor has fluctuated. For the top decile of companies, COVID provided some tailwinds and accelerated growth even in 1H20. This acceleration continued throughout 2020 and we saw a massive spike in Q121. For the top and second quartile of companies, we saw a precipitous drop in their Scale Acceleration Factor in 1H20 but rapid recovery occurred in 2H20. What’s even more impressive is that the second quartile of companies actually re-accelerated their growth, topping historical top quartile performance! Looking Forward Is this level of growth acceleration sustainable long-term? Practically, no. If the median company continued to accelerate at this pace, the compounding effect of accelerating growth would soon mean that the company’s revenue would eclipse the economies of many countries. But this level of acceleration could well be the norm for the next few quarters, effectively resetting what “normal” growth looks like."
    },
    {
      "title": "Enterprise Automation’s Second Wave: End-to-End Automation",
      "body": "On the heels of UiPath’s announcement of their upcoming IPO, I wanted to highlight how we are thinking about enterprise automation at large and where it’s going. One of the distinctive features of this area is that while most software products assist people in doing the work, these automation products actually do the work for you. Looking at the development of enterprise automation in this light explains a lot about how the market got started (task automation), where it’s at today (end-to-end automation), and where it will go over the next decade and beyond (predictive automation). We’ve spent a lot of time here as a firm because automation is a big complicated topic. We’ve had dozens of conversations with vendors and customers, and wanted to share some of what we’ve learned. The Automation Journey Companies have always wanted to find smarter and faster ways to do their work. The first versions of “automation” were just simply outsourcing work to BPOs. BPM companies like Pega and Appian then came onto the scene to help standardize and structure those workflows into software. In the last two decades, those products moved to the cloud and, in recent years, RPA has really come to the forefront. Over the past few years, the term automation has often been used interchangeably with RPA. While it’s a big part of what is going on, it’s only a part of the story. Several other interconnected categories (IDP, BPM, PM, etc.) play important roles here as we move beyond the first phase of automation. But let’s start with an outline of the overall waves of automation that we’re seeing: Task Automation. When RPA first got popularized, one of the shining use cases was automating rote back office tasks. Customers were just getting introduced to the technology and vendors were just getting familiar with how to successfully sell and implement it. So this converged on low hanging fruit use cases that were low complexity, fast time to implement, aimed at a low-risk way of quickly showing fast ROI. This led customers to use RPA for what was “task automation” or, in other words, automating data entry. It allowed companies still stuck on legacy systems to move data around their internal systems without having to hire an army of people. Workflows like invoice processing and insurance claims processing fit naturally into this. Ideal customers were those stuck on legacy systems and ideal workflows were a combo of both high frequency and low variance. The downside was it only applied to a certain customer type, had a high services cost (both implementing and maintaining the automations), and ultimately worked with only limited use cases. End-to-end automation. While task automation was great to get companies started with using RPA, it was relegated to a limited set of use cases. The introduction of AI combined with increased customer familiarity with using automation in production workflows allowed automation to expand from low-complexity workflows to high-complexity workflows. With the broadening of scope, companies could now focus on end-to-end workflows that could interact directly with their own customers. For example, companies could build an automation to manage an entire customer support flow. Start with ingesting inbound support requests from multiple sources, then use AI to classify them into different buckets, and then execute different actions depending on the classification. This is dramatically increasing the value of the automations and allows customers to really ratchet up the complexity and addressable use cases that they could automate. End-to-end automation moves us from low-complexity/high-frequency workflows to high-complexity/high-frequency workflows. Predictive automation. End-to-end automations interact directly with customers but are still ultimately reactive – they wait for inputs and then run workflows against them. The next level is about moving from reactive to predictive. Customers want automations to not only help execute actions but, more importantly, help drive decision making. An example we’ve heard from customers is wanting to build automations to help account managers monitor their customers. For example, in a manufacturing environment, a company might want to build a bot to monitor consumption and usage for a customer, then pair that with third-party data to forecast what their customer will ultimately need, and then depending on the results either ping the account manager or directly contact the customer to tell them they should buy more to get ahead of any risk of shortage. This automation is high-complexity, low-frequency but, most importantly, high value – and will finally allow automation to flip from back office and cost savings to front office and customer facing, driving higher value. Taking stock of where we are today, it feels like we are squarely past the task automation phase and now deep in end-to-end automation. The combination of tech, customer, and market maturity is giving the end-to-end phase the necessary tailwinds and explains a lot of the current activity here. It also seems like the tech and customer capabilities are not quite yet ready to support predictive automation. Given it took us nearly a decade to transition from task automation to end-to-end automation, we still have some time to go before we move on to predictive automation. Getting there will take a step change in the ability to build products that can systematically discover the right processes, easily design the right bots, and then run them effectively. Trends in End-to-End Automation So we’re in the throes of end-to-end automation. The big picture story is that we are evolving from smaller, simpler automations to longer, more complex ones. This is pushing incumbents to expand their product suites to compete in this world and also bringing new startups into the market from different angles. For example, process mining is getting a lot of attention right now as it’s a necessary component of end-to-end automation – having an accurate map of your current processes is a critical part of building these longer workflows. So lots of new process mining startups and incumbents are acquiring or adding these products to their suites. But there’s a lot of noise and figuring out which approaches will ultimately work is hard. Some observations about what’s happening: Best-of-breed is winning over the suite. A big change in customer preferences starts with maturity. Customers now know what they want, are willing to spend, and aren’t afraid of integrating multiple tools together. The first generation of RPA vendors originally started by bundling everything into a single suite. It reduced risk, promised more functionality, and customers had the “one throat to choke”. As customers have gotten more mature, they’re more focused on performance and willing to stomach more risk by having to deal with multiple vendors. Now it’s more about which vendor is the best for that particular function over a suite that can do it all. And this isn’t just for RPA-adjacent markets, but also for RPA itself! A fight for the orchestration layer. As customers evolve from basic to complex workflows, the number of integrations, decision points, users, etc. has expanded alongside it. Task automation was simpler with shorter workflows, meaning they could all be contained in a single system. But these complex workflows span multiple products and are too big to be contained in a single space. And so there is a fight between all of the major vendors to become the orchestration layer that customers use to manage all of the workflows and the underlying services, monitor uptime/performance, and so on. The hope is that owning this layer is how you own the workflows, which then allow you to own the customer, and ultimately commoditize the rest of the services. Machine + human is a first-class feature. An important part of every complex workflow is the idea of exception handling. While the machines can do an increasing amount of these workflows, humans are still responsible for a healthy chunk of it. And so if we imagine a very long automation, there are likely steps in the middle of that workflow that have a handoff from machine to human and perhaps then back to machine. Whereas task automation had humans involved in the process, they were primarily either at the start or the end of the workflow. End-to-end automation is focused on the coordination and collaboration between humans and machines within the same workflows. So products will need a clean and usable human interface but be built on a framework that allows the “work” to flow seamlessly between humans and machines. There are clear winners in the first wave of automation but the second wave of end-to-end automation is still up for grabs. The winners of end-to-end will be well-positioned to lead the charge into enterprise automations’ predictive future. The (Evolving) Enterprise Automation Market Map Up to this point, we’ve looked at enterprise automation from 50,000 feet. But there’s a lot going on at the startup level, with some really interesting new products tackling process mining and document processing as well as more established categories like RPA and BPM. The endpoint of true end-to-end process automation is still a ways out. But we can see an outline of how the overall market will evolve simply because end-to-end automation by definition requires a lot of different systems to work together really well. Those functional pillars are how we’re laying out our (work in progress) market map: Broadly speaking, being able to reliably and flexibly automate a process from start to finish takes several things: Process mining helps companies figure out what their actual processes are. Programmatically identifying the workflows your people are actually running (and all the variants) is foundational to figuring out how to standardize them and eventually automate. Companies like Skan.ai and others stand out here. Document processing makes documents machine readable. Valuable data is often stuck in PDFs, written forms, unstructured documents, etc. Turning those into structured machine-readable forms are critical to enabling machine-powered automations behind it. Companies like Veryfi and Sensible help developers more easily incorporate this into their products. RPA itself is getting more complex as customers look to scale live bots in the workplace. Robocorp is super interesting as they focus on empowering RPA developers with classic software development paradigms/tools and ultimately giving developers tools to reliably scale their bots. BPM structures human-based workflows. These products take messy human processes and turn them into repeatable flows. Products like Flowdash, Macro, and Tonkean help turn human workflows into processes that are more compatible with machines so we can build a clean interplay between those two. The exciting thing here is how incredibly useful and valuable all of these products are to their customers. They help automate away rote work, standardize workflows for more efficiency, and help elevate the plane that we all work in. Each area is taking their own approach to solving this problem but one thing is clear: the ambition for everyone is to build a full automation platform and own the whole thing. We’re super excited with all of the innovation in this market and please do reach out if you’re working on anything here! Our enterprise automation market map is a work in progress. Send suggestions or additions to chris@scalevp.com."
    },
    {
      "title": "From Lab to Value: Observations on Robotics Startups",
      "body": "Robotics startups deploying integrated hardware / software solutions face challenges distinct from traditional enterprise SaaS ventures. The complexity of this approach changes practically every dynamic inside a company in its early stages, from capital costs to unit economics to sales and go-to-market. Not unlike the era of the first emerging SaaS companies, there is not yet an established script for taking a robotics business from the lab to deployment to ROI in the field. But this uncertainty is balanced against the potential for leadership in massive markets waiting to be won. Here’s a taste of what we mean. Relative to enterprise software companies, robotics startups can face added complexities like: Engineering the technical interplay of hardware and software Managing a longer concurrent hardware/software development cadence Lengthier proofs of concept and sales cycles Complex customer implementations, including onboarding and training Higher sustained capital costs coupled with margin pressure, especially at less mature stages Customer resistance to subscription pricing models Capital requirements for hardware purchases (a byproduct of success is often additional capital expense) Delicate SLAs and expectations for 24/7, instantaneous customer support Scale is an investor in both Locus Robotics and Soft Robotics, as well as companies like KeepTruckin and Root where hardware is a component of the value proposition. Along the way, we’ve met dozens of other management teams that are focused on robotics that serve a variety of different use cases and end markets. Below, we’ve packaged up some observations from speaking with robotics startups that have wound up finding product-market fit. Our hope is that this is the beginning of a conversation with other founders about their thinking on the many paths to value in robotics. Here are a few things we’ve seen work: Accomplishing one task really well. There is often a disconnect between what a robot is capable of doing in the lab (within a controlled, closed environment) versus how it performs in the field (within a dynamic, open environment). The more complex and ambitious the use case, the harder it can be to engineer a commercially viable system. One key is “thinking small.” That is, demonstrating that your system can automate a specific task very well. For example, AMP Robotics’ Cortex system uses computer vision-powered robotics to supplement existing recyclable conveyor systems and their teams, focused on handling the dangerous (and thus expensive) removal of non-recyclables like diapers and other contaminants. AMP does not sell a robot recycling factory in a box; it sells units that can be inserted on top of an existing conveyor system and quickly add value. Very few (if any) robotics solutions handle a broad range of tasks out of the box. By marrying software and hardware to automate one task or a discrete set of related tasks, robotics startups can help customers more easily achieve their SLAs - maximizing time in production while minimizing time offline - while also containing costs for maintenance and repairs. Focusing on high-frequency tasks that amount to massive markets. Enterprise customers don’t need robots per se. They need better, cheaper, and faster alternatives to the status quo. A corollary to “think small” is that automating a single task -- when it’s a high-frequency task central to a big, complex, expensive system like e-commerce fulfillment -- can multiply out to massive market potential. All without doing backflips. Take ecommerce order fulfillment, for example. Every time a consumer clicks the “buy” button, a team of associates travels around a warehouse to pick the products that have been ordered and place them in a bin to be packaged and shipped. Each day in 2020, Americans spent almost $2.2B online, which amounts to a lot of associates, doing a lot of picking, throughout a lot of warehouses, over and over again. Locus Robotics didn’t set out to automate the entirety of warehouse operations but instead identified a wedge in reducing the amount of time warehouse associates spend traveling from one item to the next. Path Robotics is another good example of a team that is focusing on a single task that amounts to a massive market. Path’s automated welding system augments manufacturing assembly lines with high-quality welds, tapping into a market for skilled welders that’s around $20 billion annually even before the estimated 400,000 skilled welder vacancies expected by 2024. Demonstrating tangible ROI to customers. Enterprises are engaged in a constant process of increasing output and reducing costs. They’re going to be responsive to robotics companies that can demonstrate a clear case for business value and tangible ROI on those terms. Customer testimonials that speak to that impact are particularly powerful in this ecosystem. That business value, however, can take many forms. Locus Robotics has focused on increasing the number of picks and packs that the same number of warehouse associates could otherwise do. That’s an easy-to-measure productivity boost based on a comparison of all-in costs. Balancing capital costs, sales cycles, and unit economics. Robotics startups need to figure out how to finance their hardware components when the business model starts to scale. Robots can be costly, and as new systems are sold, additional capital is required to purchase the hardware. This can add another layer of complexity to the already delicate balance of managing costs during a startup’s early stages. Subscription revenue models and customer prepayments can of course help alleviate this burden, as can the declining costs of producing robots at greater scale. It is not uncommon to experience lengthy sales cycles within the robotics ecosystem. Task automation overhauls can be heavy investments, and can therefore be accompanied by lengthy qualification and rigorous testing. These factors, of course, aren’t unique to robotics startups - many large enterprise SaaS platforms or software (especially those aimed at regulated verticals) can have longer sales cycles as well. The flipside to this equation, however, is that once a system is set in motion and demonstrates productivity uplift, it can become very difficult to pull out. What this all amounts to is that the financials of an early stage robotics startup may look quite different from your standard, plain vanilla SaaS business. There often isn’t a clean build of net new ARR quarter after quarter, and there will likely be more upfront capital costs, lower gross margins, and a higher R&D; operating line than many venture investors are accustomed to seeing. With scale, however, these unit economics can start to shift dramatically and, over time, start to resemble their early stage SaaS counterparts. Offering a low(ish) implementation threshold and high real-world performance. For a standard software company, streamlined customer onboarding is a hugely important factor in long-term customer retention. This dynamic is even more pronounced within the robotics ecosystem. Getting robots up and running is not easy and can be a drain on resources. Too much friction can erode customer goodwill and degrade the overall value proposition. Ideally, the robotics company should figure out a way to share some of the implementation lift with the customer. But this is more of an ideal than reality much of the time, especially in the early going. Once systems are up and running, systems also need to perform to real-world standards. Too often, robotics work well within fixed, controlled environments but then fail in real-world settings. Thus, one major requirement is being able to adapt/respond to unexpected interference. Here, a diverse set of customers can make the difference, providing the real-world data a company needs to generalize performance. Not just working alongside an existing system -- but becoming the system itself. As we’ve discussed, many robotics startups start their journey by automating a specific task that is one part of a much larger existing system (take shuttling goods from point to point within an e-commerce warehouse environment, for example). But there are some circumstances when the reach of the robot (and the software that powers it) can extend beyond automating a specific task and begin to act as the control plane for the system as a whole. Continuing with the example above, in addition to helping shuttle goods from point to point, a robotics system that also helps warehouse operators optimize the flow of goods and people throughout connected warehouse environments becomes that much more powerful. Some of the most promising robotics companies enhance the value of an entire system, and have the potential to earn the financial rewards that accompany this dynamic. Bottom line: It’s all about productivity gains We want to end on an optimistic note after spending time on a few of the idiosyncrasies inherent in finding product-market fit in robotics markets. The reason so many talented founders set out to build robotics platforms is the unrivaled opportunity to become true partners to their customers. These companies “join the team” and become an integral part of their customers’ core business processes and share in the collective productivity uplift and market expansion created by that synergy."
    },
    {
      "title": "SaaS Metrics: The Magic Number Chain Reaction",
      "body": "For even the most focused leaders within early-stage startups, it’s hard to ignore SaaS valuations in the public market. This article is a reminder that building a company with solid fundamentals is the most reliable way to reach IPO viability. As we’ll see in the data below, so many startups are choosing to rush past go-to-market fundamentals that they’re actually impacting the long-term efficiency trends as measured by the Magic Number. Supercharged SaaS Valuations As of late, it has been hard to ignore investor excitement for SaaS in the public markets. We’ve officially entered an era where valuations have reached an all-time high, and despite trepidation among certain investors, multiples have continued to climb. Some seem to think we’ve officially achieved a new level of silliness. “Get out while you can! Make money now!” From every anecdote I’ve ever heard or read about the DotCom bubble, what we’re going through right now seems to have a similar ring. But this isn’t completely unfounded. The last decade of private market SaaS investing -- principally driven by venture capital -- is generating returns that haven’t been seen in many years. According to Cambridge Associates, the median internal rate of return across U.S. venture capital has been higher than in any other U.S.-based public market index over the past 10 years. Money is coming back and institutional investors are cashing checks driven by wins in their venture portfolios. While other segments of the economy are facing a long road to recovery, businesses delivering SaaS and cloud-based solutions are providing the connective tissue that is holding the economy together and moving forward during a time of great uncertainty. The world is becoming digitized, and the cloud has been the driving force. Venture returns as of late have been stellar, driven by liquidity in public markets that are valuing SaaS and cloud stocks at revenue multiples more than three times the long-term median of 5x. In the world of software, investors underwrite growth, and even if they have to squint, are willing to look way into the future to find it. We’ve arguably entered an era where the growth prospects for SaaS and cloud are better positioned (vs. other market segments) than they have ever been, as the business world quickly tilts towards a remote-first orientation. Some investors are more thesis-driven than others, but when the smell of money is impossible to ignore, others take note. Investors from all corners of the financial ecosystem are racing to pile on, and U.S.-based venture managers are now sitting on a record amount of dry powder, with firms from Boston to Silicon Valley eagerly searching to fund the next unicorn. And the opportunity set is seemingly boundless. The number of SaaS and cloud startups receiving first rounds of funding within the last half decade is also at a historical high. Barriers to entry have never been lower, with hosting and other cloud computing costs having plummeted in recent years. And on the application side, there is no shortage of DIY solutions (see my teammate Chris Yin’s recent overview of low-code / no-code) that allow founders to break into the market. Needless to say, starting up a new business in the land of SaaS has never been easier or cheaper. Watch Out for the Magic Number Chain Reaction But despite the wide-spread exuberance, the confluence of all the dynamics above has created the perfect setup for sloppy go-to-market efficiency - in what I like to call, “the Magic Number chain reaction.” On one side, there is a market white hot for public SaaS, lots of recent liquidly that has been driving strong historical venture returns, and billions of dollars in dry powder that has recently flown back into the ecosystem, ready to be invested. On the other side, there are more SaaS and cloud businesses coming into the market than ever before, driven by low barriers to entry, dirt cheap cloud computing costs, and so many low-code / no-code applications to get started that it makes your head start to hurt. At Scale, we’ve often measured go-to-market efficiency using the Magic Number, which is a proxy for the amount of first-year revenue a SaaS business is able to generate compared to the equivalent dollars invested behind it. The long-term median Magic Number across our database is 0.7x, which is to say that a SaaS company at scale will typically spend $1.00 to drive $0.70 in first year revenue, recouping its customer acquisition cost somewhere in the second year. But take a quick look at the chart below, which we first published in our History of the Magic Number: There is a very strong negative correlation between public market valuations and overall go-to-market efficiency (Magic Number). With a few blips (the most recent February - April was pronounced, but very short lived), SaaS has been on a decade-plus bull market run. This dynamic is the byproduct of two factors: competition and speed. In SaaS today, there is no time to waste. Competition has never been stiffer. There are more early-stage SaaS and cloud businesses in the ring grinding it out to achieve unicorn stardom -- and no shortage of venture investors to fund them. For founders, this pursuit becomes a race against time. “Found product-market fit? It’s go-time!” Double down on your pursuit of relentless growth. Management reacts by starting to scale, recruiting marketing, sales, and business development leaders to take aim at all corners of the market. And investors encourage it. If there were once one or two other hungry SaaS wolves on your trail, there are now multiples of that. “These kinds of valuations won’t last forever, so there is no time to waste.” The expectations have now been set for Chia Pet-esque ARR growth. The success in a market like this is perhaps worth more than it has ever been, but so too is the pressure set by the valuations that private investors have recently needed to pay. Get the money, spend it on growth, beat the competition before they have a chance to use the same playbook against you. The Magic Number chain reaction is real. What’s a Founder to Do? But just because hot public SaaS markets can lead to poor sales efficiency in the private markets doesn’t mean that it should. Building and refining go-to-market operations is something core to what we do at Scale, and it doesn’t have to (and shouldn’t) be a zero-sum game. Scaling should be done carefully and methodically. Management should be seeing meaningful signals of repeatability and should have a clear understanding of where to find customers, how to sell to them, and (hopefully) how to retain them prior to pouring crazy amounts of capital into their go-to-market channels. In a world of cheap money and fierce competition, scaling as quickly as possible might seem like the right thing to do. But doing so without the right fundamentals in place can be a very difficult and expensive problem to reverse. Not just in terms of dollars, but in the amount of time it will take you to rebuild. This is especially true in the enterprise segment with long rep ramp times and lengthy sales cycles -- and there can be a long-delayed onset before you’re aware that it might have been better to wait. For every SaaS company, there is a right time to scale go-to-market activity. Don’t let the public markets (or anything else) cloud your judgment on when your company is ready."
    },
    {
      "title": "SaaS Metrics: There’s More to Growth than Growth Rate",
      "body": "There is way more to growth -- and managing for growth -- than growth rate. Yes, your company’s growth rate matters in the process of securing top-tier venture investors. But the analysis that those investors perform on your data room goes much deeper than simply cross-checking how fast your ARR has grown and projecting the rate it will grow in the future. This article covers three methods that the Scale investing team uses to evaluate a startup’s growth. As we’ve done throughout our Vital Signs series this year (covering sales efficiency, cash burn, customer retention, and now growth), we’ll start with an advanced primer then go deeper into the topic in the articles that follow. Stay tuned. iCAGR: Growth Right Now Back in 2016, Scale’s Jeremy Kaufmann published The Growth Rate Mirage, an analysis of how different growth rate calculations yield different results (and tell different stories) for early-stage, high-growth startups. The article introduced Scale’s “instantaneous compound annual growth rate” or iCAGR. The name might be a little unwieldy but it’s value is pretty straightforward: it is a calculation that we use to understand how fast a company is growing right now. Why that’s useful takes some explaining. The most common measure of growth is the quarter-over-quarter / year-over-year growth rate. It’s the rate most everyone in SaaS is using when they say “We expect to grow 150% this year.” The formula is the same whether you’re talking ARR or GAAP revenue: So what’s the problem? There’s not a problem, but there is a subtle shortcoming: the year-over-year growth rate can mask a recent slowdown in growth. Here’s how. Follow the ARR Growth Rate line in the following table of a hypothetical early-stage hypergrowth startup. Now look at what’s going on with Net New ARR: it’s rapidly slowing on a sequential basis. There can be perfectly understandable reasons for why, but we would see that as a red flag. The point is that year over year growth looks strong despite slowing in Net New ARR, a key leading indicator of future growth. That’s where iCAGR comes in. By tweaking the standard compound annual growth rate formula, iCAGR tells us more about recent changes in growth. The formula looks like this: We’ve learned that iCAGR is best used alongside other metrics (it is a weather vane, not the weather) and that a “good” iCAGR varies considerably by revenue level (it’s best used to compare companies at similar stages of growth). You’ll see iCAGR front and center on our Scale Studio Reference Benchmarks page. Definitely spend some time with the original article for the full analysis and commentary. It’s a great primer on a super useful SaaS growth metric. Growth Persistence: A Reality Check on Growth Projections Way back in 2012, Andy Vitus led an effort to answer a tricky question. The question arises at every company at the end of every year, as the annual planning process kicks off: How fast can, or should, we grow? It’s tricky because “can” and “should” will often have two very different answers. “How fast can we grow?” can be calculated because, at any given point in time, a company’s resources (people, processes, capital) will determine a theoretical maximum to how much revenue can be generated next year. And thus next year’s growth rate. Think of the jet engines hanging from a plane’s wings: they can power the airplane up to a certain speed and no faster. “How fast should we grow?” is a more complicated question. First, it can’t be answered without a defined goal. Is your company racing to cash flow breakeven as quickly as possible to reduce reliance on outside capital? Are you maximizing growth to keep your company on the shortest possible path to IPO? Or something in between? The best answer to these questions is often “it depends”. But we wanted to give our portfolio companies better advice than “it depends” and landed on the idea of “growth persistence”. We won’t repeat the original growth decay article, but take the time to read it. As we’ll see in a second, it remains incredibly useful and accurate even eight years later. The first big takeaway is this: For SaaS companies from $1M to $1B in ARR and 10% to 120% current-year growth rates, next year’s growth rate tends to be 85% of this year’s growth rate. That right there is a powerful reality check on your annual planning assumptions. If your ARR falls into the $1M to $1B range, and you expect to match or exceed your current year growth rate next year -- stop immediately and dig deep into your assumptions. Growth acceleration at scale is pretty rare. To see just how rare, here’s the original data set. The points far above the trendline represent companies that greatly accelerate year over year growth. There aren’t that many. (Important note: the analysis omitted companies in the very early, very high growth rate category. Out of the gate, early-stage startups can and should have accelerating growth rates. But even at 120% year-on-year growth, growth persistence is at work.) In case you’re wondering whether that 85% figure has held up: it has. We recently re-ran the analysis on the public SaaS companies. * Note that this data derives from a smaller set of companies than the original analysis. We ran this analysis on the readily available public SaaS comps to verify that the original 85% holds true. We see that annual growth persistence rates jump around but cluster around the original 85% figure. As a rule of thumb, then, 85% holds up -- keep it in mind every year during annual planning. The bottom-line takeaway: Increasing next year’s growth rate is exceptionally difficult for recurring revenue businesses that have scaled beyond the early hyper-growth period. Even maintaining growth takes flawless execution. The Mendoza Line: Defining the IPO Growth Track “How fast do I need to be growing to be interesting to a venture investor?” Here is another question that often gets answered with an “it depends” or a rule of thumb like the Triple-Triple-Double-Double. Not satisfied with either answer, Rory O’Driscoll developed a model called the Mendoza Line to define the minimum growth rate at any revenue level necessary to be on track for an eventual IPO (and see the original article for background on the baseball reference). We defined “IPO ready” as follows: Looking at the realistic low bar of what it takes to be a public company, this implies being at run-rate ARR of $100MM at the time of IPO with a forward (next year) growth rate of 25%+. Factoring in the reality of growth persistence, we derived the line that defines IPO viability at any point in time. The original Mendoza Line of SaaS article does a great job of explaining the model’s strengths and shortcomings. Just know that your company’s early growth rates are critically important because of the compounding effect of growth. The higher above the Mendoza Line early on, the better positioned you are to weather the persistent decay in forward growth as your company scales revenue. Finally, keep in mind that “growth” isn’t some feature of your company that can be modified at will. Your growth rate can be thought of as a score that tells a comprehensive story of how well your company is performing. Improvements to business processes, go-to-market strategies, or enterprise sales outcomes can contribute to improved growth rates. As we stated in the original Mendoza analysis, “diagnosis isn’t death”. There are countless examples of companies that retool and re-accelerate growth. Big picture: Your current growth rate, times an estimate of growth persistence, is your most likely future. To change that future and raise that trajectory requires an act of will to overcome the inertia that all businesses face. It may mean new executives, new products, or changing business models. On the Up and Up We hope this overview of growth metrics and concepts has been helpful. The next part in this series will look at benchmarks on growth rates at various revenue levels. We’ll zero in on the rate of change of the rate of change -- math geeks prepare yourselves!"
    },
    {
      "title": "Creator-User-Buyer Is a Mouthful. It’s Also a Major Trend in Enterprise Software.",
      "body": "If you want to see the future of software, watch what product teams are buying or building. Developers buying software is one of the most important trends in software today. SaaS is well understood and priced. AI may in time be bigger, but it’s unclear just how big or how far the tech can take us. From my vantage point as an enterprise software investor at Scale, the most important trend to watch is what is happening with developers buying their own software. Let’s call them creator-user-buyers. To understand why I say this, we need to look at how software procurement has evolved over the past 30 years. I’d argue that the reason SaaS was such a significant development wasn’t the deployment model but the buying model. Pre-SaaS, IT owned software procurement. They were the buyers but, importantly, not the users. It allowed for a lot of misaligned incentives around external software. While there was plenty of money to be made by software companies, inside the enterprise the IT incentive problem led to suboptimal software. SaaS and the User-Buyer Then in 1999, Benioff launched Salesforce, ushering in the era of the SaaS deployment model and the new buying model that came with it. For the first time, business leaders bought their own software. Because IT didn’t operate the software, they had less influence over the procurement process. This had two important implications. It (1) made for more valuable companies because subscription revenue was more predictable, sales cycles were shorter, and customer long-term value was higher. This is often where people stop in explaining the value of SaaS. And while important, it is not as important as the fact that (2) it led to better software. The economic virtues of SaaS (point #1) have been well documented. The user experience virtues of SaaS (point #2) are critical to understanding how this impacts developers. Enterprise software had historically been bad (difficult to use and bloated with discontinuous features) because buyers were not users. IT and procurement organizations did their best, but they didn’t or couldn’t know what users really wanted. They could compare feature lists, which is what an RFP/Q experience excels at, and thus bias buying decisions towards more documented features and more IT control/customizability. This was more optimal for IT and less optimal for users. Thus, when business leaders and eventually users started to buy their own software, they voted with their dollars and said, “Actually, I want this user experience and I don’t need all the bloat.” This created a tighter feedback loop between creators and users. Rise of the Developer Meanwhile, developers, like business users before them, were also unempowered. They were handed their tools by IT until, a decade after Salesforce, AWS completed its IaaS platform with the launch of RDS, and with it a new buying model. Developers no longer had to ask IT permission to get resources, and soon started buying their own software and tools. Just as significant as IaaS/PaaS to this trend is the increasing prominence of developers inside an organization. Once seen as fungible resources to build what sales-driven CEOs wanted to sell, developers, led by the young founders of internet startups like Google and Facebook, became the new captains of industry. At these companies, engineering is the primary decision-making function. Like finance or sales is in other industries, or was in earlier times, developers became expensive and their productivity became paramount. Both of these groups, first the empowered business users and then the empowered engineering users, adopted similar grassroots buying models. Business users in the 2000s sought out their own software solutions and then tried them out themselves. Content marketing efforts steered them to landing pages where they reached out to inside sales teams, a novel sales motion for the time. More recently, engineers (with business users following their lead) took this “bottom-up” adoption to a new level by shunning salespeople entirely. They wanted frictionless access to try and buy. This approach is what’s come to be called “product-led” growth, popularized by the developer tooling company Attlassian. Product-led growth has mostly been engineering-led growth, meaning it is the product teams, the software creators, that are the early adopters and trend setters. Creator-User-Buyers Slack, Trello, Airtable, and Notion were all built by product teams (engineers and designers) for product teams before spreading to other parts of the organization. It turns out that the only thing that produces better, more usable software than user-buyers (SaaS) is creator-user-buyers. With SaaS, if sales teams wanted a better CRM, they could ask IT and get Seibel or shop themselves and get Salesforce. Today, if a developer wants a better way to track customers, she just makes it or asks friends what they are making. Creator-users have communities of early adopters borrowing ideas from one another to rapidly iterate on the future. They are solving user needs far faster than non-user teams are even aware of them. When something is so good that these creator-users stick with it (rather than build their own slightly improved version), it spreads like wildfire inside and outside of the community--as we saw with chat, kanban, database, and wiki tools. The creator-user-buyer is a big deal because of its sheer explosion in size. Growth in the number of developers is outpacing any other function. In order to keep up with demand, creators are coming out of the woodwork. Whether training in “bootcamps” or self-taught, creating software is no longer limited to formally trained professionals and academics. Looking Ahead Software has evolved from professional buyers (IT) to user-buyers (SaaS) to today’s emerging creator-user-buyers. If a need exists for a new software application, and those that can build it are well acquainted with it, they will be the first to build/discover it. Given developers’ prominence as influencers in business organizations today, this holds true for all horizontal business software. You’d expect it to emerge from early adopter product communities. (As an aside, this also implies that vertical, field, or non-office software still requires product teams to innovate on behalf of customers. Those are still in the realm of user-buyers.) If you want to see the future of software, watch what product teams are buying or building. They are the new enterprise trendsetters. This narrative also suggests that this is the endgame for digital businesses. There’s no room for another *aaS revolution because you can’t get closer to the inspiration center than creator-user-buyer communities."
    },
    {
      "title": "Cognitive Applications with Intelligence Inside",
      "body": "We believe that the next decade will result in the emergence of a new class of application software which we term “Cognitive Apps”. These new applications will seek to make predictions about the world using machine learning algorithms applied to information they gather themselves. Rather than representing an abrupt discontinuity with the past few decades, we believe Cognitive Apps will build on the considerable groundwork already laid in cloud computing, connectivity, and data storage, three trends in which Scale has long invested. Yet, to an historian of technology looking back from 2030, we believe it will be clear that the way we build applications, and, more importantly, the way people engage with software had begun to change in 2020 with the addition of “Intelligence Inside”. The Way We Build Software Over the past two decades, Scale has invested in application software delivered “as-a-service“. In most every software market the old client-server architecture was replaced by a modern SaaS product, invariably at the expense of the large, slower-moving incumbent. There has been a proliferation of new, cloud-native software companies and as of today, SaaS represents the most rapidly growing segment of IT technology adoption. While the SaaS model is by no means obsolete, Scale believes that two large and disruptive infrastructure trends are now combining to enable new entrants in many software markets: More intelligent software incorporating the latest breakthroughs in machine learning and pattern recognition, and Dramatically deeper connectivity to both real-world sensors and between software applications These trends require fundamental shifts in software architecture. For example, machine learning algorithms require the following: Flexible, dynamic access to 100x more compute processors Storage of vast amounts of sensor data that will reshape the world’s database infrastructure Inter-application communication that requires new security protocols We believe public cloud infrastructure is best suited to deliver this new architecture and Cognitive Apps will invariably be built on top of the existing cloud platforms. In other words, we view Cognitive Apps to be the next evolution of Software-as-a-Service. The Way We Use Software Business software applications to date have primarily served to store and report on data. Rather than assisting users in their daily tasks, applications have been the task. The new possibilities ushered in by the changes in how we build software will fundamentally alter the way we utilize software. Users will increasingly expect software to intelligently work for, rather than against us or instead of us having to work for the software. We will become comfortable with software making its own decisions based on what it has learned about the world. We believe that this shift in the way Cognitive Apps will be designed is so substantial that in many cases incumbent software vendors will be unable to modify their legacy applications. Said differently, adding intelligence to an application cannot be achieved as an afterthought. We believe there is tremendous scope for investment both at the infrastructure level and at the application level, and some examples of current Scale portfolio companies that embrace these transformations will follow. Root Insurance Root is a next generation automotive insurance provider that leverages the power of mobile devices, sensors, and machine learning (AI) to optimize the underwriting process. As opposed to analyzing credit score, age, gender, and other attributes on which traditional carriers have relied, Root instead collects and analyzes actual driving data to understand the risk profile of its prospective customers. Root then runs these data points against its machine learning models to determine how well (or poorly) one is driving, and by extension, that person’s likelihood of causing an accident. This setup allows Root to discern good drivers from bad, while providing good drivers with cheaper rates and discouraging more risky drivers by serving up more expensive quotes. As more drivers apply for insurance and the company broadens its insured base, the team is able to hone its algorithms to drive more favorable unit economics for the company vs. more traditional car insurance carriers that rely on static data that are less related to one’s driving behavior. In addition to offering more favorable pricing to good drivers, Root provides a faster, more seamless onboarding process for its customers. The company onboards 100% of its drivers through its mobile application and allows customers to purchase insurance in minutes versus traditional carriers that can take days to underwrite a policy. Today, Root is one of the fastest growing car insurance carriers in the United States. Locus Robotics Locus Robotics is the developer of autonomous mobile robots that improve productivity within e-commerce warehouse environments. The Company’s bots work collaboratively with warehouse associates in order to increase the speed at which items are picked and reduce overall fulfillment time by an order of 2x - 3x. The Locus bots rely on a combination of advanced AI techniques including computer vision, simultaneous localization and mapping (SLAM), and sensor technologies in order to conduct autonomous navigation within the warehouse. The idea is for the bots to be able to map the warehouse floor plan and recognize and avoid objects in real time. These capabilities are provided by underlying computing technologies that allow the bots to sense and perceive the environment around them like humans. The Continued Adoption of the Public Cloud The other dominant trend for the next decade will be the continued adoption of public cloud computing services. For clarity when we say “public cloud computing services” we mean the general-purpose computing platforms offered by companies like Amazon Web Services (AWS) and Microsoft. These platforms can be used by enterprise companies for bespoke internal applications and by SaaS and new Cognitive Apps companies as the compute foundation upon which to build and resell enterprise applications. Scale continues to closely track the proliferation of cloud computing services. Microsoft, which is now the largest player in this market having recently eclipsed AWS, generates more than $45 billion in annual revenue, growing at 39% year on year. Across the world’s largest cloud computing vendors, continued adoption has been driven by increasing accessibility to developers and the expansion of platform capabilities. This delivery model, whereby network access and computing resources are shared and operated by a “public” third-party vendor, also leads to significant economies of scale for customers leveraging the technology. According to Gartner, the worldwide public cloud services market is projected to grow 17% year over year to $266.4B. Scale believes that the inevitable migration of computing workloads from company-hosted data centers to workloads running in the cloud will ultimately force a re-write of the entire stack of technologies used by companies to develop new software, including products aimed at developing, deploying, monitoring, and securing hosted infrastructure and applications. You see our conviction in this thesis in investments that hinge upon this trend, including, CircleCI, BigID, and Matillion. Let’s look more closely at two specific examples: CircleCI CircleCI is building a SaaS platform that provides continuous integration (CI) capabilities to modern developers. The company’s technology promotes faster release cycles, allowing today’s engineers to update application code at a far more rapid pace (a process known as continuous integration). Software delivery today has become a continuous loop, whereby a single workflow ties together an automated build (from source code, to test, to deploy). This process repeats frequently - sometimes 100x per day - and affords freedom for developers that would otherwise be required to review code for stability. CircleCI helps remove this need by ensuring smoother integration into an existing application. The movement towards more agile development is one component of a larger trend that Scale has been closely following - that is, the transition from a monolithic to a microservices architecture, whereby application services are loosely coupled, deployed rapidly, and frequently disposed. The following graphic demonstrates this integration process. BigID BigID is a data privacy management platform that allows large enterprises to better understand their underlying customer data. The company’s core intellectual property leverages machine learning to build data mapping and discovery modules for security and compliance purposes (such as the European General Data Protection Regulation and the California Consumer Privacy Act) allowing stakeholders to understand where personal identifiable information sits, who has accessed it, as well as any associated risks. BigID can run behind the firewall or within the cloud instance of a customer and is able to find personal information (such as Social Security numbers, ZIP codes, protected health information, phone numbers, and private financial information) based on its relation to an identity in any kind of structured or unstructured data set. The company’s platform leverages intelligent search and machine learning algorithms that become more efficient at identifying targeted information as additional data passes through the platform. Two Inter-connected Megatrends We believe that these two megatrends will be intertwined over the next decade. The basic SaaS model will continue to be adopted by corporate customers. Even after two decades, in 2020 there were still laggard enterprise sectors that had been hesitant to switch to a cloud architecture. The advent of Covid will probably eliminate that as an option. In the next five years, we expect 90%+ of application software will be delivered over the cloud and sold via a SaaS subscription. This, together with the natural emergence of new application categories and more nimble next generation applications vendors, means that the SaaS market will continue to be a fertile ground for founders to build and grow their companies. At the same time, the Public Cloud will continue to expand at the infrastructure layer. Just as Covid was the last nail in the coffin of “on premise” third-party applications, so too will it further accentuate the move to close enterprise datacenters and move even custom enterprise built and owned applications to the cloud. We believe there will be a compelling market for the software tools and services to help make this happen. Finally, the emerging trend of Cognitive Apps will leverage the public cloud to build a whole new class of applications with Intelligence Inside. These will frequently (but not always) be sold via the same subscription model that is used by current-generation SaaS vendors. Both in terms of how they are built and how they are used, we expect Cognitive Apps will represent the next generation of where enterprise technology is going."
    }
  ]
}